





关于mysql和优化

客户机－服务器软件 主从式架构（Client-Server Model）或客户端－服务器（Client/Server）结构简称C/S结构，是一种网络架构，通常在该网络架构下，软件分为客户端（Client）和服务器（Server）。

服务器是整个应用系统资源的存储与管理中心，多个客户端则各自处理相应的功能，共同实现完整的应用。在客户端／服务器结构中，客户端用户的请求被传送到数据库服务器，数据库服务器进行处理后，将结果返回给用户，从而减少了网络数据的传输量。 用户使用应用程序时，首先启动客户端通过有关命令告知服务器进行连接以完成各种操作，而服务器则按照此请示提供相应的服务。每一个客户端软件的实例都可以向一个服务器或应用程序服务器发出请求。 这种系统的特点是，客户端和服务器程序不在同一台计算机上运行，这些客户端和服务器程序通常归属不同的计算机。 主从式架构通过不同的途径应用于很多不同类型的应用程序，比如，现在人们熟悉的在因特网上使用的网页。例如，当顾客想要在当当网上买书的时候，电脑和网页浏览器就被当作一个客户端，同时，组成当当网的电脑、数据库和应用程序被当作服务器。当顾客的网页浏览器向当当网请求搜寻数据库相关的图书时，当当网服务器从其数据库中找出所有该类型的图书信息，结合成一个网页，再发送给顾客的浏览器。服务器端一般使用高性能的计算机，并配合使用不同类型的数据库，比如Oracle、Sybase、MySQL等；客户端需要安装专门的软件，比如专门开发的客户端工具浏览器等。 

# MySQL经典36问



# mysql和oracle的区别是什么

**1、类型和成本的区别**

oracle数据库是一个对象关系数据库管理系统（ORDBMS），一个重量型数据库。它通常被称为Oracle RDBMS或简称为Oracle，是一个收费的数据库。

MySQL是一个开源的关系数据库管理系统（RDBMS），一个是轻量型数据库。它是世界上使用最多的RDBMS，作为服务器运行，提供对多个数据库的多用户访问。它是一个开源、免费的数据库。

**2、存储上的区别**

与Oracle相比，MySQL没有表空间，角色管理，快照，同义词和包以及自动存储管理。

**3、安全性上的区别**

MySQL使用三个参数来验证用户，即用户名，密码和位置；Oracle使用了许多安全功能，如用户名，密码，配置文件，本地身份验证，外部身份验证，高级安全增强功能等。

**4、对事务的支持**

MySQL在innodb存储引擎的行级锁的情况下才可支持事务，而Oracle则完全支持事务

**5、性能诊断上的区别**

MySQL的诊断调优方法较少，主要有慢查询日志。

Oracle有各种成熟的性能诊断调优工具，能实现很多自动分析、诊断功能。比如awr、addm、sqltrace、tkproof等



**(1) 对事务的提交**
   MySQL默认是自动提交，而Oracle默认不自动提交，需要用户手动提交，需要在写commit;指令或者点击commit按钮
**(2) 分页查询**
   MySQL是直接在SQL语句中写"select... from ...where...limit x, y",有limit就可以实现分页;而Oracle则是需要用到伪列ROWNUM和嵌套查询
**(3) 事务隔离级别**
 **1.数据库默认隔离级别: mysql ---repeatable,oracle,sql server ---read commited**，同时二者都支持serializable串行化事务隔离级别，可以实现最高级别的【InnoDB 存储引擎默认使用 **REPEATABLE-READ（可重读）并不会有任何性能损失。**】
   读一致性。每个session提交后其他session才能看到提交的更改。Oracle通过在undo表空间中构造多版本数据块来实现读一致性，每个session
   查询时，如果对应的数据块发生变化，Oracle会在undo表空间中为这个session构造它查询时的旧的数据块
  MySQL没有类似Oracle的构造多版本数据块的机制，只支持read commited的隔离级别。一个session读取数据时，其他session不能更改数据，但
   可以在表最后插入数据。session更新数据时，要加上排它锁，其他session无法访问数据
**(4) 对事务的支持**
   MySQL在innodb存储引擎的行级锁的情况下才可支持事务，而Oracle则完全支持事务
**(5) 保存数据的持久性**
   MySQL是在数据库更新或者重启，则会丢失数据，Oracle把提交的sql操作线写入了在线联机日志文件中，保持到了磁盘上，可以随时恢复
**(6) 并发性**
   MySQL以表级锁为主，对资源锁定的粒度很大，如果一个session对一个表加锁时间过长，会让其他session无法更新此表中的数据。
 虽然InnoDB引擎的表可以用行级锁，但这个行级锁的机制依赖于表的索引，如果表没有索引，或者sql语句没有使用索引，那么仍然使用表级锁。
 Oracle使用行级锁，对资源锁定的粒度要小很多，只是锁定sql需要的资源，并且加锁是在数据库中的数据行上，不依赖与索引。所以Oracle对并发性的支持要好很多。
**(7) 逻辑备份**
   MySQL逻辑备份时要锁定数据，才能保证备份的数据是一致的，影响业务正常的dml使用,Oracle逻辑备份时不锁定数据，且备份的数据是一致
**(8) 复制**
   MySQL:复制服务器配置简单，但主库出问题时，丛库有可能丢失一定的数据。且需要手工切换丛库到主库。
   Oracle:既有推或拉式的传统数据复制，也有dataguard的双机或多机容灾机制，主库出现问题是，可以自动切换备库到主库，但配置管理较复杂。
**(9) 性能诊断**
   MySQL的诊断调优方法较少，主要有慢查询日志。
   Oracle有各种成熟的性能诊断调优工具，能实现很多自动分析、诊断功能。比如awr、addm、sqltrace、tkproof等   
**(10)权限与安全**
   MySQL的用户与主机有关，感觉没有什么意义，另外更容易被仿冒主机及ip有可乘之机。
   Oracle的权限与安全概念比较传统，中规中矩。
**(11)分区表和分区索引**
   MySQL的分区表还不太成熟稳定。
   Oracle的分区表和分区索引功能很成熟，可以提高用户访问db的体验。
**(12)管理工具**
   MySQL管理工具较少，在linux下的管理工具的安装有时要安装额外的包（phpmyadmin， etc)，有一定复杂性。
   Oracle有多种成熟的命令行、图形界面、web管理工具，还有很多第三方的管理工具，管理极其方便高效。
**(13)最重要的区别**
   MySQL是轻量型数据库，并且免费，没有服务恢复数据。
   Oracle是重量型数据库，收费，Oracle公司对Oracle数据库有任何服务。

# 解释⼀下什么是池化设计思想。什么是数据库连接池?为什么需要数据库连接池?
池话设计应该不是一个新名词。我们常见的如java线程池、jdbc连接池、redis连接池等就是这类设计的代表实现。这种设计会初始预设资源，解决的问题就是抵消每次获取资源的消耗，如创建线程的开销，获取远程连接的开销等。就好比你去食堂打饭，打饭的大妈会先把饭盛好几份放那里，你来了就直接拿着饭盒加菜即可，不用再临时又盛饭又打菜，效率就高了。除了初始化资源，池化设计还包括如下这些特征：池子的初始值、池子的活跃值、池子的最大值等，这些特征可以直接映射到java线程池和数据库连接池的成员属性中。

数据库连接本质是一个socket连接，数据库服务端还要维护一些缓存和用户权限信息之类的，所以占用了一些内存，我们可以把数据库连接池看做是维护的数据连接的缓存，以便将来需要对数据库的请求时可以重用这些连接，在连接池中创建连接后将其放置在池中，并再次使用它，因此不必建立新的连接。如果使用了所有连接，则会建立一个新连接并将其添加到池中，连接池还减少了用户必须等待建立与数据库的连接的时间。

# 数据库三范式是什么?

1.第一范式（1NF）：字段具有原子性,不可再分。(所有关系型数据库系统都满足第一范式数据库表中的字段都是单一属性的，不可再分)
2.第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。要求数据库表中的每个实例或行必须可以被惟一地区分。通常需要为表加上一个列，以存储各个实例的惟一标识。这个惟一属性列被称为主关键字或主键。
3.满足第三范式（3NF）必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。 >所以第三范式具有如下特征： >>1. 每一列只有一个值 >>2. 每一行都能区分。 >>3. 每一个表都不包含其他表已经包含的非主关键字信息。

# 【最新】SQL优化13连问

[SQL优化13连问，收藏好！ (qq.com)](https://mp.weixin.qq.com/s/MDdp1TsAXUjxzi7nUpWepg)



## 1.日常工作中，你是怎么优化SQL的？

大家可以从这几个维度回答这个问题：

- 分析慢查询日志
- 使用explain查看执行计划
- 索引优化
- 深分页优化
- 避免全表扫描
- 避免返回不必要的数据（如`select`具体字段而不是`select*`）
- 使用合适的数据类型（如可以使用`int`类型的话，就不要设计为`varchar`）
- 优化sql结构（如`join`优化等等）
- 适当分批量进行 (如批量更新、删除)
- 定期清理无用的数据
- 适当分库分表
- 读写分离

## 2. 是否遇到过深分页问题，如何解决

我们可以通过减少回表次数来优化。一般有**标签记录法**和**延迟关联法**。

**标签记录法**

> 就是标记一下上次查询到哪一条了，下次再来查的时候，从该条开始往下扫描。就好像看书一样，上次看到哪里了，你就折叠一下或者夹个书签，下次来看的时候，直接就翻到啦。

假设上一次记录到100000，则SQL可以修改为：

```
select  id,name,balance FROM account where id > 100000 limit 10;
```

这样的话，后面无论翻多少页，性能都会不错的，因为命中了`id`索引。但是这种方式有局限性：需要一种类似连续自增的字段。

**延迟关联法**

延迟关联法，就是把条件转移到主键索引树，然后减少回表。假设原生SQL是这样的的，其中`id`是主键，`create_time`是普通索引

```
select id,name,balance from account where create_time> '2020-09-19' limit 100000,10;
```

使用延迟关联法优化，如下：

```
select  acct1.id,acct1.name,acct1.balance FROM account acct1 INNER JOIN 
(SELECT a.id FROM account a WHERE a.create_time > '2020-09-19' limit 100000, 10) 
AS acct2 on acct1.id= acct2.id;
```

优化思路就是，先通过`idx_create_time`二级索引树查询到满足条件的`主键ID`，再与原表通过`主键ID`内连接，这样后面直接走了主键索引了，同时也减少了回表。

## 3. 聊聊explain执行计划

当`explain`与`SQL`一起使用时，MySQL将显示来自优化器的有关语句执行计划的信息。即`MySQL`解释了它将如何处理该语句，包括有关如何连接表以及以何种顺序连接表等信息。

一条简单SQL，使用了`explain`的效果如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/PoF8jo1PmpwfIW9ftvOzL2DFpnG8keNC5lrG8Gwb6OIfaBnZTluHN9Ed37xuOZ5oFDg1gtoj8Ib8SZWLxswNTg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

一般来说，我们需要重点关注`type、rows、filtered、extra、key`。

3.1 type

type表示**连接类型**，查看索引执行情况的一个重要指标。以下性能从好到坏依次：`system > const > eq_ref > ref > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL`

- system：这种类型要求数据库表中只有一条数据，是`const`类型的一个特例，一般情况下是不会出现的。
- const：通过一次索引就能找到数据，一般用于主键或唯一索引作为条件，这类扫描效率极高，，速度非常快。
- eq_ref：常用于主键或唯一索引扫描，一般指使用主键的关联查询
- ref : 常用于非主键和唯一索引扫描。
- ref_or_null：这种连接类型类似于`ref`，区别在于`MySQL`会额外搜索包含`NULL`值的行
- index_merge：使用了索引合并优化方法，查询使用了两个以上的索引。
- unique_subquery：类似于`eq_ref`，条件用了`in`子查询
- index_subquery：区别于`unique_subquery`，用于非唯一索引，可以返回重复值。
- range：常用于范围查询，比如：between ... and 或 In 等操作
- index：全索引扫描
- ALL：全表扫描

3.2 rows

该列表示MySQL估算要找到我们所需的记录，需要读取的行数。对于InnoDB表，此数字是估计值，并非一定是个准确值。

3.3 filtered

该列是一个百分比的值，表里符合条件的记录数的百分比。简单点说，这个字段表示存储引擎返回的数据在经过过滤后，剩下满足条件的记录数量的比例。

3.4 extra

该字段包含有关MySQL如何解析查询的其他信息，它一般会出现这几个值：

- Using filesort：表示按文件排序，一般是在指定的排序和索引排序不一致的情况才会出现。一般见于order by语句
- Using index ：表示是否用了覆盖索引。
- Using temporary: 表示是否使用了临时表,性能特别差，需要重点优化。一般多见于group by语句，或者union语句。
- Using where : 表示使用了where条件过滤.
- Using index condition：MySQL5.6之后新增的索引下推。在存储引擎层进行数据过滤，而不是在服务层过滤，利用索引现有的数据减少回表的数据。

3.5 key

该列表示实际用到的索引。一般配合`possible_keys`列一起看。

**注意**:有时候，`explain`配合`show WARNINGS; `（可以查看优化后,最终执行的sql），效果更佳哦。

## 4.说说大表的优化方案

![image-20230405000302723](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20230405000302723.png) 

- **数据库设计优化**

合理的数据库设计可以极大地提高查询效率。我们在设计大表时，可以考虑**拆分表、使用分区表、添加索引等方式**来优化表结构。同时也要**避免使用大量冗余字段、避免频繁使用join查询**等操作。

- **索引优化**

对于大表的查询操作，索引优化是非常重要的一环。可以考虑**增加或者修改索引、使用覆盖索引、使用联合索引等方式来提高查询效率**。同时也要注意**定期清理冗余的索引以及对于经常使用的查询语句建立索引**。

- **分区优化**

将大表按照某个列分成多个分区表，每个分区表的数据量较小，可以提高查询和更新的性能。分区表还可以帮助在维护表结构的同时，减少锁表时间，提高并发处理能力。

- **数据清理归档**

对于一些历史数据或者无用数据，**可以进行定期归档，避免数据过多造成SQL查询效率降低**。同时也要注意对于大表进行定期的数据备份以及紧急数据恢复的准备工作。

- **缓存优化**

对于一些经常被查询的数据，可以使用缓存优化。使用`Redis`等缓存中间件来缓存常用的数据，以减少查询数据库的次数，提高查询效率。

- **SQL语句优化**

在编写SQL查询语句时，要尽可能地简单明了，避免复杂的查询语句，同时也要避免一些不必要的查询操作。对于复杂的查询语句，可以使用`Explain`执行计划来进行优化。同时也要注意避免使用`OR`等耗费性能的操作符。

- **分库分表**

如果数据量千万级别，需要考虑分库分表哈。分库分表相关知识点，可以看我之前这篇文章哈，我们为什么要分库分表？

## 5.哪些因素可能导致MySQL慢查询？

慢查询一般有以下这些原因：

![image-20230405000336118](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20230405000336118.png) 

- 添加合适索引（在`where、group by、order by`等后面的字段添加合适索引）
- 选择合适的索引类型 (`B-tree`索引适合范围查询、哈希索引适合等值查询)
- 注意不适合加索引的场景（数据量少的表，更新频繁的字段，区分度低的字段）
- 加索引的时候，需要考虑覆盖索引，减少回表，**考虑联合索引的最左前缀原则**
- `explain`查看`SQL`的执行计划，确认是否会命中索引。
- 注意索引并不是越多越好，**通常建议在单个表中不要超过5个索引**。因为索引会占用磁盘空间，索引更新代价高。

## 7.聊聊慢SQL的优化思路

1. 查看慢查询日志记录，分析慢SQL
2. explain分析SQL的执行计划
3. profile 分析执行耗时
4. Optimizer Trace分析详情
5. 确定问题并采用相应的措施

7.1 查看慢查询日志记录，分析慢SQL

如何定位慢SQL呢、我们可以通过**slow log**来查看慢`SQL`。默认的情况下呢，MySQL数据库是不开启慢查询日志（`slow query log`）呢。所以我们需要手动把它打开。

查看下慢查询日志配置，我们可以使用`show variables like 'slow_query_log%'`命令，如下：

##  order by查询效率慢,如何优化.

大家是否还记得`order by`查询为什么会慢嘛?

`order by`排序，分为全字段排序和`rowid`排序。它是拿`max_length_for_sort_data`和结果行数据长度对比，如果结果行数据长度超过`max_length_for_sort_data`这个值，就会走`rowid`排序，相反，则走全字段排序。

`rowid`排序，一般需要回表去找满足条件的数据，所以效率会慢一点.如果是`order by`排序,可能会借助磁盘文件排序的话，效率就更慢一点.

如何优化`order by`的文件排序?

- 因为数据是无序的，所以就需要排序。如果数据本身是有序的，那就不会再用到文件排序啦。而索引数据本身是有序的，我们通过建立索引来优化`order by`语句。
- 我们还可以通过调整`max_length_for_sort_data、sort_buffer_size`等参数优化；

## . group by 查询慢的话,如何优化呀.

`group by`一般用于分组统计，它表达的逻辑就是根据一定的规则，进行分组。日常开发中，我们使用得比较频繁。如果不注意，很容易产生慢`SQL`。

`group by`可能会慢在哪里？**因为它既用到临时表，又默认用到排序。有时候还可能用到磁盘临时表。**

- 如果执行过程中，会发现内存临时表大小到达了上限（控制这个上限的参数就是`tmp_table_size`），会把内存临时表转成磁盘临时表。
- 如果数据量很大，很可能这个查询需要的磁盘临时表，就会占用大量的磁盘空间。

如何优化group by呢?

- group by 后面的字段加索引
- order by null 不用排序
- 尽量只使用内存临时表
- 使用SQL_BIG_RESULT

# 连接管理

系统（客户端）访问`MySQL`服务器前，做的第一件事就是建立`TCP`连接。

经过三次握手建立连接成功后，`MySQL`服务器对`TCP`传输过来的账号密码做身份认证、权限获取。

- **用户名或密码不对，会收到一个Access denied for user错误，客户端程序结束执行**
- **用户名密码认证通过，会从权限表查出账号拥有的权限与连接关联，之后的权限判断逻辑，都将依赖于此时读到的权限**

接着我们来思考一个问题

一个系统只会和`MySQL`服务器建立一个连接吗？

只能有一个系统和`MySQL`服务器建立连接吗？

当然不是，多个系统都可以和`MySQL`服务器建立连接，每个系统建立的连接肯定不止一个。

所以，为了解决`TCP`无限创建与`TCP`频繁创建销毁带来的资源耗尽、性能下降问题。

`MySQL`服务器里有专门的`TCP`连接池限制接数，采用长连接模式复用`TCP`连接，来解决上述问题。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nzDaJd7LOWUfTL7rtaKlqibiab08oUCH2Bo6h5beN4IAToLBYszia9icumYZsIBFB7icLnVmvt8WFznQTg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

`TCP`连接收到请求后，必须要分配给一个线程去执行，所以还会有个线程池，去走后面的流程。

这些内容我们都归纳到`MySQL`的**连接管理**组件中。

所以**连接管理**的职责是负责认证、管理连接、获取权限信息。

当我们通过驱动程序（mysql-connector-python,pymysql）连接 MySQL 服务端的时候，就是把连接参数传递给驱动程序，驱动程序再根据参数会发起到 MySQL 服务端的 TCP 连接。当 TCP 连接建立之后驱动程序与服务端之间会按特定的格式和次序交换数据包，数据包的格式和发送次序由 MySQL 协议 规定。



# 一些命令

## 查看隔离级别

mysql> select @@global.tx_isolation;
+-----------------------+
| @@global.tx_isolation |
+-----------------------+
| REPEATABLE-READ       |
+-----------------------+

## 查询锁情况的命令

3.show engine innodb status\G; 这条命令可以查询InnoDB存储引擎的运行时信息，包括死锁的详细信息。 

查询information_schema用户下的表 通过information_shcema下的innodb_locks、innodb_lock_waits和innodb_trx这三张表可以更新监控当前事务并且分析存在锁的问题。

## 创建用户 授权

CREATE USER 'username'@'host' IDENTIFIED BY 'password';

GRANT privileges ON databasename.tablename TO  'username'@'host' 

查看表结构

desc table_name;

## mysql有关权限的表

MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别**user，db，table_priv，columns_priv和host**。下面分别介绍一下这些表的结构和内容：
user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。
db权限表：记录各个帐号在各个数据库上的操作权限。
table_priv权限表：记录数据表级的操作权限。
columns_priv权限表：记录数据列级的操作权限。
host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。
这个权限表不受GRANT和REVOKE语句的影响。

## 如何查询最后一行记录？



select * from table_name order by id desc limit 1;



## Mysq 1删除表的几种方式？区别？



l.  delete :仅删除表数据，支持条件过滤，支持回滚。记录日志。因此比较慢。

delete from table_name;

\2.  truncate:美[ˈtrʌŋkeɪt]仅删除所有数据，不支持条件过滤，不支持回滚。不记录日志，效率高于delete。

truncate table table_name;

drop:删除表数据同时删除表结构。将表所占的空间都释放掉。删除效率最高。

drop table table_name;

##  看当前表有哪些索引？

show index from table

# sql 预编译语句

预编译语句是什么
通常我们的一条sql在db接收到最终执行完毕返回可以分为下面三个过程：

词法和语义解析
优化sql语句，制定执行计划
执行并返回结果
我们把这种普通语句称作Immediate Statements。

但是很多情况，我们的一条sql语句可能会反复执行，或者每次执行的时候只有个别的值不同（比如query的where子句值不同，update的set子句值不同,insert的values值不同）。
如果每次都需要经过上面的词法语义解析、语句优化、制定执行计划等，则效率就明显不行了。

所谓预编译语句就是将这类语句中的值用占位符替代，可以视为将sql语句模板化或者说参数化，一般称这类语句叫Prepared Statements或者

Parameterized Statements

预编译语句的优势在于归纳为：一次编译、多次运行，省去了解析优化等过程；此外预编译语句能防止sql注入。
当然就优化来说，很多时候最优的执行计划不是光靠知道sql语句的模板就能决定了，往往就是需要通过具体值来预估出成本代价。

3. MySQL的预编译功能

通过 PREPARE stmt_name FROM preparable_stm的语法来预编译一条sql语句

mysql> prepare ins from 'insert into t select ?,?';
Query OK, 0 rows affected (0.00 sec)
Statement prepared
过EXECUTE stmt_name [USING @var_name [, @var_name] …]的语法来执行预编译语句

```
mysql> set @a=999,@b='hello';
Query OK, 0 rows affected (0.00 sec)

mysql> execute ins using @a,@b;
Query OK, 1 row affected (0.01 sec)
Records: 1  Duplicates: 0  Warnings: 0'
```

在类似MyBatis等ORM框架中，往往会大量用到预编译语句。例如MyBatis中语句的statementType默认为PREPARED，因此通常语句查询时都会委托connection调用prepareStatement来获取一个java.sql.PreparedStatement对象。

![image-20220213035109246](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220213035109246.png)





#  Mysql中的数值类型？



| **整数类型**             | **字节** | **最小值**                                                   | **最大值**                                              |
| ------------------------ | -------- | ------------------------------------------------------------ | ------------------------------------------------------- |
| TINYINT                  | 1        | 有符号-128  无符号0                                          | 有符号127  无符号255                                    |
| SMALLINT                 | 2        | 有符号-32768  无符号0                                        | 有符号32767  无符号65535                                |
| MEDIUM INT               | 3        | 有符号-8388608  无符号0                                      | 有符号8388607  无符号1677215                            |
| INT、INTEGER             | 4        | 有符号-2147483648  无符号0                                   | 有符号 2147483647  无符号 4294967295                    |
| BIGINT                   | 8        | 有符号-9223372036854775808  无符号0                          | 有符号 9223372036854775807  无符号 18446744073709551615 |
| **浮点数类型**           | **字节** | **最小值**                                                   | **最大值**                                              |
| FLOAT                    | 4        | ±1.175494351E-38                                             | ±3.402823466E+38                                        |
| DOUBLE                   | 8        | ±2.2250738585072014E-308                                     | 土 1.7976931348623157E+308                              |
| **定点数类型**           | **字节** | **描述**                                                     |                                                         |
| DEC(M,D),  DECIM AL(M,D) | M+2      | 最大取值范围与DOUBLE相同，给定DECIMAL的有效取值范围由M和D 决定 |                                                         |
| **位类型**               | **字节** | **最小值**                                                   | **最大值**                                              |
| BIT(M)                   | 1〜8     | BIT(l)                                                       | bit(64)                                                 |

# 主键和唯一索引区别？

本质区别，主键是一种约束，唯一索引是一种索引。

主键不能有空值（非空+唯一），唯一索引可以为空。

主键可以是其他表的外键，唯一索引不可以。

一个表只能有一个主键，唯一索引可以多个。

都可以建立联合主键或联合唯一索引。

主键->聚簇索引，唯一索引-＞非聚簇索引。



# 说出数据连接池的工作机制是什么?

J2EE服务器启动时会建立一定数量的池连接，并一直维持不少于此数目的池连接。客户端程序需要连接时，池驱动程序会返回一个未使用的池连接并将其标记为忙。如果当前没有空闲连接，池驱动程序就新建一定数量的连接，新建连接的数量有配置参数决定。当使用的池连接调用完成后，池驱动程序将此连接表记为空闲，其他调用就可以使用这个连接。
实现方式，返回的Connection是原始Connection的代理，**代理Connection的close方法不是真正关连接，而是把它代理的Connection对象还回到连接池中**



# 5.7新特性



与MySQL 5.6相比，MySQL 5.7具有以下几个方面的新功能。 

·随机root密码：MySQL 5.7数据库初始化完成后，会自动生成一个root@localhost用户，root用户的密码不为空，而是随机产生一个密码。 ·

自定义test数据库：MySQL 5.7默认安装完成后没有test数据库。用户可以自行创建test数据库并对其进行权限控制。 ·

默认SSL加密：MySQL 5.7采用了更加简单的SSL安全访问机制，默认连接使用SSL的加密方式。 ·

密码过期策略：MySQL 5.7支持用户设置密码过期策略，要求用户在一定时间过后必须修改密码。 ·

用户锁：MySQL 5.7为管理员提供了暂时禁用某个用户的功能，使被锁定的用户无法访问和使用数据库。 ·

全面支持JSON：MySQL 5.7在服务器端提供了一组便于操作JSON的函数。存储的方法是将JSON编码成BLOB后再由存储引擎进行处理。这样，MySQL就同时拥有了关系型数据库和非关系型数据库的优点，并且可以提供完整的事务支持。 ·支持两类生成列（generated column）：生成列是通过数据库中的其他列计算得到的一列。当为生成列创建索引时，可以便捷地加快查询速度。MySQL 5.7支持虚拟生成列和存储生成列。虚拟生成列仅将数据保存在表的元数据中，作为缺省的生成列类型；存储生成列则是将数据永久保存在磁盘上，需要更多的磁盘空间。 ·

引入系统库（sys schema）：系统库中包含一系列视图、函数和存储过程，通过多线程、多进程、组合事务提交和基于行的优化方式将复制功能提高5倍以上，用户向外扩充其跨商品系统的工作负载时，得以大幅提升复制的效能和效率。 



# 解析与优化

经过了连接管理，现在`MySQL`服务器已经获取到`SQL`字符串。

如果是查询语句，`MySQL`服务器会使用`select SQL`字符串作为`key`。

去缓存中获取，命中缓存，直接返回结果（**返回前需要做权限验证**），未命中执行后面的阶段，这个步骤叫**查询缓存**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nzDaJd7LOWUfTL7rtaKlqibiap7tgOblicg4YxFribqHbzXN2vG18LJyPgcqZ6pJELkmMTnX5XjC38MTg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

需要注意，`select SQL`字符串要完全匹配，有任何不同的地方都会导致缓存不被命中（**空格、注释、大小写、某些系统函数**）。

> 小贴士：虽然查询缓存有时可以提升系统性能，但也不得不因维护这块缓存而造成一些开销，从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。

没有命中缓存，或者非`select SQL`就来到**分析器**阶段了。

因为系统发送过来的只是一段文本字符串，所以`MySQL`服务器要按照`SQL`语法对这段文本进行解析。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nzDaJd7LOWUfTL7rtaKlqibiaicm6nh8rVmibE0oRr8nvTSgra5ptic7k0VwRDYzHqrOA9Uu18oEmff6vg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

如果你的`SQL`字符串不符合语法规范，就会收到`You have an error in your SQL syntax`错误提醒

通过了**分析器**，说明`SQL`字符串符合语法规范，现在`MySQL`服务器要执行`SQL`语句了。

`MySQL`服务器要怎么执行呢？

你需要产出执行计划，交给`MySQL`服务器执行，所以来到了**优化器**阶段。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nzDaJd7LOWUfTL7rtaKlqibiawPicQgxONOtb1T8R2zG6icMzyicbKGgsWBqJGpBVFialDtrWqsguQ6ymxg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

优化器不仅仅只是生成执行计划这么简单，这个过程它会帮你优化`SQL`语句。

如**外连接转换为内连接、表达式简化、子查询转为连接、连接顺序、索引选择**等一堆东西，优化的结果就是执行计划。

截止到现在，还没有真正去读写真实的表，仅仅只是产出了一个执行计划。

于是就进入了**执行器**阶段，`MySQL`服务器终于要执行`SQL`语句了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nzDaJd7LOWUfTL7rtaKlqibiaAv6QqdfpRz2nN4c573GVrQ9eKfF8STslib8IUrAokibmOg9JG5LxFAEA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

开始执行的时候，要先判断一下对这个表有没有相应的权限，如果没有，就会返回权限错误。

如果有权限，根据执行计划调用存储引擎`API`对表进行的读写。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nzDaJd7LOWUfTL7rtaKlqibiaJice8RCjInQEFjEQWlrZQ1EscvOB3IX22WDqmytl4Hp86ERq4vUMRPQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

存储引擎`API`只是抽象接口，下面还有个**存储引擎层**，具体实现还是要看表选择的存储引擎。

讲到这里，上面提到的**查询缓存、分析器、优化器、执行器**都可以归纳到`MySQL`的**解析与优化**组件中。

所以**解析与优化**的职责如下：

- **缓存**
- **SQL语法解析验证**
- **SQL优化并生成执行计划**
- **根据执行计划调用存储引擎接口**

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nzDaJd7LOWUfTL7rtaKlqibiarGf6kibRqjyazon4ppMfzIEZBU45JkdCRGLToY9I8icr55vtp9zOt5JQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中**连接管理**与**解析与优化**处于`MySQL`架构中的`Server`层。



`MySql`架构分为`Servce`层与**存储引擎**层。

**连接管理、解析与优化**这些并不涉及读写表数据的组件划分到`Servce`层，读写表数据而是交给**存储引擎层**来做。

通过这种架构设计，我们发现`Servce`层其实就是公用层，**存储引擎层**就是多态层，按需选择具体的存储引擎。

再细想下，它和**模板方法设计模式**一摸一样，它们的执行流程是固定的，`Servce`层等于公用模板函数，**存储引擎层**等于抽象模板函数，按需子类实现

# 一条sql语句执行流程

简单来说 MySQL 主要分为 Server 层和存储引擎层：

Server 层：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。
存储引擎： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。

连接器： 身份认证和权限相关(登录 MySQL 的时候)。
查询缓存: 执行查询语句的时候，会先查询缓存（MySQL 8.0 版本后移除，因为这个功能不太实用）。
分析器: 没有命中缓存的话，SQL 语句就会经过分析器，分析器说白了就是要先看你的 SQL 语句要干嘛，再检查你的 SQL 语句语法是否正确。

第一步，词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。

第二步，语法分析，主要就是判断你输入的 sql 是否正确，是否符合 MySQL 的语法。

完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。


优化器： 按照 MySQL 认为最优的方案去执行。
执行器: 执行语句，然后从存储引擎返回数据。

当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果。

![image-20220226081155684](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220226081155684.png)

# 什么是MySQL的套接字文件？

  答案：MySQL有两种连接方式，常用的是TCP/IP方式，如下所示： 还有一种是套接字方式。Unix系统下本地连接MySQL可以采用Unix套接字方式，这种方式需要一个套接字（Socket）文件。套接字文件就是当用套接字方式进行连接时需要的文件。套接字方式比用TCP/IP的方式更快、更安全，但只适用于MySQL和客户端在同一台PC上的场景。套接字文件可由参数socket控制，一般在/tmp 目录下，名为mysql.sock，也可以放在其他目录下



# mysql字符集

**6.**   **Mysq**l字符集

mysql服务器可以支持多种字符集（可以用show character set命令查看所有mysq 1支持的字符集），在同 一台服务器、同一个数据库、甚至同一个表的不同字段都可以指定使用不同的字符集。

mysql的字符集包括字符集（CHARACTER）和校对规则（COLLATION）两个概念。

如何选择字符集？

建议在能够完全满足应用的前提下，尽量使用小的字符集。因为更小的字符集意味着能够节省空间、减 少网络传输字节数，同时由于存储空间的较小间接的提高了系统的性能。

有很多字符集可以保存汉字，比如Utf8、gb2312、gbk、latin等等，但是常用的是gb2312和gbk。因 为gb2312字会比gbk字会小，有些偏僻字（例如:学）不能保存，因此在选择字符集的时候一定要权衡这 些偏僻字在应用出现的几率以及造成的影响，不能做出肯定答复的话最好选用gbk。





# 什么是MySQL的pid文件？ 

答案：pid文件是MySQL实例的进程ID文件。当MySQL实例启动时，会将自己的进程ID写入一个文件中，该文件即为pid文件。该文件可由参数pid_file控制，默认路径位于数据库目录下，文件名为：主机名.pid，如下所示： 

show variables like 'pid_file';



## MySQL有哪几类物理文件？

 答案：MySQL数据库的文件包括：

1）参数文件：my.cnf。 

2）日志文件，包括错误日志、查询日志、慢查询日志、二进制日志。 

3）MySQL表文件：用来存放MySQL表结构的文件，一般以.frm为后缀。 

4）Socket文件：当用Unix域套接字方式进行连接时需要的文件。 

5）pid文件：MySQL实例的进程ID文件。 

6）存储引擎文件：每个存储引擎都有自己的文件夹来保存各种数据，这些存储引擎真正存储了数据和索引等数据。 



# Mysql 架构器中各个模块都是什么？

（2017-11-25-wzz）

（1）、连接管理与安全验证是什么？
每个客户端都会建立一个与服务器连接的线程，**服务器会有一个线程池来管理这些 连接**；如果客户端需要连接到MYSQL 数据库还需要进行验证，包括用户名、密码、 主机信息等。
（2）、解析器是什么？
解析器的作用主要是分析查询语句，最终生成解析树；首先解析器会对查询语句的语法进行分析，分析语法是否有问题。还有解析器会查询缓存，如果在缓存中有对应的语句，就返回查询结果不进行接下来的优化执行操作。前提是缓存中的数据没有被修改，当然如果被修改了也会被清出缓存。
（3）、优化器怎么用？
优化器的作用主要是对查询语句进行优化操作，包括选择合适的索引，数据的读取方式，包括获取查询的开销信息，统计信息等，这也是为什么图中会有优化器指向存储引擎的箭头。之前在别的文章没有看到优化器跟存储引擎之间的关系，在这里我个人的理解是因为优化器需要通过存储引擎获取查询的大致数据和统计信息。
（4）、执行器是什么？
执行器包括执行查询语句，返回查询结果，生成执行计划包括与存储引擎的一些处理操作。

# sql语句的执行流程？

客户端连接数据库，验证身份。

获取当前用户权限。

当你查询时，会先去缓存看看，如果有返回。 如果没有，分析器对sql做词法分析。

优化器对sql进行“它认为比较好的优化”。 执行器负责具体执行sql语句。

最后把数据返回给客户端。

# MySQL InnoDB 的内存组件

`MySQL`中执行一条`SQL`语句，相应表数据的读写都是由存储引擎去做（**更新数据、查询数据**）。

在这个过程，存储引擎需要决策一些事情

- **数据是从内存查还是从硬盘查**
- **数据是更新在内存，还是硬盘**
- **内存的数据什么时候同步到硬盘**

所以存储引擎会按照内部逻辑与内存、硬盘交互。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nxwUmLaZQARHt8gs7q78SPA6kg4tiblRTsewVzjeGW40av5Yyna05QgWocIZN8OE7HOB47kLPicRssw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

我们可以按需选择存储引擎，比如常见的 `InnoDB、MyISAM、Memory` 等等。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nxwUmLaZQARHt8gs7q78SPAWP3icmLs7JMqKpIuv1RrClXCqS3hGamjrdTDsoXraTc2ur4rTIu38nA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

众多存储引擎中，`InnoDB`是最为常用的，从 `MySQL5.5.8` 版本开始，`InnoDB`是默认的存储引擎。

InnoDB简介

`InnoDB`存储引擎支持事务，其设计目标主要面向在线事务处理（`OLTP`）的应用。

特点是行锁设计、支持`MVCC`、外键，提供一致性非锁定读，同时本身设计能够最有效的利用内存和`CPU`，是 `MySQL` 最常用的存储引擎。

InnoDB的重要内存结构

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nxwUmLaZQARHt8gs7q78SPAy24fia9libictC5o0ePYfbRBoAe6IYv5ebSTSzNP8DuaRLzqcEFGx2N9w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

`InnoDB`存储引擎在内存中有两个非常重要的组件，分别是缓冲池（`Buffer Pool`）和重做日志缓存（`redo log buffer`）。

## Buffer Pool简介

[十一. MySQL InnoDB 三大特性之 BufferPool_苹果香蕉西红柿的博客-CSDN博客](https://blog.csdn.net/qq_29799655/article/details/128022977) 

缓冲池（`Buffer Pool`）里面会缓存很多的数据，比如数据页、索引页、锁信息等等。

**`MySQL`表数据是以页为单位，你查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放入到 `Buffer Pool` 中。**

后续的查询先从 `Buffer Pool` 中找，没有命中再去硬盘加载，减少硬盘 `IO` 开销，提升性能。

更新表数据的时，如果 `Buffer Pool` 里命中数据，就直接在 `Buffer Pool` 里更新。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nxwUmLaZQARHt8gs7q78SPAva8TuTwa2fzwOJlHv4OZsn83tmwYs2CbA98VWP9ibXH5GsPVyXHp7gQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

总之 `Buffer Pool` 会缓存很多的数据，以便后续的查询与更新。

**怎么知道数据页是否被缓存？**数据库中有一个**数据页缓存哈希表**，用**表空间号+数据页号**，作为一个key，然后缓存页的地址作为value**表空间号+数据页号 = 缓存页地址这里找到空的缓存页是用free链表实现的，它是一个双向链表，链表节点是空闲的缓存页对应的描述信息块（空的缓存页）
记载进free的时机：**

1.数据库启动时，会申请内存创建buffer pool，buffer pool分成一个个缓存页及其缓存页描述信息块，描述信息块加入到free链表中

**2.如果缓存页不够用了，会把lru冷数据区尾部的缓存页刷盘，清空；该缓存页从lru链表和flush链表中移除，加入到free链表中**

3.mysql后台线程也会定时把lru冷数据区尾部的缓存页刷盘，清空；定时把flush链表中的缓存页刷盘，清空，加入到free链表中链表上除了描述信息块，还有一个基础节点，存储了free链有多少个描述信息块，也就是有多少个空闲的缓存页当我们加载数据的时候，会从free链中找到空闲的缓存页，把数据页的表空间号和数据页号写入描述信息块；加载数据到缓存页后，会把缓存页对应的描述信息块从free链表中移除
在执行update的时候，会先修改buffer pool中的内存页，这时候就有了脏页，也就是磁盘和内存中数据不一致了，这里就要借助flush链表找到脏页，它也是一个双向链表，**链表结点是被修改过的缓存页的描述信息块**如果更新了缓存页，会把该缓存页加入到flush链表中和free链表一样，也有一个基础结点，链接首尾结点，并存储了有多少个描述信息块最后要把flush链表上结点对应的缓存页刷盘，后台线程会在MySQL不怎么繁忙的时候，找个时间把flush链表中的缓存页都刷入磁盘中，这样被你修改过的数据，迟早都会刷入磁盘的；缓存页从flush链表中移除，加入到free链表当中



![image-20220827152905266](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220827152905266.png)





## redo log buffer简介

接着思考一个问题，假设我们把 `Buffer Pool` 中某个数据页的某条数据修改了，但是硬盘的数据还未同步，此时数据是不一致的，如果 `MySQL` 宕机了，数据就丢失了。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nxwUmLaZQARHt8gs7q78SPAvPD9alC4mGJYfAFClvXYg5icfvcnM8jSBLg4ibqsAAzwCX1olcURadxw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这可怎么办呢。

为了保证数据的持久性，`InnoDB`存储引擎加入了 `redo` 日志功能，也叫重做日志。

每当我们对表数据进行更新时，会把“在某个数据页上做了什么修改”记录到重做日志缓存（`redo log buffer`）里。比如：将第0号表空间的100号⻚⾯的偏移量为1000处的值更新为2。

当事务提交时，会把 `redo log buffer` 清空，刷盘到 `redo` 日志文件。这样我们在事务提交时，把上述内容刷新到磁盘中，即使之后系统崩溃了，重启之后只要按照上述内容所记录的步骤重新更新⼀下数据⻚，那么该事务对数据库中所做的修改⼜可以被恢复出来，也就意味着满⾜持久性的要求。因为在系统奔溃重启时需要按照上述内容所记录的步骤重新更新数据⻚，所以上述内容也被称之为重做⽇志。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nxwUmLaZQARHt8gs7q78SPAibIVoLuk4JE6ICic0T1NOTuMbjVnDNFLrv0VVgIUZYvVrAqwFs5UXFWQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这样 `MySQL` 宕机了也没关系，因为重启后会根据 `redo` 日志去恢复数据。

![图片](https://mmbiz.qpic.cn/mmbiz_png/23OQmC1ia8nxwUmLaZQARHt8gs7q78SPAm1KKZY9ibzVzAVS7uNjot8PYzEo99x6yic5CCQHmJbY9hSPdjVlh2LqQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



小结

其实不难发现，缓冲池（`Buffer Pool`）和重做日志缓存（`redo log buffer`），它们都是为了减少硬盘 `IO` 开销。

redo log优点：

redo⽇志占⽤的空间⾮常⼩
存储表空间ID、⻚号、偏移量以及需要更新的值所需的存储空间是很⼩的，关于redo⽇志的格式我们稍后会详细唠叨，现在只要知道⼀条redo⽇志占⽤的空间不是很⼤就好了。

redo⽇志是顺序写⼊磁盘的
在执⾏事务的过程中，每执⾏⼀条语句，就可能产⽣若⼲条redo⽇志，这些⽇志是按照产⽣的顺序写⼊磁盘的，也就是使⽤顺序IO。

redo⽇志本质上只是记录了⼀下事务对数据库做了哪些修改。



# myslam innodb对比

MyISAM 支持以下三种类型的索引：

 1、B-Tree 索引
B-Tree 索引，顾名思义，就是所有的索引节点都按照balance tree 的数据结构来存储，所有的索引数据节点都在叶节点。
2、R-Tree 索引
R-Tree 索引的存储方式和b-tree 索引有一些区别，主要设计用于为存储空间和多维数据的字段做索引，所以目前的 MySQL 版本来说，也仅支持 geometry 类型的字段作索引。
3、Full-text 索引
Full-text 索引就是我们长说的全文索引，他的存储结构也是 b-tree。主要是为了解决在我们需要用like 查询的低效问题。

MyISAM 上面三种索引类型中，最经常使用的就是B-Tree 索引了，偶尔会使用到Full- text，但是R-Tree 索引一般系统中都是很少用到的。另外MyISAM 的B-Tree 索引有一个较大的限制，那就是参与一个索引的所有字段的长度之和不能超过 1000 字节

![image-20210826144107791](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210826144107791.png)

MyISAM的b+树索引图示

myisam索引是平行的关系

![image-20210826144334820](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210826144334820.png)

Innodb的b+树索引图示

只能创建一个聚集索引，最多创建255个非聚集索引

![image-20210826144634302](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210826144634302.png)

MyISAM 存储引擎的某个表文件出错之后，仅影响到该表，而不会影响到其他表，更不会影响到其他的数据库。如果我们的出据苦正在运行过程中发现某个 MyISAM 表出现问题了， 则可以在线通过check table 命令来尝试校验他，并可以通过 repair table 命令来尝试修复。在数据库关闭状态下，我们也可以通过 myisamchk 工具来对数据库中某个（或某些）表进行检测或者修复。不过强烈建议不到万不得已不要轻易对表进行修复操作，修复之前尽量做好可能的备份工作，以免带来不必要的后果。



Innodb 之所以能如此受宠，主要是在于其功能方面的较多特点： 

1、支持事务安全
Innodb 在功能方面最重要的一点就是对事务安全的支持，这无疑是让Innodb 成为MySQL 最为流行的存储引擎之一的一个非常重要原因。而且实现了 SQL92 标准所定义的所有四个级别（READ UNCOMMITTED，READ COMMITTED，REPEATABLE READ 和SERIALIZABLE）。对事务安全的支持，无疑让很多之前因为特殊业务要求而不得不放弃使用 MySQL 的用户转向支持MySQL，以及之前对数据库选型持观望态度的用户，也大大增加了对MySQL 好感。
2、数据多版本读取

Innodb 在事务支持的同时，为了保证数据的一致性已经并发时候的性能，通过对undo 信息，实现了数据的多版本读取。
3Innodb 改变了 MyISAM 的锁机制，实现了行锁。虽然 Innodb 的行锁机制的实现是通过索引来完成的，但毕竟在数据库中 99%的 SQL 语句都是要使用索引来做检索数据的。所以， 行锁定机制也无疑为Innodb 在承受高并发压力的环境下增强了不小的竞争力。



<font color="red">行锁</font>

*行锁是指上锁的时候锁住的是表的某一行或多行记录，其他事务访问同一张表时，只有被锁住的记录不能访问，其他的记录可正常访问；*
*特点：粒度小，加锁比表锁麻烦，不容易冲突，相比表锁支持的并发要高；*

<font color="red">间隙锁(Gap Lock)</font>

属于行锁中的一种，间隙锁是在事务加锁后其锁住的是表记录的某一个区间，当表的相邻ID之间出现空隙则会形成一个区间，遵循左开右闭原则。
范围查询并且查询未命中记录，查询条件必须命中索引、间隙锁只会出现在REPEATABLE_READ（重复读)的事务级别中。
触发条件：防止幻读问题，事务并发的时候，如果没有间隙锁，就会发生如下图的问题，在同一个事务里，A事务的两次查询出的结果会不一样。
比如表里面的数据ID 为 1,4,5,7,10 ,那么会形成以下几个间隙区间，-n-1区间，1-4区间，7-10区间，10-n区间 （-n代表负无穷大，n代表正无穷大）

<font color="red">临建锁</font>

也属于行锁的一种，并且它是INNODB的行锁默认算法，总结来说它就是记录锁和间隙锁的组合，临键锁会把查询出来的记录锁住，同时也会把该范围查询内的所有间隙空间也会锁住，再之它会把相邻的下一个区间也会锁住
触发条件：范围查询并命中，查询命中了索引。
结合记录锁和间隙锁的特性，临键锁避免了在范围查询时出现脏读、重复读、幻读问题。加了临键锁之后，在范围区间内数据不允许被修改和插入。



## 简述区别

\1. InnoDB支持事务，MyISAM不支持，对于InnoDB每一条SQL语言都默认封装成事务，自动提交，这样会影响速度，所以最好把多条SQL语言放在begin和commit之间，组成一个事务；
\2. InnoDB支持外键，而MyISAM不支持。对一个包含外键的InnoDB表转为MYISAM会失败；
\3. InnoDB主键是聚集索引，使用B+Tree作为索引结构，数据文件是和索引绑在一起的， MyISAM是非聚集索引，也是使用B+Tree作为索引结构，索引和数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。

4.InnoDB不保存表的具体行数，执行select count(*) from table时需要全表扫描。而MyISAM用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快



那么为什么InnoDB没有了这个变量呢？

    因为InnoDB的事务特性，在同一时刻表中的行数对于不同的事务而言是不一样的，因此count统计会计算对于当前事务而言可以统计到的行数，而不是将总行数储存起来方便快速查询。InnoDB会尝试遍历一个尽可能小的索引除非优化器提示使用别的索引。如果二级索引不存在，InnoDB还会尝试去遍历其他聚簇索引。
    如果索引并没有完全处于InnoDB维护的缓冲区（Buffer Pool）中，count操作会比较费时。可以建立一个记录总行数的表并让你的程序在INSERT/DELETE时更新对应的数据。和上面提到的问题一样，如果此时存在多个事务的话这种方案也不太好用。如果得到大致的行数值已经足够满足需求可以尝试SHOW TABLE STATUS

6.Innodb不支持全文索引，而MyISAM支持全文索引，在涉及全文索引领域的查询效率上MyISAM速度更快高；PS：5.7以后的InnoDB支持全文索引了

\7. InnoDB支持表、行(默认)级锁，而MyISAM只支持表级锁
8、InnoDB表必须有唯一索引（如主键）（用户没有指定的话会自己找/生产一个隐藏列Row_id来充当默认主键），而Myisam可以没有

一个InnoDb引擎存储在一个文件空间（共享表空间，表大小不受操作系统控制，一个表可能分布在多个文件里），也有可能为多个（设置为独立表空，表大小受操作系统文件大小限制，一般为2G），受操作系统文件大小的限制；
主键索引采用聚集索引（索引的数据域存储数据文件本身），辅索引的数据域存储主键的值；因此从辅索引查找数据，需要先通过辅索引找到主键值，再访问辅索引；最好使用自增主键，防止插入数据时，为维持B+树结构，文件的大调整。

![image-20220211191650601](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220211191650601.png)

## 日志文件

Innodb 的日志文件和Oracle 的redo 日志比较类似，同样可以设置多个日志组（最少 2个），同样采用轮循策略来顺序的写入，甚至在老版本中还有和 Oracle 一样的日志归档特性 。如
果你的数据库中有创建了Innodb 的表，那么千万别全部删除 innodb 的日志文件，因为很可能就会让你的数据库crash，无法启动，或者是丢失数据。

由于Innodb 是事务安全的存储引擎，所以系统Crash 对他来说并不能造成非常严重的损失，由于有redo 日志的存在，有checkpoint 机制的保护，Innodb 完全可以通过redo 日志将数据库Crash 时刻已经完成但还没有来得及将数据写入磁盘的事务恢复，也能够将所有部分完成并已经写入磁盘的未完成事务回滚并将数据还原。

Innodb 的在线redo 日志：innodb redo log
Innodb 是一个事务安全的存储引擎，其事务安全性主要就是通过在线 redo 日志和记录在表空间中的 undo 信息来保证的。**redo 日志中记录了Innodb 所做的所有物理变更和事务信息，通过 redo 日志和undo 信息，Innodb 保证了在任何情况下的事务安全性**。Innodb 的redo 日志同样默认存放在数据目录下，可以通过 innodb_log_group_home_dir 来更改设置日志的存放位置，通过innodb_log_files_in_group 设置日志的数量。



**2.是否支持事务**

MyISAM 不提供事务支持。

InnoDB 提供事务支持，具有提交(commit)和回滚(rollback)事务的能力。

**3.是否支持外键**

MyISAM 不支持，而 InnoDB 支持。

拓展一下：

一般我们也是不建议在数据库层面使用外键的，应用层面可以解决。不过，这样会对数据的一致性造成威胁。具体要不要使用外键还是要根据你的项目来决定

**4.是否支持数据库异常崩溃后的安全恢复**

MyISAM 不支持，而 InnoDB 支持。

使用 InnoDB 的数据库在异常崩溃后，数据库重新启动的时候会保证数据库恢复到崩溃前的状态。这个恢复的过程依赖于 `redo log` 

拓展一下：

MySQL InnoDB 引擎使用 **redo log(重做日志)** 保证事务的**持久性**，使用 **undo log(回滚日志)** 来保证事务的**原子性**。

MySQL InnoDB 引擎通过 **锁机制**、**MVCC** 等手段来保证事务的隔离性（ 默认支持的隔离级别是 **`REPEATABLE-READ`** ）。

保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。

《MySQL 高性能》上面有一句话这样写到:

不要轻易相信“MyISAM 比 InnoDB 快”之类的经验之谈，这个结论往往不是绝对的。在很多我们已知场景中，InnoDB 的速度都可以让 MyISAM 望尘莫及，尤其是用到了聚簇索引，或者需要访问的数据都可以放入内存的应用。





一般情况下我们选择 InnoDB 都是没有问题的，但是某些情况下你并不在乎可扩展能力和并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题的话，选择 MyISAM 也是一个不错的选择。但是一般情况下，我们都是需要考虑到这些问题的。

因此，对于咱们日常开发的业务系统来说，你几乎找不到什么理由再使用 MyISAM 作为自己的 MySQL 数据库的存储引擎。



## 使用redo日志



使用Redo日志记录数据变化，当数据库发生故障启动时可以根据Redo日志进行恢复（前滚）。这一点相信大家都知道，但是在深入分析时就会想到：恢复时究竟从哪一时刻开始？恢复到哪一点算完成？如何判断数据是否需要恢复？ 如果Redo日志足够大，则可以从Redo日志的开始位置扫描进行恢复。但是如果一个数据库运行了很久，Redo日志非常大，那么这样的数据库在做实例恢复时所需要的时间就会非常长，显然上面这种方式不可取，**所以在数据库中引入了CheckPoint（检查点）的概念。** 《性能优化金字塔法则》
19.3.4　CheckPoint概念 

为了减少数据库做实例恢复时的时间，引入了CheckPoint的概念。在MySQL中存在一个递增的序列数字（Log Sequence Number，LSN）LSN可被理解为Redo日志中操作的时间点。 在数据库运行中，当Buffer Pool（缓冲池）中的脏数据达到一定比例时会进行刷新操作，当上一次CheckPoint的值和当前LSN的差值达到一定比例时会进行CheckPoint操作，将CheckPoint的值向前推。这样在CheckPoint的值之前的数据我们可以认为是安全落盘的，在做实例恢复应用Redo日志时只需要从CheckPoint开始进行扫描恢复即可，减少了实例恢复的时间。 在数据库中存在两种CheckPoint方式。 

1．Sharp CheckPoint（全量检查点），这种方式发生在数据库关闭时，当发生CheckPoint操作时，数据库停止所有操作，将Buffer Pool中的脏数据刷新到磁盘上，结束时将LSN1作为CheckPoint的值，恢复时从LSN1开始进行恢复。但是在数据库运行中我们肯定不能接受这种方式，因为会影响数据库的正常使用。 

 2．Fuzzy CheckPoint（模糊检查点） 这种方式与前一种方式不同的地方在于发生CheckPoint操作时允许数据库操作，这时将LSN1作为CheckPoint的值，在这个期间将数据的变化记录到Redo日志中，同时有可能会一并将脏数据刷新到磁盘中。如图19-2所示，在发生CkeckPoint操作期间，数据P1、P3发生了变化，产生了对应的Redo日志记录R1、R2，并且最新的数据也被刷新到磁盘中，当数据库做实例恢复时，会从CheckPoint的值也就是LSN1处开始恢复文件，这里面还会涉及一个问题就是幂等，幂等就是反复执行多次结果都是相同的。根据前面所讲的，Redo日志不完全是物理日志，如果重复执行就会违反幂等；逻辑语句执行多次会造成数据不一致，所以数据库在做实例恢复时会进行LSN的比较，如果页面上对应的LSN大于或等于Redo日志中的LSN，则跳过，因为此时页面数据已经是最新的了。 



## 锁机制





**MyISAM 和 InnoDB 存储引擎使用的锁：**

- MyISAM 采用表级锁(table-level locking)。
- InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁

表级锁和行级锁对比：

- **表级锁：** MySQL 中锁定 **粒度最大** 的一种锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM 和 InnoDB 引擎都支持表级锁。该锁定机制最大的特点是实现逻辑非常简单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。由于表级锁一次会将整个表锁定，所以可以很好的避免困扰我们的死锁问题。
  当然，锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并大度大打折扣。
- **行级锁：** MySQL 中锁定 **粒度最小** 的一种锁，只针对当前操作的行进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。
- 页级锁定（page-level）
  页级锁定是 MySQL 中比较独特的一种锁定级别，在其他数据库管理软件中也并不是太常见。页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力也同样是介于上面二者之间。另外，页级锁定和行级锁定一样，会发生死锁。
  在数据库实现资源锁定的过程中，随着锁定资源颗粒度的减小，锁定相同数据量的数据所需要消耗的内存数量是越来越多的，实现算法也会越来越复杂。不过，随着锁定资源颗粒度的减小，应用程序的访问请求遇到锁等待的可能性也会随之降低，系统整体并发度也随之提升。
  在MySQL 数据库中，使用表级锁定的主要是MyISAM，Memory，CSV 等一些非事务性存储引擎，而使用行级锁定的主要是Innodb 存储引擎和NDB Cluster 存储引擎，页级锁定主要是BerkeleyDB 存储引擎的锁定方式。
- MySQL 的如此的锁定机制主要是由于其最初的历史所决定的。在最初，MySQL 希望设计一种完全独立于各种存储引擎的锁定机制，而且在早期的MySQL 数据库中，MySQL 的存储引擎（MyISAM 和 Momery）的设计是建立在“任何表在同一时刻都只允许单个线程对其访问（包括读）”这样的假设之上。但是，随着MySQL 的不断完善，系统的不断改进，在MySQL3.23 版本开发的时候，MySQL 开发人员不得不修正之前的假设。因为他们发现一个线程正在读某个表的时候，另一个线程是可以对该表进行insert 操作的，只不过只能INSERT 到数据文件的最尾部。==这也就是从MySQL 从3.23 版本开始提供的我们所说的Concurrent Insert==。
  当出现Concurrent Insert 之后，MySQL 的开发人员不得不修改之前系统中的锁定实现功能，但是仅仅只是增加了对 Concurrent Insert 的支持，并没有改动整体架构。可是在不久之后，随着 BerkeleyDB 存储引擎的引入，之前的锁定机制遇到了更大的挑战。因为BerkeleyDB存储引擎并没有MyISAM 和Memory 存储引擎同一时刻只允许单一线程访问某一个表的限制，而是将这个单线程访问限制的颗粒度缩小到了单个page，这又一次迫使MySQL 开发人员不得不再一次修改锁定机制的实现。由于新的存储引擎的引入，导致锁定机制不能满足要求，让 MySQL 的人意识到已经不可能实现一种完全独立的满足各种存储引擎要求的锁定实现机制。如果因为锁定机制的拙劣实现而导致存储引擎的整体性能的下降，肯定会严重打击存储引擎提供者的积极性，这是 MySQL 公司非常不愿意看到的，因为这完全不符合 MySQL 的战略发展思路。所以工程师们不得不放弃了最初的设计初衷，在锁定实现机制中作出修改，允许存储引擎自己改变MySQL 通过接口传入的锁定类型而自行决定该怎样锁定数据。

**InnoDB 存储引擎的锁的算法有三种：**

- Record lock：记录锁，单个行记录上的锁

- Gap lock：间隙锁，锁定一个范围，不包括记录本身

- 间隙锁 是 Innodb 在 RR(可重复读) 隔离级别 下为了解决幻读问题时引入的锁机制。间隙锁是innodb中行锁的一种。

  请务必牢记：使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据。

  举例来说，假如emp表中只有101条记录，其empid的值分别是1,2,...,100,101，下面的SQL：

   SELECT * FROM emp WHERE empid > 100 FOR UPDATE
  当我们用条件检索数据，并请求共享或排他锁时，InnoDB不仅会对符合条件的empid值为101的记录加锁，也会对empid大于101（这些记录并不存在）的“间隙”加锁。

  这个时候如果你插入empid等于102的数据的，如果那边事物还没有提交，那你就会处于等待状态，无法插入数据。
  临键锁（Next-Key Locks）
  Next-key锁是记录锁和间隙锁的组合，它指的是加在某条记录以及这条记录前面间隙上的锁。

  也可以理解为一种特殊的间隙锁。通过临建锁可以解决幻读的问题。 每个数据行上的非唯一索引列上都会存在一把临键锁，当某个事务持有该数据行的临键锁时，会锁住一段左开右闭区间的数据。需要强调的一点是，InnoDB 中行级锁是基于索引实现的，临键锁只与非唯一索引列有关，在唯一索引列（包括主键列）上不存在临键锁。

- Next-key lock：record+gap临键锁，锁定一个范围，包含记录本身

**缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。** 因此，开启查询缓存要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十 MB 比较合适。此外，**还可以通过 sql_cache 和 sql_no_cache 来控制某个查询语句是否需要缓存：**

-----------------------------------------------------------------------------------------------------



虽然对于我们这些使用者来说 MySQL 展现出来的锁定（表锁定）只有读锁定和写锁定这两种类型， 但是在MySQL 内部实现中却有多达11 种锁定类型，由系统中一个枚举量（thr_lock_type）定义，

![image-20210809142837866](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210809142837866.png)

读锁定
一个新的客户端请求在申请获取读锁定资源的时候，需要满足两个条件： 

1、 请求锁定的资源当前没有被写锁定；
2、 写锁定等待队列（Pending write-lock queue）中没有更高优先级的写锁定等待；
如果满足了上面两个条件之后，该请求会被立即通过，并将相关的信息存入 Current read-lock queue 中，而如果上面两个条件中任何一个没有满足，都会被迫进入等待队列Pending read-lock queue 中等待资源的释放。

写锁定
当客户端请求写锁定的时候，MySQL 首先检查在 Current write-lock queue 是否已经有锁定相同资源的信息存在。
如果 Current write-lock queue 没有， 则再检查 Pending write-lock queue， 如果在 Pending write-lock queue 中找到了，自己也需要进入等待队列并暂停自身线程等待锁定资源。反之，如果Pending write-lock queue 为空，则再检测 Current read-lock queue，如果有锁定存在，则同样需要进入Pending write-lock queue 等待。当然，也可能遇到以下这两种特殊情况：
1. 请求锁定的类型为WRITE_DELAYED;

2. 请求锁定的类型为 WRITE_CONCURRENT_INSERT 或者是 TL_WRITE_ALLOW_WRITE， 同时Current read lock 是READ_NO_INSERT 的锁定类型。
    当遇到这两种特殊情况的时候，写锁定会立即获得而进入Current write-lock queue 中
    如果刚开始第一次检测就Current write-lock queue 中已经存在了锁定相同资源的写锁定存在，那么就只能进入等待队列等待相应资源锁定的释放了。

  

lock table user read;
update user set pwd = name;

![image-20210809144300811](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210809144300811.png)

unlock tables;



行级锁定不是 MySQL 自己实现的锁定方式，而是由其他存储引擎自己所实现的，如广为大家所知的Innodb 存储引擎，以及MySQL 的分布式存储引擎NDB Cluster 等都是实现了行级锁定。



## myisam的锁

命令local table xx(表名) read local;

此时会对本地会话加读锁，且当前会话不能进行插入和更新操作，但另一个会话可以进行插入操作，但是这个加了锁的会话在select时读不到另一个会话插入的数据，其他会话进行更新操作时会阻塞

解锁命令：unlock tables;

如果不加local变量，则另一个会话无法进行插入，加了锁的会话也无法进行插入，且也无法读取其他数据库

myiam在执行查询之前会自动给涉及到的所有表加读锁，在执行更新操作之前，会自动给所涉及的表加写锁，并不需要用户显式进行操作





## 存储引擎优化



无论是对于哪一种数据库来说，缓存技术都是提高数据库性能的关键技术，物理磁盘的访问速度永远都会与内存的访问速度永远都不是一个数量级的。通过缓存技术无论是在读还是写方面都可以大大提高数据库整体性能

Innodb_buffer_pool_size 的合理设置
Innodb 存储引擎的缓存机制和MyISAM 的最大区别就在于**Innodb 不仅仅缓存索引，同时还会缓存实际的数据**。所以，完全相同的数据库，使用Innodb 存储引擎可以使用更多的内存来缓存数据库相关的信息，当然前提是要有足够的物理内存。这对于在现在这个内存价格不断降低的时代，无疑是个很吸引人的特性。

注意　对于MyISAM存储引擎表，MySQL数据库只缓存其索引文件，数据文件的缓存交由操作系统本身来完成，这与其他使用LRU算法缓存数据的大部分数据库大不相同。此外，在MySQL 5.1.23版本之前，无论是在32位还是64位操作系统环境下，缓存索引的缓冲区最大只能设置为4GB。在之后的版本中，64位系统可以支持大于4GB的索引缓冲区。

innodb_buffer_pool_size 参数用来设置Innodb 最主要的Buffer(Innodb_Buffer_Pool)的大小，也就是缓存用户表及索引数据的最主要缓存空间，对Innodb 整体性能影响也最大。无论是MySQL 官方手册还是网络上很多人所分享的Innodb 优化建议，都简单的建议将Innodb 的 Buffer Pool 设置为整个系统物理内存的 50% ～ 80% 之间。如此轻率的给出此类建议，我个人觉得实在是有些不妥。

就从Innodb 的Buffer Pool 到底该设置多大这个问题来看，我们首先需要确定的是这台主机是不是就只提供MySQL 服务？MySQL 需要提供的的最大连接数是多少？MySQL 中是否还有MyISAM 等其他存储引擎提供服务？如果有，其他存储引擎所需要使用的Cache 需要多大？

假设是一台单独给MySQL 使用的主机，物理内存总大小为 8G，MySQL 最大连接数为 500，同时还使用了MyISAM 存储引擎，这时候我们的整体内存该如何分配呢？
内存分配为如下几大部分：
a) 系统使用，假设预留 800M；
b) 线程独享，约 2GB = 500 * (1MB + 1MB + 1MB + 512KB + 512KB)，组成大概如下: sort_buffer_size：1MB
join_buffer_size：1MB read_buffer_size：1MB read_rnd_buffer_size：512KB thread_statck：512KB
c) MyISAM Key Cache，假设大概为 1.5GB；
d) Innodb Buffer Pool 最大可用量：8GB - 800MB - 2GB - 1.5GB = 3.7GB；

上面只是一个简单的示例分析，实际情况并不一定是这样的，这里只是希望大家了解，在设置一些参数的时候，千万不要想当然，一定要详细的分析可能出现的情况，然后再通过不断测试调整来达到自己所处环境的最优配置。就我个人而言，正式环境上线之初，我一般都会采取相对保守的参数配置策 略。上线之后，再根据实际情况和收集到的各种性能数据进行针对性的调整。

当系统上线之后，我们可以通过Innodb 存储引擎提供给我们的关于Buffer Pool 的实时状态信息作出进一步分析，来确定系统中Innodb 的Buffer Pool 使用情况是否正常高效：
sky@localhost : example 08:47:54> show status like 'Innodb_buffer_pool_%';

innodb_log_buffer_size 参数的使用

顾名思义，这个参数就是用来设置Innodb 的Log Buffer 大小的，系统默认值为 1MB。Log Buffer 的主要作用就是缓冲Log 数据，提高写Log 的IO 性能。一般来说，如果你的系统不是写负载非常高且以大事务居多的话，8MB 以内的大小就完全足够了。
我们也可以通过系统状态参数提供的性能统计数据来分析Log 的使用情况： sky@localhost : example 10:11:05> show status like 'innodb_log%';
+---------------------------+-------+
| Variable_name | Value |
+---------------------------+-------+
| Innodb_log_waits | 0 |
| Innodb_log_write_requests | 6 |
| Innodb_log_writes | 2 |
+---------------------------+-------+
通过这三个状态参数我们可以很清楚的看到Log Buffer 的等待次数等性能状态。
当然，如果完全从Log Buffer 本身来说，自然是大一些会减少更多的磁盘IO。但是由于Log 本身是为了保护数据安全而产生的，而Log 从Buffer 到磁盘的刷新频率和控制数据安全一致的事务直接相关， 并且也有相关参数来控制（innodb_flush_log_at_trx_commit），所以关于Log 相关的更详细的实现机制和优化在后面的“事务优化”中再做更详细的分析，这里就不展开了。







# innodb详解



由于主键是聚族索引的缘故，Innodb 的基于主键的查询效率非常高。如果我们在创建一个Innodb 存储引擎的表的时候并没有创建主键，那么Innodb 会尝试在创建于我们表上面的其他索引，如果存在由单个 not null 属性列的唯一索引，Innodb 则会选择该索引作为聚族索引。如果也没有任何单个 not null 属性列的唯一索引，Innodb 会自动生成一个隐藏的内部列，该列会在每行数据上占用 6 个字节的存储长度。所以，**实质上每个Innodb 表都至少会有一个索引存在。**

在 Innodb 上面出了聚族索引之外的索引被称为 secondary index，每个 secondary index 上都会包含有聚族索引的索引键信息，方便通过其他索引查找数据的时候能够更快的定位数据位置所在。

当然，聚族索引也并不是只有好处没有任何问题，要不然其他所有数据库早就大力推广了。==聚族索引的最大问题就是当索引键被更新的时候，所带来的成本并不仅仅只是索引数据可能会需要移动，而是相关的所有记录的数据都需要移动。所以，为了性能考虑，我们应该尽可能不要更新Innodb 的主键值。==

Page
Innodb 存储引擎中的所有数据，不论是表还是索引，亦或是存储引擎自己的各种结构，都是以page 作为最小物理单位来存放，每个page 默认大小为 16KB。
extent
extent 是一个由多个连续的page 组成一个物理存储单位。一般来说，每个extent 为 64 个page。
segment
segment 在 Innodb 存储引擎中实际上也代表“files”的意思，每个 segment 由一个或多个 extent 组成，而且每个segment 都存放同一种数据。一般来说，==每个表数据会存放于一个单独的segment 中，实际上也就是每个聚族索引会存放于一个单独的segment 中。==
tablespace
tablespace 是 Innodb 中最大物理结构单位了，由多个segment 组成。

当tablespace 中的某个segment 需要增长的时候，Innodb 最初仅仅分配某一个extent 的前32 个pages， 然后如果继续增长才会分配整个 extent 来使用。我们还可以通过执行如下命令来查看 Innodb 表空间的使用情况：
sky@localhost : example 01:26:43> SHOW TABLE STATUS like 'test'\G

===========================================================================================================================================================================================================================================================================

虽然每个索引页（index page）大小为 16KB，但是实际上 Innodb 在第一次使用该 page 的时候，如果是一个顺序的索引插入，都会预留 1KB 的空间。而如果是随机插入的话，那么大约会使用 (8- 15/16) KB 的空间，而如果一个 Index page 在进行多次删除之后如果所占用的空间已经低于 8KB（1/2）的话， Innodb 会通过一定的收缩机制收缩索引，并释放该 index page。此外，每个索引记录中都存放了一个 6 字节的头信息，主要用于行锁定时候的记录以及各个索引记录的关联信息。
<font color="red">Innodb 在存放数据页的时候不仅仅只是存放我们实际定义的列，同时还会增加两个内部隐藏列，</font>其中一个隐含列的信息主要为事务相关信息，会占用6 个字节的长度。另外一个则占用7 字节长度，主要用来存放一个指向 Undo Log 中的 Undo Segment 的指针相关信息，主要用于事务回滚，以及通过 Undo Segment 中的信息构造多版本数据页。
通过上面的信息，我们至少可以得出以下几点对性能有较大影响的地方：

1. 为了尽量减小 secondary index 的大小，提高访问效率，作为主键的字段所占用的存储空间越小越好，最好是INTEGER 类型。当然这并不是绝对的，字符串类型的数据同样也可以作为Innodb 表的主键；
2. 创建表的时候尽量自己指定相应的主键，让数据按照自己预设的顺序排序存放，一提高特定条件下的访问效率；
3. 尽可能不要在主键上面进行更新操作，减少因为主键值的变化带来数据的移动。
4. 尽可能提供主键条件进行查询；



InnoDB存储引擎对传统的LRU算法做了一些优化。因为如果直接将最新读取的页放到LRU列表的首部，那么某些SQL操作（比如全表扫描或者索引扫描）如果要访问很多页，甚至是全部页（通常来说仅在此次查询操作中需要，并不是活跃的热点数据），很可能会将活跃的热点数据挤出LRU列表。在下一次需要读取这些热点数据页时，InnoDB存储引擎则需要再一次从磁盘读取。所以InnoDB存储引擎的LRU列表中还加入了midpoint位置，即新读取的页并不是直接放到LRU列表首部，而是放到LRU列表的midpoint位置。这个算法在InnoDB存储引擎下称为midpoint insertion strategy，默认该位置在LRU列表长度的5/8处。midpoint的位置可由参数innodb_old_blocks_pct控制，在图2-3中，innodb_old_blocks_pct默认值为37，表示新读取的页插入到LRU列表尾端37%的位置。在InnoDB存储引擎中，把midpoint之后的列表称为old列表，之前的列表称为new列表。new列表的数据可以简单理解为都是活跃的热点数据。InnoDB还有一个参数innoDB_old_blocks_time，表示页读取到midpoint位置后需要等待多久才会被加入到LRU列表的热端。 

MySQL 5.6之后的版本提供了一个新特性来快速预热buffer_pool缓冲池，如图2-4所示。参数innodb_buffer_pool_dump_at_shutdown=ON表示在关闭MySQL时会把内存中的热数据保存在磁盘里ib_buffer_pool文件中，其保存比率由参数innodb_buffer_pool_dump_pct控制，默认为25%。参数innodb_buffer_pool_load_at_startup=ON表示在启动时会自动加载热数据到Buffer_Pool缓冲池里。这样，始终保持热数据在内存中。 

![image-20210927230342809](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210927230342809.png)







## Innodb 还有其他一些可能影响到性能的参数设置：

◆ Innodb_flush_method
用来设置Innodb 打开和同步数据文件以及日志文件的方式，不过只有在Linux & Unix 系统上面有效。系统默认值为fdatasync，即 Innodb 默认通过 fsync()来 flush 数据和日志文件数据。
此外，还可以设置为 O_DSYNC 和 O_DIRECT，当我们设置为 O_DSYNC，则系统以 O_SYNC 方式打开和刷新日志文件， 通过 fsync() 来打开和刷新数据文件。而设置为 O_DIRECT 的时候， 则通过O_DIRECT（Solaris 上为 directio()）打开数据文件，同时以 fsync()来刷新数据和日志文件。
总的来说，innodb_flush_method 的不同设置主要影响的是Innodb 在不同运行平台下进行IO 操作的时候所调用的操作系统IO 借口的区别。而不同的IO 操作接口对数据的处理方式会有一定的区别，所以处理性能也会有一定的差异。一般来说，如果我们的磁盘是通过RAID 卡做了硬件级别的 RAID，建议可以使用 O_DIRECT，可以一定程度上提高IO 性能，但如果RAID Cache 不够的话，还是需要谨慎对待。此外，根据MySQL 官方手册上面的介绍，如果我们的存储环境是SAN 环境，使用 O_DIRECT 有可能会反而使性能降低。对于支持 O_DSYNC 的平台，也可以尝试设置为O_DSYNC 方式看是否能对写IO 性能有所帮助。

◆ innodb_thread_concurrency
这个参数主要控制Innodb 内部的并发处理线程数量的最大值，系统内部会有相应的检测机制进行检测控制并发线程数量，Innodb 建议设置为CPU 个数与磁盘个数之和。但是这个参数一直是一个非常有争议的参数，而且还有一个非常著名的BUG（#15815）一直被认为就于innodb_thread_concurrency 参数所控制的内容相关。从该参数在系统中的默认值的变化我们也可以看出即使是Innodb 开发人员也并不是很清楚到底该将innodb_thread_concurrency 设置为多少合适。在MySQL5.0.8 之前，默认值为 8，从MySQL5.0.8 开始到MySQL5.0.18，默认值又被更改为 20，然后在MySQL5.0.19 和MySQL5.0.20 两个版本中又默认设置为 0。之后，从MySQL5.0.21 开始默认值再次被更改回 8。
innodb_thread_concurrency 参数的设置范围是 0～1000，但是在MySQL5.0.19 之前的版本，只要该值超过20，Innodb 就会认为不需要对并发线程数做任何限制，也就是说Innodb 不会再进行并行线程的数目检查。同样，我们也可以通过设置为 0 来禁用并行线程检查，完全让Innodb 自己根据实际需要创建并行线程，而且在不少场景下设置为 0 还是一个非常不错的选择，尤其是当系统写IO 压力较大的时候。
总的来说，innodb_thread_concurrency 参数的设置并没有一个很好的规则来判断什么场景该设置多大，完全需要通过不断的调整尝试，寻找出适合自己应用的设置。
◆ autocommit

autocommit 的用途我想大家应该都很清楚，就是当我们将该参数设置为true(1)之后，在我们每次执行完一条会修改数据的Query 之后，系统内部都会自动提交该操作，==基本上可以理解为屏蔽了事务的概念。==
设置aotucommit 为true（1）之后，我们的提交相对于自己手工控制commit 时机来说可能会变得要频繁很多。这样带来的直接影响就是Innodb 的事务日志可能会需要非常频繁的执行磁盘同步操作，当然还与innodb_flush_log_at_trx_commit 参数的设置相关。
一般来说，在我们通过LOAD ... INFILE ... 或者其他的某种方式向Innodb 存储引擎的表加载数据的时候，==将autocommit 设置为false 可以极大的提高加载性能。==而在正常的应用中，也最好尽量通过自行控制事务的提交避免过于频繁的日志刷新来保证性能。

Innodb 性能监控
我们可以通过执行“SHOW INNODB STATUS”命令来获取比较详细的系统当前Innodb 性能状态，如下：
sky@localhost : example 03:11:19> show engine innodb status;

![image-20210818143114447](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210818143114447.png)

![image-20210818143137764](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210818143137764.png)

我们可以看到整个信息分为 7 个部分，分别说明如下：
◆ SEMAPHORES，这部分主要显示系统中当前的信号等待信息以及各种等待信号的统计信息，这部分输出的信息对于我们调整innodb_thread_concurrency 参数有非常大的帮助，当等待信号量非常大的时候，可能就需要禁用并发线程检测设置innodb_thread_concurrency=0；
◆ TRANSACTIONS，这里主要展示系统的锁等待信息和当前活动事务信息。通过这部分输出，我们可以查追踪到死锁的详细信息；
◆ FILE I/O，文件IO 相关的信息，主要是IO 等待信息；
◆ INSERT BUFFER AND ADAPTIVE HASH INDEX；显示插入缓存当前状态信息以及自适应Hash Index 的状态；
◆ LOG，Innodb 事务日志相关信息，包括当前的日志序列号（Log Sequence Number），已经刷新同步到哪个序列号，最近的Check Point 到哪个序列号了。除此之外，还显示了系统从启动到现在已经做了多少次Ckeck Point，多少次日志刷新；
◆ BUFFER POOL AND MEMORY，这部分主要显示Innodb Buffer Pool 相关的各种统计信息，以及其他一些内存使用的信息；
◆ ROW OPERATIONS，顾名思义，主要显示的是与客户端的请求Query 和这些Query 所影响的记录统计信息。





## 后台线程



![image-20210729230343470](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210729230343470.png)

图2-1简单显示了InnoDB的存储引擎的体系架构，从图可见，InnoDB存储引擎有多个内存块，可以认为这些内存块组成了一个大的内存池，负责如下工作： 维护所有进程/线程需要访问的多个内部数据结构。 缓存磁盘上的数据，方便快速地读取，同时在对磁盘文件的数据修改之前在这里缓存。 重做日志（redo log）缓冲。

后台线程的主要作用是负责刷新内存池中的数据，保证缓冲池中的内存缓存的是最近的数据。此外将已修改的数据文件刷新到磁盘文件，同时保证在数据库发生异常的情况下InnoDB能恢复到正常运行状态。

后台线程 InnoDB存储引擎是多线程的模型，因此其后台有多个不同的后台线程，负责处理不同的任务。 

1. ==Master Thread==

   Master Thread是一个非常核心的后台线程，主要负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性，包括脏页的刷新、合并插入缓冲（INSERT BUFFER）、UNDO页的回收等。2.5节会详细地介绍各个版本中Master Thread的工作方式。 

   2. ==IO Thread== 在InnoDB存储引擎中大量使用了AIO（Async IO）来处理写IO请求，这样可以极大提高数据库的性能。而IO Thread的工作主要是负责这些IO请求的回调（call back）处理。InnoDB 1.0版本之前共有4个IO Thread，分别是write、read、insert buffer和log IO thread。在Linux平台下，IO Thread的数量不能进行调整，但是在Windows平台下可以通过参数innodb_file_io_threads来增大IO Thread。从InnoDB 1.0.x版本开始，read thread和write thread分别增大到了4个，并且不再使用innodb_file_io_threads参数，而是分别使用innodb_read_io_threads和innodb_write_io_threads参数进行设置，如：

   ```sql
   mysql> show variables like'innodb_version'\G;
   *************************** 1. row ***************************
   Variable_name: innodb_version
           Value: 5.7.26
   mysql> show variables like'innodb_%io_threads'\G;
   mysql> show engine innodb status\G;
   ```

   ![image-20210729231535058](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210729231535058.png)

3. ==Purge Thread== 

   事务被提交后，其所使用的undolog可能不再需要，因此需要PurgeThread来回收已经使用并分配的undo页。在InnoDB 1.1版本之前，purge操作仅在InnoDB存储引擎的Master Thread中完成。而从InnoDB 1.1版本开始，purge操作可以独立到单独的线程中进行，以此来减轻Master Thread的工作，从而提高CPU的使用率以及提升存储引擎的性能。用户可以在MySQL数据库的配置文件中添加如下命令来启用独立的Purge Thread： [mysqld] innodb_purge_threads=1 在InnoDB 1.1版本中，即使将innodb_purge_threads设为大于1，InnoDB存储引擎启动时也会将其设为1，并在错误文件中出现如下类似的提示： 120529 22：54：16[Warning]option'innodb-purge-threads'：unsigned value 4 adjusted to 1 从InnoDB 1.2版本开始，InnoDB支持多个Purge Thread，这样做的目的是为了进一步加快undo页的回收。同时由于Purge Thread需要离散地读取undo页，这样也能更进一步利用磁盘的随机读取性能。如用户可以设置4个Purge Thread

4. \  Page Cleaner Thread 

   Page Cleaner Thread是在InnoDB 1.2.x版本中引入的。其作用是将之前版本中脏页的刷新操作都放入到单独的线程中来完成。而其目的是为了减轻原Master Thread的工作及对于用户查询线程的阻塞，进一步提高InnoDB存储引擎的性能。 



## 内存

Innodb 存储引擎通过缓存技术，==将常用数据和索引缓存到内存中==，这样我们在读取数据或者索引的时候就可以尽量减少物理IO 来提高性能。那我们修改数据的时候Innodb 是如何处理的呢，是否修改数据的时候Innodb 是不是象我们常用的应用系统中的缓存一样，更改缓存中的数据的同时，将更改同时应用到相应的数据持久化系统中？

可能很多人都会有上面的这个疑问。实际上，Innodb 在修改数据的时候同样也只是修改Buffer Pool 中的数据，并不是在一个事务提交的时候就将BufferPool 中被修改的数据同步到磁盘，而是通过另外一种支持事务的数据库系统常用的手段，将修改信息记录到相应的事务日志中。
为什么不是直接将Buffer Pool 中被修改的数据直接同步到磁盘，还有记录一个事务日志呢，这样不是反而增加了整体IO 量了么？是的，对于系统的整体IO 量而言，确实是有所增加。但是，对于系统的整体性能却有很大的帮助。

这里我们需要理解关于磁盘读写的两个概念：连续读写和随机读写。简单来说，磁盘的顺序读写就是将数据顺序的写入连续的物理位置，而随机读写则相反，数据需要根据各自的特定位置被写入各个位置，也就是被写入了并不连续的物理位置。对于磁盘来说，写入连续的位置最大的好处就是磁头所做的寻址动作很少，而磁盘操作中最耗费时间的就是磁头的寻址。所以，在磁盘操作中，连续读写操作比随即读写操作的性能要好很多。
我们的应用所修改的Buffer Pool 中的数据都很随机，每次所做的修改都是一个或者少数几个数据页，多次修改的数据页也很少会连续。如果我们每次修改之后都将Buffer Pool 中的数据同步到磁盘， 那么磁盘就只能一直忙于频繁的随机读写操作。而事务日志在创建之初就是申请的连续的物理空间，而且每次写入都是紧接着之前的日志数据顺序的往后写入，基本上都是一个顺序的写入过程。所以，日志的写入操作远比同步Buffer Pool 中被修改的数据要更快。
当然，由于事务日志都是通过几个日志文件轮循反复写入，而且每个日志文件大小固定，即使再多 的日志也会有旧日志被新产生的日志覆盖的时候。所以，Buffer Pool 中的数据还是不可避免的需要被刷新到磁盘上进行持久化，而且这个持久化的动作必须在旧日志被新日志覆盖之前完成。只不过，随着被更新的数据（Dirty Buffer）的增加，需要刷新的数据的连续性就越高，所需要做的随机读写也就越少，自然，IO 性能也就得到了提升。

而且事务日志本身也有Buffer（log buffer），每次事务日志的写入并不是直接写入到文件，也都是暂时先写入到log buffer 中，然后再在一定的事件触发下才会同步到文件。当然，为了尽可能的减少事务日志的丢失，我们可以通过innodb_log_buffer_size 参数来控制log buffer 的大小。关于事务日志何时同步的说明稍后会做详细分析。
事务日志文件的大小与Innodb 的整体IO 性能有非常大的关系。理论上来讲，日志文件越大，则Buffer Pool 所需要做的刷新动作也就越少，性能也越高。但是，我们也不能忽略另外一个事情，那就是当系统Crash 之后的恢复。
==事务日志的作用主要有两个，一个就是上面所提到的提高系统整体IO 性能，另外一个就是当系统Crash 之后的恢复。==

总的来说，Innodb 的事务日志文件设置的越大，系统的IO 性能也就越高，但是当遇到MySQL ，OS 或者主机 Crash 的时候系统所需要的恢复时间也就越长；反之，日志越小，IO 性能自然也就相对会差一些，但是当MySQL，OS 或者主机 Crash 之后所需要的恢复时间也越小。所以，到底该将事务日志设置多大其实是一个整体权衡的问题，既要考虑到系统整体的性能，又要兼顾到 Crash 之后的恢复时间。一般来说，在我个人维护的环境中，比较偏向于将事务日志设置为3 组，每个日志设置为256MB 大小，整体效果还算不错。



------------------------------------------------------------------------------------------------------

## 缓冲池：

InnoDB存储引擎有各种缓冲池（Buffer Pool），这些缓冲块组成了一个大的InnoDB存储引擎内存池，主要负责的工作是：维护所有进程/线程需要访问的多个内部数据结构；缓存磁盘上的数据，方便快速读取，同时在对磁盘文件修改之前进行缓存；重做日志缓存等。

1. 缓冲池 InnoDB存储引擎是基于磁盘存储的，并将其中的记录按照页的方式进行管理。因此可将其视为基于磁盘的数据库系统（Disk-base Database）。在数据库系统中，**由于CPU速度与磁盘速度之间的鸿沟，基于磁盘的数据库系统通常使用缓冲池技术来提高数据库的整体性能**。 缓冲池简单来说就是一块内存区域，通过内存的速度来弥补磁盘速度较慢对数据库性能的影响。在数据库中进行读取页的操作，首先将从磁盘读到的页存放在缓冲池中，这个过程称为将页“FIX”在缓冲池中。下一次再读相同的页时，首先判断该页是否在缓冲池中。若在缓冲池中，称该页在缓冲池中被命中，直接读取该页。否则，读取磁盘上的页。 对于数据库中页的修改操作，则首先修改在缓冲池中的页，然后再以一定的频率刷新到磁盘上。这里需要注意的是，**页从缓冲池刷新回磁盘的操作并不是在每次页发生更新时触发，而是通过一种称为Checkpoint的机制刷新回磁盘。**同样，这也是为了提高数据库的整体性能。 综上所述，缓冲池的大小直接影响着数据库的整体性能。由于32位操作系统的限制，在该系统下最多将该值设置为3G。此外用户可以打开操作系统的PAE选项来获得32位操作系统下最大64GB内存的支持。随着内存技术的不断成熟，其成本也在不断下降。单条8GB的内存变得非常普遍，而PC服务器已经能支持512GB的内存。因此为了让数据库使用更多的内存，强烈建议数据库服务器都采用64位的操作系统。 对于InnoDB存储引擎而言，其缓冲池的配置通过参数innodb_buffer_pool_size来设置。下面显示一台MySQL数据库服务器，其将InnoDB存储引擎的缓冲池设置为15GB。  







查看缓冲池大小：

```xml
mysql> show variables like'innodb_buffer_pool_size'\G;
```

![image-20210729232502499](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210729232502499.png)



具体来看，缓冲池中缓存的数据页类型有：索引页、数据页、undo页、插入缓冲（insert buffer）、自适应哈希索引（adaptive hash index）、InnoDB存储的锁信息（lock info）、数据字典信息（data dictionary）等。不能简单地认为，缓冲池只是缓存索引页和数据页，它们只是占缓冲池很大的一部分而已。图2-2很好地显示了InnoDB存储引擎中内存的结构情况。 

![image-20210729232612290](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210729232612290.png)

从InnoDB 1.0.x版本开始，允许有多个缓冲池实例。每个页根据哈希值平均分配到不同缓冲池实例中。这样做的好处是减少数据库内部的资源竞争，增加数据库的并发处理能力。可以通过参数innodb_buffer_pool_instances来进行配置，该值默认为1。 从InnoDB 1.0.x版本开始，允许有多个缓冲池实例。每个页根据哈希值平均分配到不同缓冲池实例中。这样做的好处是减少数据库内部的资源竞争，增加数据库的并发处理能力。可以通过参数innodb_buffer_pool_instances来进行配置，该值默认为1。 在配置文件中将innodb_buffer_pool_instances设置为大于1的值就可以得到多个缓冲池实例。

2. LRU List、Free List和Flush List   

在前一小节中我们知道了缓冲池是一个很大的内存区域，其中存放各种类型的页。那么InnoDB存储引擎是怎么对这么大的内存区域进行管理的呢？这就是本小节要告诉读者的。 **通常来说，数据库中的缓冲池是通过LRU（Latest Recent Used，最近最少使用）算法来进行管理的**。即最频繁使用的页在LRU列表的前端，而最少使用的页在LRU列表的尾端。当缓冲池不能存放新读取到的页时，将首先释放LRU列表中尾端的页。 在InnoDB存储引擎中，缓冲池中页的大小默认为16KB，同样使用LRU算法对缓冲池进行管理。稍有不同的是InnoDB存储引擎对传统的LRU法做了一些优化。在InnoDB的存储引擎中，LRU列表中还加入了midpoint位置。新读取到的页，虽然是最新访问的页，但并不是直接放入到LRU列表的首部，而是放入到LRU列表的midpoint位置。这个算法在InnoDB存储引擎下称为midpoint insertion strategy。在默认配置下，该位置在LRU列表长度的5/8处。midpoint位置可由参数innodb_old_blocks_pct控制，如：  

![image-20210729234153659](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210729234153659.png)

（报错ERROR 2013 (HY000): Lost connection to MySQL server during query

输入

mysql> set global connect_timeout=60

mysql>  show variables like'innodb_old_blocks_pct'\G;

从上面的例子可以看到，参数innodb_old_blocks_pct默认值为37，表示新读取的页插入到LRU列表尾端的37%的位置（差不多3/8的位置）。在InnoDB存储引擎中，把midpoint之后的列表称为old列表，之前的列表称为new列表。可以简单地理解为new列表中的页都是最为活跃的热点数据。  

## MySQL中的change buffer

**写缓存(change buffer)**

　　对于读请求，缓冲池能够减少磁盘IO，提升性能。问题来了，**那写请求呢？**

MySQL的一条语句，大致流程查看内存→读取磁盘数据页→返回数据。

当比如查找一个 a=5的记录的时候，并不是只查找出这一条数据，它所在的整个数据页都会查找出来(每个数据页16KB)。

下次查找a=6的记录的时候，发现该页已经在内存中了，直接返回，不需要磁盘IO。

但是当时增、删、改操作时，并不会每一次操作都进行一次磁盘IO，使用change buffer可以降低磁盘随机IO。

**change buffer首先是可持久化的数据。**

当更新某个数据页时，该页在内存中，那么直接更新。

如果该页不在内存中，那么先将更新操作记录在change buffer中，这时不需要从磁盘中读出数据页。

将操作记录到redo log中，防止机器意外关闭导致数据丢失。

这时已经可以返回给客户端更新成功了，因为即使机器意外重启，也可以通过redo log找回数据。

change buffer 使用的是 buffer pool里的内存，不能无限增大，可以通过参数 innodb_change_buffer_max_xize来动态设置，这个参数为50的时候，标识change buffer 的大小最多只能占用 buffer pool 的50%。

**什么时候将change buffer中的数据更新到磁盘中？**

当下一次查询命中这个数据页的时候，会先从磁盘中读取数据页到内存中，然后先执行change buffer的merge操作，保证数据逻辑的正确性。除了查询操作外，系统有后台线程会定期merge，数据库正常关闭(shutdown)的时候，也会进行merge操作。

## innodb锁机制



总的来说，Innodb 的锁定机制和Oracle 数据库有不少相似之处。Innodb 的行级锁定同样分为两种类型，共享锁和排他锁，而在锁定机制的实现过程中为了让行级锁定和表级锁定共存， Innodb 也同样使用了意向锁（表级锁定）的概念，也就有了意向共享锁和意向排他锁这两种。

![image-20210809144808175](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210809144808175.png)

之前在第一节中我们已经介绍过，行级锁定肯定会带来死锁问题， Innodb 也不可能例外。至于死锁的产生过程我们就不在这里详细描述了,这里主要介绍一下，在Innodb 中当系检测到死锁产生之后是如何来处理的。
==在Innodb 的事务管理和锁定机制中，有专门检测死锁的机制，会在系统中产生死锁之后的很短时间内就检测到该死锁的存在==。当Innodb 检测到系统中产生了死锁之后，Innodb 会通过相应的判断来选这产生死锁的两个事务中较小的事务来回滚，而让另外一个较大的事务成功完成。那Innodb 是以什么来为标准判定事务的大小的呢？MySQL 官方手册中也提到了这个问题，实际上在 Innodb 发现死锁之后，会计算出两个事务各自插入、更新或者删除的数据量来判定两个事务的大小。也就是说哪个事务所改变的记录条数越多，在死锁中就越不会被回滚掉。但是有一点需要注意的就是，当产生死锁的场景中涉及到不止Innodb 存储引擎的时候，Innodb 是没办法检测到该死锁的，这时候就只能通过锁定超时限制来解决该死锁了。

InnoDB存储引擎实现了4种行锁，分别是共享锁、排他锁、意向共享锁、意向排他锁。下面进一步学习这4种行锁的知识。 首先，使用共享锁和排他锁必须要满足以下几个条件。

 （1）设置autocommit的值是OFF或者0。 

（2）表的数据引擎是支持事务的，比如InnoDB数据引擎。 

（3）如果不管autocommit，手动在事务里执行操作，这个时候要使用begin或者start transaction开始事务。 

（4）不要在锁定事务规定的时间外使用共享锁和排他锁。 

![image-20210912135214520](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210912135214520.png)

![image-20210912135244515](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210912135244515.png)





​                 表22.10　一个InnoDB存储引擎共享锁的例子 

MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通
过SELECT @@tx_isolation; 命令来查看
mysql> SELECT @@tx_isolation;
+-----------------+
| @@tx_isolation |
+-----------------+
| REPEATABLE-READ |
+-----------------+
这里需要注意的是：与 SQL 标准不同的地方在于 **InnoDB 存储引擎在 REPEATABLE-READ（可重读） 事务隔离级别下使用的是Next-Key Lock 锁算法，因此可以避免幻读的产生**，这与其他数据库
系统(如 SQL Server) 是不同的。所以说InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLEREAD（可重读） 已经可以完全保证事务的隔离性要求，即达到了 SQL标准的 SERIALIZABLE(可串行化) 隔离级别。因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容) ，但是你要知道的是InnoDB 存储引擎默认使用REPEAaTABLE-READ（可重读） 并不会有任何性能损失。
InnoDB 存储引擎在 分布式事务 的情况下一般会用到 SERIALIZABLE(可串行化) 隔离级别。

## innodb的行锁算法

**InnoDB有三种行锁的算法：**

1，Record Lock：单个行记录上的锁。

2，Gap Lock：间隙锁，锁定一个范围，但不包括记录本身。GAP锁的目的，是为了防止同一事务的两次当前读，出现幻读的情况。

两边都是开区间(1,10)

3，Next-Key Lock：1+2，锁定一个范围，并且锁定记录本身。对于行的查询，都是采用该方法，主要目的是解决幻读的问题。

 前开后闭区间 （5,10]，（10,15]

如student表中有seq为1 3 5 7四条记录，那么间隙是这样的：

即有以下范围：
（-∞,1]，(1,3]，(3,5]，(5,7]，(7,+∞）

select * from student where seq = 5 for update;
#锁定范围(3, 7）
如果此时另外一个事务要插入：
insert into student (name, seq) values (‘tony’, 6); 是会被阻塞的，因为6在（3, 7]里面。


但是如果insert into student (name, seq) values (‘tony’, 8);是可以执行成功的。

Next-Key locks锁是record锁 + Gaps的组合，上面只能锁(3, 7)这个区间，但是Next-Key locks是左开右闭的区间，即（3, 7],这样就能防止当前记录出现幻读。

另外间隙锁直接是不会冲突的，间隙锁必须在事务隔离级别是可重复读才开启的，如果是提交读RC级别，间隙锁默认是禁用的。也是为了防止幻读的出现才有的间隙锁。
精确的讲锁主要分为两类：
S锁，也称共享锁。S锁之间不冲突。
X锁，也称为独占锁，X锁只能一个事务获取，其他事务需要阻塞等待。

![image-20230422140518365](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20230422140518365.png)

### 乐观锁

用数据版本（Version）记录机制实现，这是乐观锁最常用的一种实现方式。何谓数据版本？即
为数据增加一个版本标识，一般是通过为数据库表增加一个数字类型的 “version” 字段来实
现。当读取数据时，将version字段的值一同读出，数据每更新一次，对此version值加1。当我
们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比
对，如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期
数据。

乐观锁
**乐观锁不是数据库自带的，需要我们自己去实现**。乐观锁是指操作数据库时(更新操作)，想法很乐观，认为这次的操作不会导致冲突，在操作数据时，并不进行任何其他的特殊处理（也就是不加锁），而在进行更新后，再去判断是否有冲突了。

通常实现是这样的：在表中的数据进行操作时(更新)，先给数据表加一个版本(version)字段，每操作一次，将那条记录的版本号加1。也就是先查询出那条记录，获取出version字段,如果要对那条记录进行操作(更新),则先判断此刻version的值是否与刚刚查询出来时的version的值相等，如果相等，则说明这段期间，没有其他程序对其进行操作，则可以执行更新，将version字段的值加1；如果更新时发现此刻的version值与刚刚获取出来的version的值不相等，则说明这段期间已经有其他程序对其进行操作了，则不进行更新操作。

举例：

下单操作包括3步骤：

1.查询出商品信息

select (status,status,version) from t_goods where id=#{id}

2.根据商品信息生成订单

3.修改商品status为2

update t_goods set status=2,version=version+1 where id=#{id} and version=#{version};

### 悲观锁

在进行每次操作时都要通过获取锁才能进行对相同数据的操作，这点跟java中synchronized很
相似，**共享锁（读锁）和排它锁（写锁）是悲观锁的不同的实现**



### 共享锁（读锁）

共享锁又叫做读锁，所有的事务只能对其进行读操作不能写操作，加上共享锁后在事务结束之前
其他事务只能再加共享锁，除此之外其他任何类型的锁都不能再加了。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。

如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获得共享锁的事务只能读数据，不能修改数据

打开第一个查询窗口

```sql
begin;/begin work;/start transaction;  (三者选一就可以)
SELECT * from TABLE where id = 1  lock in share mode;
```

然后在另一个查询窗口中，对id为1的数据进行更新

```sql
update  TABLE set name="www.souyunku.com" where id =1;
```

此时，操作界面进入了卡顿状态，过了超时间，提示错误信息

如果在超时前，执行 `commit`，此更新语句就会成功。

在查询语句后面增加 **`LOCK IN SHARE MODE`**，Mysql会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。





### 排它锁（写锁）
若某个事务对某一行加上了排他锁，只能这个事务对其进行读写，在此事务结束之前，其他事务
不能对其进行加任何锁，其他进程可以读取,不能进行写操作，需等待其释放。

若事务 1 对数据对象A加上X锁（即排他锁），事务 1 可以读A也可以修改A，其他事务不能再对A加任何锁，直到事物 1 释放A上的锁。这保证了其他事务在事物 1 释放A上的锁之前不能再读取和修改A。排它锁会阻塞所有的排它锁和共享锁

读取为什么要加读锁呢：防止数据在被读取的时候被别的线程加上写锁，

使用方式：在需要执行的语句后面加上`for update`就可以了

select status from TABLE where id=1 for update;

例1: (明确指定主键，并且数据真实存在，row lock)

```
SELECT * FROM user WHERE id=3 FOR UPDATE;

SELECT * FROM user WHERE id=3 and name='Tom' FOR UPDATE;
```

例2: (明确指定主键，但数据不存在，无lock)

```
SELECT * FROM user WHERE id=0 FOR UPDATE;
```

例3: (主键不明确，table lock)

```
SELECT * FROM user WHERE id<>3 FOR UPDATE;

SELECT * FROM user WHERE id LIKE '%3%' FOR UPDATE;
```

例4: (无主键，table lock)

```
SELECT * FROM user WHERE name='Tom' FOR UPDATE;
```

表级锁
innodb 的行锁是在有索引的情况下,没有索引的表是锁定全表的
行级锁
行锁又分共享锁和排他锁,由字面意思理解，就是给某一行加上锁，也就是一条记录加上锁。
注意：行级锁都是基于索引的，**如果一条SQL语句用不到索引是不会使用行级锁的，会使用表级锁。**

比如之前演示的共享锁语句

SELECT * from city where id = "1"  lock in share mode; 

由于对于city表中,id字段为主键，就也相当于索引。执行加锁时，会将id这个索引为1的记录加上锁，那么这个锁就是行锁。

# 什么是临建锁？（Next-Key）  

当使用范围条件而不是相等条件检索数据的时候，并请求共享或排它锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，称为“间隙（GAP）”，InnoDB也会对这个“间隙”加锁，这种锁机制就是所谓的间隙（Next-Key）锁。间隙锁是InnoDB中行锁的一种，但是这种锁锁住的不止一行数据，它锁住的是多行，是一个数据范围。间隙锁的主要作用是为了防止出现幻读（Phantom Read），用在Repeated-Read（简称RR）隔离级别下。在Read-Commited（简称RC）下，一般没有间隙锁（有外键情况下例外，此处不考虑）。间隙锁还用于恢复和复制。 

间隙锁的出现主要集中在同一个事务中先DELETE后INSERT的情况下，当通过一个条件删除一条记录的时候，如果条件在数据库中已经存在，那么这个时候产生的是普通行锁，即锁住这个记录，然后删除，最后释放锁。如果这条记录不存在，那么问题就来了，数据库会扫描索引，发现这个记录不存在，这个时候的DELETE语句获取到的就是一个间隙锁，然后数据库会向左扫描，扫到第一个比给定参数小的值，向右扫描，扫描到第一个比给定参数大的值，然后以此为界，构建一个区间，锁住整个区间内的数据，一个特别容易出现死锁的间隙锁诞生了。

 在MySQL的InnoDB存储引擎中，**如果更新操作是针对一个区间的，那么它会锁住这个区间内所有的记录，例如UPDATE XXX WHERE ID BETWEEN A AND B**，那么它会锁住A到B之间所有记录，注意是所有记录，甚至如果这个记录并不存在也会被锁住，在这个时候，如果另外一个连接需要插入一条记录到A与B之间，那么它就必须等到上一个事务结束。典型的例子就是使用AUTO_INCREMENT ID，由于这个ID是一直往上分配的，因此，当两个事务都INSERT时，会得到两个不同的ID，但是这两条记录还没有被提交，因此，也就不存在，如果这个时候有一个事务进行范围操作，而且恰好要锁住不存在的ID，就是触发间隙锁问题。所以，MySQL中尽量不要使用区间更新。

InnoDB除了通过范围条件加锁时使用间隙锁外，如果使用相等条件请求给一个不存在的记录加锁，那么InnoDB也会使用间隙锁！ 间隙锁也存在副作用，它会把锁定范围扩大，有时候也会带来麻烦。如果要关闭，那么一是将会话隔离级别改到RC下，或者开启innodb_locks_unsafe_for_binlog（默认是OFF）。**间隙锁只会出现在辅助索引(二级索引）上，唯一索引和主键索引是没有间隙锁。**间隙锁（无论是S还是X）只会阻塞INSERT操作。 在MySQL数据库参数中，控制间隙锁的参数是innodb_locks_unsafe_for_binlog，这个参数的默认值是OFF，也就是启用间隙锁，它是一个布尔值，当值为TRUE时，表示DISABLE间隙锁。 



# 隔离级别与锁的关系

s锁：share锁

1. 在Read Uncommitted级别下，读操作不加S锁；
2. 在Read Committed级别下，读操作需要加S锁，但是在语句执行完以后释放S锁；
3. 在Repeatable Read级别下，读操作需要加S锁，**但是在事务提交之前并不释放S锁，也就是必须等待事务执行完毕以后才释放S锁。**
4. 在Serialize级别下，会在Repeatable Read级别的基础上，添加一个范围锁。保证一个事务内的两次查询结果完全一样，而不会出现第一次查询结果是第二次查询结果的子集

x锁：排他锁

# delete from t1 where id = 10；MySQL会加什么锁？

如果要分析上述SQL的加锁情况，必须了解这个SQL的执行前提，MySQL的隔离级别是什么？id列是不是主键？id列有没有索引？前提不同加锁处理的方式也不同。

**可能的情况：**

- id列是不是主键？
- MySQL的隔离级别是什么？
- id列如果不是主键，那么id列上有索引吗？
- id列上如果有二级索引，那么这个索引是唯一索引吗？
- SQL的执行计划是什么？索引扫描？全表扫描？

**根据上述情况，有以下几种组合：**

- id列是主键，RC隔离级别
- id列是二级唯一索引，RC隔离级别
- id列是二级非唯一索引，RC隔离级别
- id列上没有索引，RC隔离级别
- id列是主键，RR隔离级别
- id列是二级唯一索引，RR隔离级别
- id列是二级非唯一索引，RR隔离级别
- id列上没有索引，RR隔离级别

**组合一：id主键+RC**

大部分系统都是Read Committed隔离级别，id列是主键，这种情况只需要将主键上，id = 10的记录加上X锁，也就是排他锁即可。

![image-20220302210737277](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220302210737277.png)

**结论：id是主键时，此SQL只需要在id=10这条记录上加X锁即可。**

**组合二：id唯一索引+RC**

id是一个Unique的二级索引键值。那么在RC隔离级别下，delete from t1 where id = 10; 需要加什么锁呢？

见下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/KtOtv1NYGMicdmwh3tc4ZnTfn9xoxSlkI2XicP3cEvqsE06odU1aMicuVQ9q0sKgF85h7teT3moib3UXQthaMo986A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

name列是主键，id列是唯一索引。此时，加锁的情况由于组合一有所不同。

由于id是唯一索引，因此delete语句会选择走id列的索引进行where条件的过滤，在找到id=10的记录后，首先会将唯一索引上的id=10索引记录加上X锁，同时，会根据读取到的name列，回主键索引，然后将聚簇索引上的name = ‘d’ 对应的主键索引项加X锁。

**结论：若id列上有唯一索引。那么SQL需要加两个X锁，一个对应于id唯一索引上的id = 10的记录，另一把锁对应于聚簇索引上的[name='d',id=10]的记录**

**组合三：id非唯一索引+RC**

id列只有一个普通的索引，那么此时会持有哪些锁？

见下图：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/KtOtv1NYGMicdmwh3tc4ZnTfn9xoxSlkIdzmZuOeP2LibOsPPRaCIicLLpMqulbxPLJQAAj7wAgibr3McYJyTRZe9A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

首先，id列索引上，满足id = 10查询条件的记录，均已加锁。同时，这些记录对应的主键索引上的记录也都加上了锁。与组合二唯一的区别在于，组合二最多只有一个满足等值查询的记录，而组合三会将所有满足查询条件的记录都加锁。

**结论：若id列上有非唯一索引，那么对应的所有满足SQL查询条件的记录，都会被加锁。同时，这些记录在主键索引上的记录，也会被加锁。**

**组合四：id无索引+RC**

id列上没有索引，where id = 10;这个过滤条件，没法通过索引进行过滤，那么只能走全表扫描做过滤。对应于这个组合，SQL会加什么锁？

见下图：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/KtOtv1NYGMicdmwh3tc4ZnTfn9xoxSlkIZIDq7klU5gsCpDrwlMdvur7k2MvwwBbD4PiaAyqQbsQlwC0TE5UTYGw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

由于id列上没有索引，因此只能走聚簇索引，进行全部扫描。从图中可以看到，满足删除条件的记录有两条，但是，聚簇索引上所有的记录，都被加上了X锁。无论记录是否满足条件，全部被加上X锁。既不是加表锁，也不是在满足条件的记录上加行锁。

有人可能会问？为什么不是只在满足条件的记录上加锁呢？这是由于MySQL的实现决定的。如果一个条件无法通过索引快速过滤，那么存储引擎层面就会将所有记录加锁后返回，然后由MySQL Server层进行过滤。因此也就把所有的记录，都锁上了。

**结论：若id列上没有索引，SQL会走聚簇索引的全扫描进行过滤，由于过滤是由MySQL Server层面进行的。因此每条记录，无论是否满足条件，都会被加上X锁。但是，为了效率考量，MySQL做了优化，对于不满足条件的记录，会在判断后放锁，最终持有的，是满足条件的记录上的锁，但是不满足条件的记录上的加锁/放锁动作不会省略。同时，优化也违背了2PL的约束。**

**组合五：id主键+RR**

id列是主键列，Repeatable Read隔离级别，针对delete from t1 where id = 10; 这条SQL，加锁与组合一：[id主键，Read Committed]一致。

**组合六：id唯一索引+RR**

组合六的加锁，与组合二：[id唯一索引，Read Committed]一致。两个X锁，id唯一索引满足条件的记录上一个，对应的聚簇索引上的记录一个。

**组合七：id非唯一索引+RR**

在RR隔离级别下，为了防止幻读的发生，会使用Gap锁。这里，你可以把Gap锁理解为，不允许在数据记录前面插入数据。对应于这个组合，SQL会加什么锁

见下图：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/KtOtv1NYGMicdmwh3tc4ZnTfn9xoxSlkIMFc1bJOvrJttyfgogU38qH3t8pAxCFYsMq8qtTG83ViaGkRVACchicNA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)


**结论：Repeatable Read隔离级别下，id列上有一个非唯一索引，对应SQL：delete from t1 where id = 10; 首先，通过id索引定位到第一条满足查询条件的记录，加记录上的X锁，加GAP上的GAP锁，然后加主键聚簇索引上的记录X锁，然后返回；然后读取下一条，重复进行。直至进行到第一条不满足条件的记录[11,f]，此时，不需要加记录X锁，但是仍旧需要加GAP锁，最后返回结束。**

什么时候会取得gap lock或nextkey lock 这和隔离级别有关,只在REPEATABLE READ或以上的隔离级别下的特定操作才会取得gap lock或nextkey lock。

**组合八：id无索引+RR**

Repeatable Read隔离级别下的最后一种情况，id列上没有索引。此时SQL：delete from t1 where id = 10; 没有其他的路径可以选择，只能进行全表扫描。最终的加锁情况，

见下图：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/KtOtv1NYGMicdmwh3tc4ZnTfn9xoxSlkIZQ18iaCFF9MudhfKmzpIh6hsh4ciafJhMBysOezzq1cnbaOub2G7hDIw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

**结论：在Repeatable Read隔离级别下，如果进行全表扫描的当前读，那么会锁上表中的所有记录，同时会锁上聚簇索引内的所有GAP，杜绝所有的并发 更新/删除/插入 操作。当然，也可以通过触发semi-consistent read，来缓解加锁开销与并发影响，但是semi-consistent read本身也会带来其他问题，不建议使用。**

# 事务





## 介绍

一言蔽之，**事务是逻辑上的一组操作，要么都执行，要么都不执行。**

**可以简单举一个例子不？**

事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账 1000 元，这个转账会涉及到两个关键操作就是：

1. 将小明的余额减少 1000 元
2. 将小红的余额增加 1000 元。

事务会把这两个操作就可以看成逻辑上的一个整体，这个整体包含的操作要么都成功，要么都要失败。

这样就不会出现小明余额减少而小红的余额却并没有增加的情况。



## 设置隔离级别命令

select @@global.transaction_isolation,@@transaction_isolation; （查看全局/会话隔离级别）

SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;（设置隔离级别为 可重复读）

SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;（设置隔离级别为 读已提交）



## [何为数据库事务？](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=何为数据库事务？)

数据库事务在我们日常开发中接触的最多了。如果你的项目属于单体架构的话，你接触到的往往就是数据库事务了。

平时，我们在谈论事务的时候，如果没有特指**分布式事务**，往往指的就是**数据库事务**。

**那数据库事务有什么作用呢？**

简单来说：数据库事务可以保证多个对数据库的操作（也就是 SQL 语句）构成一个逻辑上的整体。构成这个逻辑上的整体的这些数据库操作遵循：**要么全部执行成功,要么全部不执行** 。

```sql
# 开启一个事务
START TRANSACTION;
# 多条 SQL 语句
SQL1,SQL2...
## 提交事务
COMMIT;
```



![image-20210705011105865](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210705011105865.png)

1. **原子性**（`Atomicity`） ： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；

2. **一致性**（`Consistency`）： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；

   也可以说：一致性是事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后顺序都是合法的数据状态。数据库的完整性约束包括但不限于：

   实体完整性；列完整性；外键约束；用户自定义完整性（如转账前后两个账户的余额和应该不变）

3. **隔离性**（`Isolation`）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的，不互相干扰；

4. **持久性**（`Durabilily`）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。



## ACID是如何保证的

![image-20210923100733501](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923100733501.png)

从数据库层面，数据库通过原子性、隔离性、持久性来保证一致性。也就是说ACID四大特性之中，C(一致性)是目的，A(原子性)、I(隔离性)、D(持久性)是手段，是为了保证一致性，数据库提供的手段。数据库必须要实现AID三大特性，才有可能实现一致性。例如，原子性无法保证，显然一致性也无法保证。




## 什么是XA事务？

  XA（eXtended Architecture）是指由X/Open组织提出的分布式交易处理的规范。XA是一个分布式事务协议，由Tuxedo提出，所以，分布式事务也称为XA事务。XA协议主要定义了事务管理器（TM，Transaction Manager，协调者）和资源管理器（RM，Resource Manager，参与者）之间的接口。其中，资源管理器往往由数据库实现，例如Oracle、DB2、MySQL，这些商业数据库都实现了XA接口，而事务管理器作为全局的调度者，负责各个本地资源的提交和回滚**。XA事务是基于两阶段提交（Two-phase Commit，2PC）协议实现的，可以保证数据的强一致性**，许多分布式关系型数据管理系统都采用此协议来完成分布式。阶段一为准备阶段，即所有的参与者准备执行事务并锁住需要的资源。当参与者Ready时，向TM汇报自己已经准备好。阶段二为提交阶段。当TM确认所有参与者都Ready后，向所有参与者发送COMMIT命令。 XA事务允许不同数据库的分布式事务，只要参与在全局事务中的每个节点都支持XA事务。Oracle、MySQL和SQL Server都支持XA事务。

 ● XA事务由一个或多个资源管理器（RM）、一个事务管理器（TM）以及一个应用程序（Application Program）组成。 

● 资源管理器：提供访问事务资源的方法。通常一个数据库就是一个资源管理器。 

● 事务管理器：协调参与全局事务中的各个事务。需要和参与全局事务的所有资源管理器进行通信。

 ● 应用程序：定义事务的边界。==XA事务的缺点是性能不佳，且XA无法满足高并发场景。一个数据库的事务和多个数据库间的XA事务性能会相差很多。因此，要尽量避免使用XA事务，例如可以将数据写入本地，用高性能的消息系统分发数据，或使用数据库复制等技术==。只有在其他办法都无法实现业务需求，且性能不是瓶颈时才使用XA。



## 丢失更新

![image-20210831130324638](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210831130324638.png)

​                                        表6-2　第二类丢失更新 

注意T5时刻提交的事务。因为在事务1中，无法感知事务2的操作，这样它就不知道事务2已经修改过了数据，因此它依旧认为只是发生了一笔业务，所以库存变为了99，而这个结果又是一个错误的结果。这样，T5时刻事务1提交的事务，就会引发事务2提交结果的丢失，我们把这样的多个事务都提交引发的丢失更新称为第二类丢失更新。这是我们互联网系统需要关注的重点内容。为了克服这些问题，数据库提出了事务之间的隔离级别的概念，这就是本章的重点内

上面我们讨论了第二类丢失更新。为了压制丢失更新，数据库标准提出了4类隔离级别，在不同的程度上压制丢失更新，这4类隔离级别是未提交读、读写提交、可重复读和串行化，它们会在不同的程度上压制丢失更新的情景。 也许你会有一个疑问，都全部消除丢失更新不就好了吗，为什么只是在不同的程度上压制丢失更新呢？其实这个问题是从两个角度去看的，一个是数据的一致性，另一个是性能。数据库现有的技术完全可以避免丢失更新，但是这样做的代价，就是付出锁的代价，在互联网中，系统不单单要考虑数据的一致性，还要考虑系统的性能。试想，在互联网中使用过多的锁，一旦出现商品抢购这样的场景，必然会导致大量的线程被挂起和恢复，因为使用了锁之后，一个时刻只能有一个线程访问数据，这样整个系统就会十分缓慢，当系统被数千甚至数万用户同时访问时，过多的锁就会引发宕机，大部分用户线程被挂起，等待持有锁事务的完成，这样用户体验就会十分糟糕。因为用户等待的时间会十分漫长，一般而言，互联网系统响应超过 5 秒，就会让用户觉得很不友好，进而引发用户忠诚度下降的问题。所以选择隔离级别的时候，既需要考虑数据的一致性避免脏数据，又要考虑系统性能的问题。因此数据库的规范就提出了 4 种隔离级别来在不同的程度上压制丢失更新。







## 数据库隔离级别

![image-20210831193401502](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210831193401502.png)

**.为什么mysql用的是repeatable而不是read committed:在 5.0之前binlog只有statement一种格式，而主从复制存在了大量的不一致，故选用repeatable**

为什么其他数据库默认的隔离级别都会选用read commited 原因有二：repeatable存在间隙锁会使死锁的概率增大，在RR隔离级别下，条件列未命中索引会锁表！而在RC隔离级别下，只锁行



说一说脏读、不可重复读、幻读?
脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据；
不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果因此本事务先后两次读到的数据结果会不一致；
幻读：事务以范围查询的条件读取同一批数据时，事务B在事务A多次读取过程中，进行了数据的新增并提交操作，导致事务A读取数据不一致问题。

| 隔离级别         | 隔离级别的值 | 导致的问题                                                   |
| :--------------- | :----------- | :----------------------------------------------------------- |
| Read-Uncommitted | 0            | 导致脏读                                                     |
| Read-Committed   | 1            | 避免脏读，允许不可重复读和幻读                               |
| Repeatable-Read  | 2            | 避免脏读，不可重复读，允许幻读（innodb的next-key锁避免了幻读） |
| Serializable     | 3            | 串行化读，事务只能一个一个执行，避免了脏读、不可重复读、幻读。执行效率慢，使用时慎重 |

脏读：一事务对数据进行了增删改，但未提交，另一事务可以读取到未提交的数据。如果第一个事务这时候回滚了，那么第二个事务就读到了脏数据。

示例：

![image-20210831130733816](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210831130733816.png)

脏读一般是比较危险的隔离级别，在我们实际应用中采用得不多。为了克服脏读的问题，数据库隔离级别还提供了读写提交（read commited）的级别



不可重复读：一个事务中发生了两次读操作，第一次读操作和第二次操作之间，另外一个事务对数据进行了修改，这时候两次读取的数据是不一致的。

幻读：第一个事务对一定范围的数据进行批量修改，第二个事务在这个范围增加一条数据，这时候第一个事务就会丢失对新增数据的修改。

脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是在同一个事务范围内多次查询同一条数据却返回了不同的数据值，这是由于在查询间隔期间，该条数据被另一个事务修改并提交了。 幻读和不可重复读的区别为：幻读和不可重复读都是读取了另一个事务中已经提交的数据，不同的是不可重复读查询的都是同一个数据项，而幻读针对的是一个数据整体（例如数据的条数）。 在SQL标准中定义了4种隔离级别，每一种级别都规定了一个事务中所做的修改，哪些是在事务内和事务间可见的，哪些是不可见的。较低级别的隔离通常可以执行更高的并发，系统的开销也更低。SQL标准定义的四个隔离级别为：Read Uncommitted（未提交读）、Read Committed（提交读）、Repeatable Read（可重复读）、Serializable（可串行化），下面分别介绍。 

（1）Read Uncommitte读，读取未提交内容） 在该隔离级别，所有事务都可以看到其他未提交事务的执行结果，即在未提交读级别，事务中的修改，即使没有提交，对其他事务也都是可见的，该隔离级别很少用于实际应用。读取未提交的数据，也被称之为脏读（Dirty Read）。该隔离级别最低，并发性能最高。 

（2）Read Committed（提交读，读取提交内容） 这是大多数数据库系统的默认隔离级别。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。换句话说，一个事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的。 

提交读克服脏读的情形：

![image-20210831130959503](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210831130959503.png)

在T3时刻，由于采用了读写提交的隔离级别，因此事务2不能读取到事务1中未提交的库存1，所以扣减库存的结果依旧为1，然后它提交事务，则库存在T4时刻就变为了1。T5时刻，事务1回滚，因为第一类丢失更新已经克服，所以最后结果库存为1，这是一个正确的结果。但是读写提交也会产生下面的问题，如表6-5所描述的场景（不可重复读）。

 ![image-20210831131038950](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210831131038950.png)

（3）Repeatable Read（可重复读） 可重复读可以确保同一个事务，在多次读取同样数据的时候，得到同样的结果。可重复读解决了脏读的问题，不过理论上，这会导致另一个棘手的问题：幻读（Phantom Read）==MySQL数据库中的InnoDB和Falcon存储引擎通过MVCC（Multi-Version Concurrent Control，多版本并发控制）机制解决了快照读的幻读问题==。需要注意的是，多版本一般地说解决了不可重复读问题，但并不是解决了所有幻读，加上临建锁（也就是它这里所谓的并发控制）才解决了当前读的幻读问题。 

![image-20210831131206047](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210831131206047.png)

可以看到，事务2在T3时刻尝试读取库存，但是此时这个库存已经被事务1事先读取，所以这个时候数据库就阻塞它的读取，直至事务1提交，事务2才能读取库存的值。此时已经是T5时刻，而读取到的值为0，这时就已经无法扣减了，显然在读写提交中出现的不可重复读的场景被消除了。但是这样也会引发新的问题的出现，这就是幻读。假设现在商品交易正在进行中，而后台有人也在进行查询分析和打印的业务，让我们看看 

![image-20210831131551538](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210831131551538.png)

这便是幻读现象。可重复读和幻读，是读者比较难以理解的内容，这里稍微论述一下。首先这里的笔数不是数据库存储的值，而是一个统计值，商品库存则是数据库存储的值，这一点是要注意的。也就是幻读不是针对一条数据库记录而言，而是多条记录，例如，这51笔交易笔数就是多条数据库记录统计出来的。而可重复读是针对数据库的单一条记录，例如，商品的库存是以数据库里面的一条记录存储的，它可以产生可重复读，而不能产生幻读。 

（4）Serializable（可串行化、序列化） 这是最高的隔离级别，它通过强制事务排序，强制事务串行执行，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，==可能出现大量的超时现象和锁竞争。实际应用中也很少用到这个隔离级别，只有在非常需要确保数据的一致性而且可以接受没有并发的情况下，才考虑用该级别==。这是花费代价最高但是最可靠的事务隔离级别。 

查看当前会话的隔离级别：

select @@tx_isolation；

+-----------------+
| @@tx_isolation  |
+-----------------+
| REPEATABLE-READ |
+-----------------+







----------------------------------------------------------------------------------------------

读未提交（READ UNCOMMITTED）：未提交读隔离级别也叫读脏，就是事务可以读取其它事务未提交的数据。
读已提交（READ COMMITTED）：在其它数据库系统比如SQL Server 默认的隔离级别就是提交读，已提交隔离级别就是在事务未提交之前所做的修改其它事务是不可见的。
可重复读（REPEATABLE READ）：保证同一个事务中的多次相同的查询的结果是一致的，比如一个事务一开始查询了一条记录然后过了几秒钟又执行了相同的查询，保证两次查询的结果是相同的，可重复读也是mysql 的默认隔离级别。
可串行化（SERIALIZABLE）：可串行化就是保证读取的范围内没有新的数据插入，比如事务第一次查询得到某个范围的数据，第二次查询也同样得到了相同范围的数据，中间没有新的数据插入到该范围中。

总结：

隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。

大多数的数据库默认隔离级别为 Read Commited，比如 SqlServer、Oracle

少数数据库默认隔离级别为：Repeatable Read 比如：MySQL InnoDB

## Spring中的隔离级别

| 常量                       | 解释                                                         |
| :------------------------- | :----------------------------------------------------------- |
| ISOLATION_DEFAULT          | 这是个 PlatfromTransactionManager 默认的隔离级别，使用数据库默认的事务隔离级别。另外四个与 JDBC 的隔离级别相对应。 |
| ISOLATION_READ_UNCOMMITTED | 这是事务最低的隔离级别，它充许另外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。 |
| ISOLATION_READ_COMMITTED   | 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。 |
| ISOLATION_REPEATABLE_READ  | 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。 |
| ISOLATION_SERIALIZABLE     | 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。 |

  可重复读

事务A先进行数据查询，事务B进行一次数据修改，事务A再次查询数据 数据不变，事务B进行数据提交，事务A再进行一次查询，数据是不改变的。

读已提交

事务A先进行数据查询，事务B进行一次事务修改，事务A再次查询数据 数据不变，事务B进行数据提交，事务A再进行一次查询，数据是改变的。 



## **数据事务的实现原理呢？**

我们这里以 MySQL 的 InnoDB 引擎为例来简单说一下。

MySQL InnoDB 引擎使用 **redo log(重做日志)** 保证事务的**持久性**，使用 **undo log(回滚日志)** 来保证事务的**原子性**。

MySQL InnoDB 引擎通过 **锁机制**、**MVCC** 等手段来保证事务的隔离性（ 默认支持的隔离级别是 **`REPEATABLE-READ`** ）。

保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。

### [并发事务带来哪些问题?](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=并发事务带来哪些问题)

在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。

- **脏读（Dirty read）:** 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。
- **丢失修改（Lost to modify）:** 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 修改 A=A-1，事务 2 也修改 A=A-1，最终结果 A=19，事务 1 的修改被丢失。
- **不可重复读（Unrepeatableread）:** 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的‘修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。
- **幻读（Phantom read）:** 幻读与不可重复读类似**。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时**。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。

### [事务隔离级别有哪些?](https://snailclimb.gitee.io/javaguide/#/docs/database/MySQL?id=事务隔离级别有哪些)

SQL 标准定义了四个隔离级别：(什么是隔离级别？)

- **READ-UNCOMMITTED(读取未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。


  常被成为Dirty Reads（脏读），可以说是事务上的最低隔离级别：在普通的非锁定模式下SELECT 的执行时我们看到的数据可能并不是查询发起时间点的数据，因而在这个隔离度下是非Consistent Reads（一致性读）

- **READ-COMMITTED(读取已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。这个事务隔离级别有些类似 Oracle 数据库默认的隔离级。属于语句级别的隔离，如通过SELECT ... FOR UPDATE 和 SELECT ... LOCK IN SHARE MODE 来执行的请求仅仅锁定索引记录，而不锁定之前的间隙，因而允许在锁定的记录后自由地插入新记录。当然，这与Innodb 的锁定实现机制有关。如果我们的Query 可以很准确的通过索引定位到需要锁定的记录，则仅仅只需要锁定相关的索引记录，而不需要锁定该索引之前的间隙。但如果我们的Query 通过索引检索的时候无法通过索引准确定位到需要锁定的记录，或者是一个基于范围的查询，InnoDB 就必须设置 next-key 或gap locks 来阻塞其它用户对范围内的空隙插入。Consistent Reads 的实现机制与 Oracle 基本类似： 每一个 Consistent Read，甚至是同一个事务中的，均设置并作为它自己的最新快照。
  这一隔离级别下，不会出现Dirty Read，但是可能出现Non-Repeatable Reads(不可重复读) 和 Phantom Reads（幻读）。

- **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。REPEATABLE READ 隔离级别是 InnoDB 默认的事务隔离级。SELECT ... FOR UPDATE, SELECT
  ... LOCK IN SHARE MODE, UPDATE, 和 DELETE ，这些以唯一条件搜索唯一索引的，只锁定所找到的索引记录，而不锁定该索引之前的间隙。 否则这些操作将使用 next-key 锁定，以 next-key 和gap locks 锁定找到的索引范围，并阻塞其它用户的新建插入。在 Consistent Reads 中，与前一个隔离级相比这是一个重要的差别： 在这一级中，同一事务中所有的 Consistent Reads 均读取第一次读取时已确定的快照。这个约定就意味着如果在同一事务中发出几个无格式(plain)的 SELECTs，这些 SELECT 的相互关系是一致的。
  在REPEATABLE READ 隔离级别下，不会出现Dirty Reads，也不会出现Non-Repeatable Reads， 但是仍然存在幻读的可能性。

- **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。

以上四种事务隔离级别实际上就是ANSI/ISO SQL92 标准所定义的四种隔离级别，Innodb 全部都为我们实现了。对于高并发应用来说，为了尽可能保证数据的一致性，避免并发可能带来的数据不一致问题，自然是事务隔离级别越高越好。但是，对于Innodb 来说，所使用的事务隔离级别越高，实现复杂度自然就会更高，所需要做的事情也会更多，整体性能也就会更差。实际上在我们大部分的应用场景下，都只需要READ COMMITED 的事务隔离级别就可以满足需求了



MySQL InnoDB 存储引擎的默认支持的隔离级别是 **REPEATABLE-READ（可重读）**。我们可以通过`SELECT @@tx_isolation;`命令来查看，MySQL 8.0 该命令改为`SELECT @@transaction_isolation`

**MySQL InnoDB 的 REPEATABLE-READ（可重读）并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁度使用到的机制就是 Next-Key Locks**

mysql> SELECT @@tx_isolation;
+-----------------+
| @@tx_isolation  |
+-----------------+
| REPEATABLE-READ |
+-----------------+
1 row in set, 1 warning (0.00 sec)

因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 **READ-COMMITTED(读取提交内容)** ，但是你要知道的是 InnoDB 存储引擎默认使用 **REPEAaTABLE-READ（可重读）** 并不会有任何性能损失。

InnoDB 存储引擎在 **分布式事务** 的情况下一般会用到 **SERIALIZABLE(可串行化)** 隔离级别

1.innodb对于行的查询使用next-key lock

2.next-locking keying为了解决幻读问题

3.当查询的索引含有唯一属性时，将next-key lock降级为record key

4.gap锁的设计目的是为了阻止多个事务将记录查到同一范围内，会导致幻读

5.gap锁可以显式关闭

# 什么是MVCC 快照读 当前读

注意：begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个修改操作InnoDB表的语句，事务才真正启动，才会向mysql申请事务id，mysql内部是严格按照事务的启动顺序来分配事务id的。
总结：
MVCC机制的实现就是通过read-view机制与undo版本链比对机制，使得不同的事务会根据数据版本链对比规则读取同一条数据在版本链上的不同版本数据。

![image-20210923085532679](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923085532679.png)

![image-20210923085612620](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923085612620.png)

内部，`InnoDB`向存储在数据库中的每一行添加三个字段。 6 字节的`DB_TRX_ID`字段指示插入或更新该行的最后一笔 Transaction 的 Transaction 标识符。此外，删除在内部被视为更新，在该更新中，该行中的特殊位被设置为将其标记为已删除。每行还包含一个 7 字节的`DB_ROLL_PTR`字段，称为“滚动指针”。回滚指针指向写入回滚段的撤消日志记录。如果行已更新，则撤消日志记录将包含在更新行之前重建行内容所必需的信息。一个 6 字节的`DB_ROW_ID`字段包含一个行 ID，该行 ID 随着插入新行而单调增加。如果`InnoDB`自动生成聚集索引，则该索引包含行 ID 值。否则，`DB_ROW_ID`列不会出现在任何索引中。

　　什么是MVCC?
　　1、MVCC
　　MVCC,全称Multi-Version Concurrency Control.即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存。
　　MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读写冲突**，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读。**
　　2、当前读
　　像select lock in share mode(共享锁)，select for update;update,insert,delete(排他锁)这些操作都是一种当前读，**为什么叫当前读？就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁**。是通过next-key实现的即Record Lock + Gap Lock，
　　3、快照读（提高数据库的并发查询能力）也叫普通读
　　**像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别**，串行级别下的快照读会退化成当前读；之所以出现快照读的情况，是基于提高并发性能的考虑，**快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种**，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本

4、当前读、快照读、MVCC关系
　　MVCC多版本并发控制指的是维持一个数据的多个版本，使得读写操作没有冲突，快照读是MySQL为实现MVCC的一个非阻塞读功能。MVCC模块在MySQL中的**具体实现是由三个隐式字段，undo日志、read view三个组件来实现的**

在 MySQL 的可重复读隔离级别下，不存在不可重复读的问题，那么 MySQL 是如何解决的呢？

**答案就是 MVCC 机制。**MVCC 是 Mutil-Version Concurrent Control(多版本并发控制)的缩写，它指的是数据库中的每一条数据，会存在多个版本。对同一条数据而言，MySQL 会通过一定的手段(ReadView 机制)控制每一个事务看到不同版本的数据，这样也就解决了不可重复读的问题

**MVCC 机制解决的是快照读的幻读问题，并不能解决当前读的幻读问题。当前读的幻读问题是通过间隙锁解决的**





## RC、RR级别下的InnoDB快照读有什么不同

Read View(读视图)

- Read View就是事务进行快照读(select * from)操作的时候生产的读视图(Read View)，在该事务执行的快照读的那一刻，会生成事务系统当前的一个快照，记录并维护系统当前活跃事务(未提交事务)的ID(当每个事务开启时，都会被分配一个ID, 这个ID是递增的，所以最新的事务，ID值越大)。

-  Read View主要是用来做可见性判断的, 即当我们某个事务执行快照读的时候，对该记录创建一个Read View读视图，把它比作条件用来判断当前事务能够看到哪个版本的数据，既可能是当前最新的数据，也有可能是该行记录的undo log里面的某个版本的数据

- ReadView中主要包含4个比较重要的内容：

  - m_ids：表示在生成ReadView时当前系统中活跃的读写事务的事务id列表。
  - 表示在生成ReadView时当前系统中活跃的读写事务中最小的事务id，也就是m_ids中的最小值。
  - max_trx_id：表示生成ReadView时系统中应该分配给下一个事务的id值。
  - creator_trx_id：表示生成该ReadView的快照读操作产生的事务id。

  - 有了这个ReadView，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见：
    - 如果被访问版本的trx_id属性值与ReadView中的creator_trx_id值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。
    - 如果被访问版本的trx_id属性值小于ReadView中的min_trx_id值，表明生成该版本的事务在当前事务生成ReadView前已经提交，所以该版本可以被当前事务访问。
    - 如果被访问版本的trx_id属性值大于ReadView中的max_trx_id值，表明生成该版本的事务在当前事务生成ReadView后才开启，所以该版本不可以被当前事务访问。
    - 如果被访问版本的trx_id属性值在ReadView的min_trx_id和max_trx_id之间，那就需要判断一下trx_id属性值是不是在m_ids列表中，如果在，说明创建ReadView时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建ReadView时生成该版本的事务已经被提交，该版本可以被访问

　　因为Read View生成时机的不同，从而造成RC、RR级别下快照读的结果的不同
　　1、在RR级别下的某个事务的对某条记录的第一次快照读会创建一个快照即Read View,将当前系统活跃的其他事务记录起来，此后在调用快照读的时候，还是使用的是同一个Read View,**所以只要当前事务在其他事务提交更新之前使用过快照读，那么之后的快照读使用的都是同一个Read View,所以对之后的修改不可见**

2、在RR级别下，快照读生成Read View时，Read View会记录此时所有其他活动和事务的快照，这些事务的修改对于当前事务都是不可见的，而早于Read View创建的事务所做的修改均是可见

3、在RC级别下，事务中，每次快照读都会新生成一个快照和Read View,这就是我们在RC级别下的事务中可以看到别的事务提交的更新的原因。
　　总结：在RC隔离级别下，**是每个快照读都会生成并获取最新的Read View,而在RR隔离级别下，则是同一个事务中的第一个快照读才会创建Rea**d View,之后的快照读获取的都是同一个Read View.





## MVCC解决的问题是什么？

在MySQL中，MVCC只在读取已提交（Read Committed）和可重复读（Repeatable Read）两个事务级别下有效。其是通过Undo日志中的版本链和ReadView一致性视图来实现的。MVCC就是在多个事务同时存在时，SELECT语句找寻到具体是版本链上的哪个版本，然后在找到的版本上返回其中所记录的数据的过程。

　　数据库并发场景有三种，分别为：
　　1、读读：不存在任何问题，也不需要并发控制
　　2、读写：有线程安全问题，可能会造成事务隔离性问题，可能遇到脏读、幻读、不可重复读

3、写写：有线程安全问题，可能存在更新丢失问题MVC是一种用来解决读写冲突的无锁并发控制，也就是为事务分配单项增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照，所以MVCC可以为数据库解决以下问题：
　　1、在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能
　  2　、***解决脏读、幻读、不可重复读等事务隔离问题，但是不能解决更新丢失问题*** 



## MVCC原理 

当更新记录时，将原记录放入Undo表空间中，我们查询看到的未修改的数据就是从Undo表空间中返回的，如果存在多个数据的版本就会构成一个链表。在MySQL中就是根据记录上的回滚段指针及事务ID判断记录是否可见的。具体的判断流程如下： 在每个事务开始时，都会将当前系统中所有的活跃事务拷贝到一个列表（Read View）中。当读取一行记录时，会根据行记录上的TRX_ID值与ReadView中的最大TRX_ID值、最小TRX_ID值的比较来判断是否可见。 比较TRX_ID值是否小于Read View中的最小TRX_ID值，如果是，则说明此事务早于Read View中的 有事务结束，可以输出返回；如果不是，则判断TRX_ID值是否大于Read View中的最大TRX_ID值。 

·　如果是，则根据行记录上的回滚段指针找到回滚段中的对应记录且取出TRX_ID值赋给当前行的TRX_ID，并重新执行比较操作（说明此行记录在事务开始之后发生了变化）。 

·　如果不是，则判断TRX_ID值是否在Read View中。如果在Read View中，则根据行记录上的回滚段指针找到回滚段中的对应记录且取出TRX_ID值（说明此行记录在事务开始时处于活跃状态）；如果不是，则返回记录。



·　在创建表时，如果指定了主键，则将其作为聚集索引。 

·　如果没有指定主键，则选择第一个NOT NULL的唯一索引作为聚集索引。 

·　如果没有唯一索引，则内部会生成一个6字节的rowid作为主键。



不知道大家有没有想过一个问题，在日常操作中，为什么读写两种操作互相不阻塞呢？在MVCC技术出现之前，为了保证在特定隔离级别下多个事务之间不出现异常现象，需要通过加锁的方式来保证事务之间的并发。例如：事务1正在读取一行数据，这时不能有其他事务对这行数据进行操作，因为很有可能造成事务1读取的数据不一致。 而MVCC最重要的核心便是解决了读写直接不阻塞的问题，提高了事务之间的并发性。下面我们通过一个例子来逐步学习MySQL中的MVCC实现 



如何解决幻读？ MVCC +间隙锁



 multi-version concurrency control

mysql的默认隔离级别是RR（可重复读），网上随便一查都知道RR会导致幻读（同一个事务里面多次查询的结果不一致），可是我自己测试过后发现在RR下并不存在幻读的问题，哪mysql是怎么解决幻读的呢？有两种手段。1，mvcc（多版本控制），2，范围锁

\1. mvcc

每次开启事务后都会递增创建一个版本号（version），之后的增删查改都是基于这个版本号进行操作的。

SELECT (version)

读取创建版本小于或等于当前事务版本号，并且删除版本为空或大于当前事务版本号的记录。这样可以保证在读取之前记录是存在的。version >= createVersion and version < deleteVersion

INSERT (createVersion)

将当前事务的版本号保存至行的创建版本号。 createVersion = version

UPDATE (createVersion)

 

新插入一行，并以当前事务的版本号作为新行的创建版本号，同时将原记录行的删除版本号设置为当前事务版本号。 新行createVersion = version，旧行deleteVersion = version

DELETE (deleteVersion)

将当前事务的版本号保存至行的删除版本号。 deleteVersion = version

例子：

初始数据

表名：test

id  number  createVersion  deleteVersion

1   1  1  

select * from test; //当前version = 2，由于id=1的createVersion=1小于当前version所以可以查出来。

插入一条数据

id  number  createVersion  deleteVersion

1    1    1  

2    3    3  

这时候如果上面的selct事务没结束继续查询的话并不会查询到id=2的数据。由于新插入的数据createVersion = 3 大于查询的version，这样就避免了幻读的情况。

2. 间隙锁

mysql的间隙所是基于索引的，对于唯一索引innodb会把间隙所降级为行锁，非唯一索引的话就需要用到间隙锁（也叫范围锁）

 

id  number

1   1

2   3

13  3

23  3

31  11

40  40

事务一：select * from test where number = 3 for update

对于number索引可以分为多个范围

(无穷小,1)(1,3)(3,3)(3,11)(11，无穷大)

这时候锁住的是(3,3)区间，对应的临界记录是(id=1,number=1)(id=31,number=11)，对于这范围内的数据都是被锁住的。

 

事务二：insert into test(id, number) value(5, 3) //是会被阻塞

事务三：insert into test(id, number) value(25, 4) //也是会被阻塞 

事务四：insert into test(id, number) value(35, 4) //也是会被阻塞 

事务五：insert into test(id, number) value(22, 12) //插入成功 （因为12>11所以在锁区间外） 

事务六：insert into test(id, number) value(71, 11) //插入成功 （number值一样，但是id71>31所以在锁区间外）



# 视图

![image-20210831193933301](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210831193933301.png)



# 为什么要主从同步

　　mysql为什么需要主从同步？
　　1、在业务复杂的系统中，有这么一个情景，有一句sql语句需要锁表，导致暂时不能使用读的服务，那么就很影响运行中的业务，使用主从复制，让主库负责写，从库负责读，这样，即使主库出现了锁表的情景，通过读从库也可以保证业务的正常运作。
　　2、做数据的热备
　　3、架构的扩展。业务量越来越大，1/0访问频率过高，单机无法满足，此时做多库的存储，降低磁盘1/0访问的频率，提高单个机器的/0性能。
　　 

# mysql主从同步原理

![image-20210923090006192](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923090006192.png)

![image-20210923090122186](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923090122186.png)

**半同步复制：**在半同步复制出现之前，虽然异步复制可以满足主从实例之间的数据同步，同时row格式的binlog也能够大幅度避免主从实例的数据不一致的情况，但是如果碰到主库崩溃，写业务故障切换到从库，将从库提升为主库时，原来的主库上可能有一部分数据还没来得及被从库接收，而事实上这部分丢失的数据可能在主库上已经正常提交完成了。为解决这个问题，在MySQL 5.5版本中引入了半同步复制，半同步复制的关键改进就是当客户端在主库上写入一个事务时，需要等待从库接收到主库的binlog，**且主库接收到ACK确认之后，客户端才能收到事务成功提交的消息**，如图18-3所示（该图来自Oracle MySQL官方）。 

![image-20210923104835620](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923104835620.png)

![image-20210923090027783](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923090027783.png)

**异步复制**：异步复制主要利用三个线程来实现数据流转：主库binlog dump线程、从库I/O线程和SQL线程。

![image-20210923090050019](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923090050019.png)

·　用户对数据的修改进行提交，然后Master（以下统称为“主库”）把所有数据库的变更写进binlog中，主库线程binlog dump把binlog内容推送给Slave（以下统称为“从库”，从库被动接收数据，而不是主动去获取数据的）。

·　从库I/O线程读取（接收）主库上的binlog信息，并把binlog写到本地中继日志（relay log）中。

 ·　从库**SQL线程读取并解析ralay log内容，按照主库中的提交顺序进行事务回放**，写入本地数据文件中，这样就实现了数据在主从实例之间的同步。 这里有一个小细节需要注意：主库在写入binlog并落盘之后，通知dump线程有新的binlog产生，并发送到从库中。**然后主库并不理会从库是否接收到binlog，而是自顾自地照常进行事务的提交，** 



Replication 线程
Mysql 的 Replication 是一个异步的复制过程，从一个 Mysql instace（我们称之为Master）复制到另一个 Mysql instance（我们称之 Slave）。在 Master 与 Slave 之间的实现整个复制过程主要由三个线程来完成，其中两个线程（Sql 线程和IO 线程）在 Slave 端 ， 另外一个线程（IO 线程）在 Master 端。
要实现 MySQL 的 Replication ，首先必须打开 Master 端的Binary Log（mysql- bin.xxxxxx）功能，否则无法实现。因为整个复制过程实际上就是Slave 从 Master 端获取该日志然后再在自己身上完全顺序的执行日志中所记录的各种操作。打开 MySQL 的 Binary Log 可以通过在启动 MySQL Server 的过程中使用“—log-bin” 参数选项，或者在 my.cnf 配置文件中的 mysqld 参数组（[mysqld]标识后的参数部分）增加 “log-bin” 参数项。

MySQL 复制的基本过程如下：
1. Slave 上面的IO 线程连接上 Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容；
2. Master 接收到来自 Slave 的 IO 线程的请求后，通过负责复制的 IO 线程根据请求信息读取指定日志指定位置之后的日志信息，返回给 Slave 端的 IO 线程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息在 Master 端的 Binary Log 文件的名称以及在 Binary Log 中的位置；
3. Slave 的 IO 线程接收到信息后，将接收到的日志内容依次写入到 Slave 端的Relay Log 文件(mysql-relay-bin.xxxxxx)的最末端，并将读取到的Master 端的bin- log 的文件名和位置记录到master-info 文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log 的哪个位置开始往后的日志内容，请发给我”
4. Slave 的 SQL 线程检测到 Relay Log 中新增加了内容后，会马上解析该 Log 文件中的内容成为在 Master 端真实执行时候的那些可执行的 Query 语句，并在自身执行这些 Query。这样，实际上就是在 Master 端和 Slave 端执行了同样的 Query，所以两端的数据是完全一样的。

即使是换成了现在这样两个线程来协作处理之后，同样也还是存在 Slave 数据延时以及数据丢失的可能性的，毕竟这个复制是异步的。只要数据的更改不是在一个事务中 ， 这些问题都是存在的。

如果要完全避免这些问题，就只能用 MySQL 的 Cluster 来解决了。不过 MySQL 的Cluster 知道笔者写这部分内容的时候，仍然还是一个内存数据库的解决方案，也就是需要将所有数据包括索引全部都 Load 到内存中，这样就对内存的要求就非常大的大，对于一般的大众化应用来说可实施性并不是太大。当然，在之前与 MySQL 的 CTO David 交流的时候得知，MySQL 现在正在不断改进其 Cluster 的实现，其中非常大的一个改动就是允许数据不用全部 Load 到内存中，而仅仅只是索引全部 Load 到内存中，我想信在完成该项改造之后的 MySQL Cluster 将会更加受人欢迎，可实施性也会更大。

MySQL Replicaion 本身是一个比较简单的架构，就是一台 MySQL 服务器（Slave）从另一台 MySQL 服务器（Master）进行日志的复制然后再解析日志并应用到自身。一个复制环境仅仅只需要两台运行有 MySQL Server 的主机即可，甚至更为简单的时候我们可以在同一台物理服务器主机上面启动两个 mysqld instance，一个作为 Master 而另一个作为Slave 来完成复制环境的搭建。但是在实际应用环境中，我们可以根据实际的业务需求利用MySQL Replication 的功能自己定制搭建出其他多种更利于 Scale Out 的复制架构。如Dual Master 架构，级联复制架构等。下面我们针对比较典型的三种复制架构进行一些相应的分析介绍。



# 数据库系统安全相关因素

一、外围网络：
MySQL 的大部分应用场景都是基于网络环境的，而网络本身是一个充满各种入侵危险的环境，所以要保护他的安全，在条件允许的情况下，就应该从最外围的网络环境开始“布防”，因为这一层防线可以从最大范围内阻止可能存在的威胁。
在网络环境中，任意两点之间都可能存在无穷无尽的“道路”可以抵达，是一个真正“条 条道路通罗马”的环境。在那许许多多的道路中，只要有一条道路不够安全，就可能被入侵者利用。当然，由于所处的环境不同，潜在威胁的来源也会不一样。有些 MySQL 所处环境是暴露在整个广域网中，可以说是完全“裸露”在任何可以接入网络环境的潜在威胁者面前。而有些MySQL 是在一个环境相对小一些的局域网之内，相对来说，潜在威胁者也会少很多。处在局域网之内的MySQL，由于有局域网出入口的网络设备的基本保护，相对于暴露在广域网中要安全不少，主要威胁对象基本上控制在了可以接入局域网的内部潜在威胁者，和极少数能够突破最外围防线（局域网出入口的安全设备）的入侵者。所以，尽可能的让我们的MySQL 处在一个有保护的局域网之中，是非常必要的。

二、主机：
有了网络设备的保护，我们的MySQL 就足够安全了么？我想大家都会给出否定的回答。因为即使我们局域网出入口的安全设备足够的强大，可以拦截住外围试图入侵的所有威胁 者，但如果威胁来自局域网内部呢？比如局域网中可能存在被控制的设备，某些被控制的有权限接入局域网的设备，以及内部入侵者等都仍然是威胁者。所以说，吉使在第一层防线之内，我们仍然存在安全风险，局域网内部仍然会有不少的潜在威胁存在。
这个时候就需要我们部署第二道防线“主机层防线”了 。“主机层防线”主要拦截网络
（包括局域网内）或者直连的未授权用户试图入侵主机的行为。因为一个恶意入侵者在登录到主机之后，可能通过某些软件程序窃取到那些自身安全设置不够健壮的数据库系统的登入口令，从而达到窃取或者破坏数据的目的。如一个主机用户可以通过一个未删除且未设置密码的无用户名本地帐户轻易登入数据库，也可以通过 MySQL 初始安装好之后就存在的无密码的“root@localhost”用户登录数据库并获得数据库最高控制权限。

三、数据库：
通过第二道防线“主机层防线”的把守，我们又可以挡住很大一部分安全威胁者。但仍然可能有极少数突破防线的入侵者。而且即使没有任何“漏网之鱼”，那些有主机登入权限 的使用者呢？是否真的就是完全可信任对象？No，我们不能轻易冒这个潜在风险。对于一个有足够安全意识的管理员来说，是不会轻易放任任何一个潜在风险存在的。
这个时候，我们的第三道防线，“数据库防线”就需要发挥他的作用了。“数据库防线”也就是 MySQL 数据库系统自身的访问控制授权管理相关模块。这道防线基本上可以说是MySQL 的最后一道防线了，也是最核心最重要的防线。他首先需要能够抵挡住在之前的两层防线都没有能够阻拦住的所有入侵威胁，同时还要能够限制住拥有之前二层防线自由出入但不具备数据库访问权限的潜在威胁者，以确保数据库自身的安全以及所保存数据的安全。
之前的二层防线对于所有数据库系统来说基本上区别不大，都存在着基本相同的各种威胁，不论是 Oracle 还是MySQL，以及任何其他的数据库管理系统，都需要基本一致的“布防”策略。**但是这第三层防线，也就是各自自身的“数据库防线”对于每个数据库系统来说都存在较大的差异，因为每种数据库都有各自不太一样的专门负责访问授权相关功能的模块。不论是权限划分还是实现方式都可能不太一样。**

对于MySQL 来说，其访问授权相关模块主要是由两部分组成。一个是基本的用户管理模块，另一个是访问授权控制模块。用户管理模块的功能相对简单一些，主要是负责用户登录连接相关的基本权限控制，但其在安全控制方面的作用却不比任何环节小。他就像 MySQL 的一个“大门门卫”一样，通过校验每一位敲门者所给的进门“暗号”（登入口令），决定是否给敲门者开门。而访问授权控制模块则是随时随地检查已经进门的访问者，校验他们是否有访问所发出请求需要访问的数据的权限。通过校验者可顺利拿到数据，而未通过校验的访问者，只能收到“访问越权了”的相关反馈



## sql注入

 

原因:**用户传入的参数中注入符合sql的语法，从而破坏原有sql结构语意，达到攻击效果**。

1、SQL 语句相关安全因素：
“SQL 注入攻击”这个术语我想大部分读者朋友都听说过了？指的就是攻击者根据数据库的SQL 语句解析器的原理，利用程序中对客户端所提交数据的校验漏洞，从而通过程序动态提交数据接口提交非法数据，达到攻击者的入侵目的。

通过在Web 表单中输入（恶意）SQL 语句得到一个存在安全漏洞的网站上的数据库，而不是按照设计者意图去执行SQL 语句。举例：当执行的sql 为 select * from user where username = “admin”or “a”=“a”时，sql 语句恒成立，参数admin 毫无意义。
防止sql 注入的方式：

1. 预编译语句：如，select * from user where username = ？，sql 语句语义不会发生改
    变，sql 语句中变量用？表示，即使传递参数时为“admin or ‘a’= ‘a’”，也会把这整体当
    做一个字符串去查询。
2. Mybatis 框架中的mapper 方式中的 # 也能很大程度的防止sql 注入（$无法防止sql 注入）。



 ${ } 变量的替换阶段是在动态 SQL 解析阶段，而 #{ }变量的替换是在 DBMS 中

预编译：

**1. 定义：**

sql 预编译指的是数据库驱动在发送 sql 语句和参数给 DBMS 之前对 sql 语句进行编译，这样 DBMS 执行 sql 时，就不需要重新编译。

**2. 为什么需要预编译**

JDBC 中使用对象 PreparedStatement 来抽象预编译语句，使用预编译。预编译阶段可以优化 sql 的执行。预编译之后的 sql 多数情况下可以直接执行，DBMS 不需要再次编译，越复杂的sql，编译的复杂度将越大，预编译阶段可以合并多次操作为一个操作。预编译语句对象可以重复利用。把一个 sql 预编译后产生的 PreparedStatement 对象缓存下来，下次对于同一个sql，可以直接使用这个缓存的 PreparedState 对象。mybatis 默认情况下，将对所有的 sql 进行预编译。





# 语法

现在，我们希望显示每天日期所对应的名称和价格（日期的显示格式是 "YYYY-MM-DD"）。

我们使用如下 SQL 语句：

```sql
SELECT ProductName, UnitPrice, FORMAT(Now(),'YYYY-MM-DD') as PerDate
FROM Products
```



# MySQL 可扩展设计的基本原则

什么是可扩展性
在讨论可扩展性之前，可能很多朋有会问：常听人说起某某网站某某系统在可扩展性方面设计的如何如何好，架构如何如何出色，到底什么是扩展？怎样算是可扩展？什么又是可扩展性呢？其实也就是大家常听到的Scale，Scalable 和 Scalability 这三个词。
从数据库的角度来说，Scale（扩展）就是让我们的数据库能够提供更强的服务能力， 更强的处理能力。而 Scalable（可扩展）则是表明数据库系统在通过相应升级（包括增加单机处理能力或者增加服务器数量）之后能够达到提供更强处理能力。在理论能上来说，任何数据库系统都是 Scalable 的， 只不过是所需要的实现方式不一样而已。最后， Scalability（扩展性）则是指一个数据库系统通过相应的升级之后所带来处理能力提升的难以程度。虽然理论上任何系统都可以通过相应的升级来达到处理能力的提升，但是不同的系统提升相同的处理能力所需要的升级成本（资金和人力）是不一样的，这也就是我们所说的各个数据库应用系统的 Scalability 存在很大的差异。
在这里，我所说的不同数据库应用系统并不是指数据库软件本身的不同（虽然数据库软件不同也会存在 Scalability 的差异），而是指相同数据库软件的不同应用架构设计，这也正是本章以及后面几张将会所重点分析的内容。

首先，我们需要清楚一个数据库据系统的扩展性实际上是主要体现在两个方面，一个是横向扩展，另一个则是纵向扩展，也就是我们常说的 Scale Out 和 Scale Up。
Scale Out 就是指横向的扩展，向外扩展，也就是通过增加处理节点的方式来提高整体处理能力，说的更实际一点就是通过增加机器来增加整体的处理能力。
Scale Up 则是指纵向的扩展，向上扩展，也就是通过增加当前处理节点的处理能力来提高整体的处理能力，说白了就是通过升级现有服务器的配置，如增加内存，增加CPU，增加存储系统的硬件配置，或者是直接更换为处理能力更强的服务器和更为高端的存储系统。

通过比较两种 Scale 方式，我们很容易看出各自的优缺点。
◆ Scale Out 优点：
1. 成本低，很容易通过价格低廉的 PC Server 搭建出一个处理能力非常强大的计算集群；

2. 不太容易遇到瓶颈，因为很容易通过添加主机来增加处理能力；

3. 单个节点故障对系统整体影响较小；也存在缺点，更多的计算节点，大部分时候都是服务器主机，这自然会带来整个系统维护复杂性的提高，在某些方面肯定会增加维护成本，而且对应用系统的架构要求也会比 Scale Up 更高，需要集群管理软件的配合

  ◆ Scale Out 缺点：

4. 处理节点多，造成系统架构整体复杂度提高，应用程序复杂度提高；

5. 集群维护难以程度更高，维护成本更大；
    ◆ Scale Up 优点：

6. 处理节点少，维护相对简单；

7. 所有数据都集中在一起，应用系统架构简单，开发相对容易；
    ◆ Scale Up 缺点

8. 高端设备成本高，且竞争少，容易受到厂家限制；

9. 受到硬件设备发展速度限制，单台主机的处理能力总是有极限的，容易遇到最终无法解决的性能瓶颈；

10. 设备和数据集中，发生故障后的影响较大；





# 系统数据库

①系统数据库：系统数据库包括以下几种。 

✦✦✦information_schema系统数据库，提供了数据库的元数据信息，是数据库的数据，比如数据库的名字和数据库中的表名、字段名、字段类型等，可以说是数据库的数据字典信息。这个库中的信息并非物理地保存在表中，而是动态地去读取其他文件得到的。例如，上面一开始提到的共享表空间，用户数据中的对象（比如表结构等）就都保存在共享表空间中，information_schema库中的一些信息可以认为是直接映射到共享表空间中的信息的。 

✦✦✦**performance_schema系统数据库，是数据库性能相关的信息的数据**，记录的是数据库服务器的性能参数，保存历史事件汇总信息，为MySQL服务器性能评估提供参考信息。 

sys系统数据库，可以根据sys库中的数据快速了解系统的运行信息，方便地查询出数据库的信息，在性能瓶颈、自动化运维等方面都有很大的帮助。sys库中的信息通过视图的方式将information_schema和performance_schema库中的数据结合起来，可以得到更加直观和容易理解的信息。 

✦✦✦mysql系统数据库，存储系统的用户权限信息及帮助信息。新建的用户、用户的权限信息等都存储在mysql库。例如，在修改MySQL的root密码时，要先调用mysql这个系统库再执行用户、授权等操作。 

②用户数据库：用户数据库（如mldn）实际上是一个目录，保存了数据库中的表以及数据信息。图1-14是一个典型的数据库目录下的文件信 息

# 什么是mysqll的主从复制？

　　MySQL主从复制是指数据可以从一个MySQL数据库服务器主节点复制到一个或多个从节点。MySQL默认采用异步复制方式，这样从节点不用一直访问主服务器来更新自己的数据，数据的更新可以在远程连接上进行，从节点可以复制主数据库中的所有数据库或者特定的数据库，或者特定的表。

# InnoDB数据⻚

以下内容来自《mysql是怎样运行的5》

⼀个InnoDB数据⻚的存储空间⼤致被划分成了7个部分，有的部分占⽤的字节数是确定的，有的部分占⽤的字节数是不确定的。

![image-20220412135042037](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220412135042037.png)

![image-20220412135059939](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220412135059939.png)

在⻚的7个组成部分中，我们⾃⼰存储的记录会按照我们指定的⾏格
式存储到User Records部分。但是在⼀开始⽣成⻚的时候，其实
并没有User Records这个部分，每当我们插⼊⼀条记录，都会从
Free Space部分，也就是尚未使⽤的存储空间中申请⼀个记录⼤⼩
的空间划分到User Records部分，当Free Space部分的空间全
部被User Records部分替代掉之后，也就意味着这个⻚使⽤完
了，如果还有新的记录插⼊的话，就需要去申请新的⻚了

![image-20220412140307343](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220412140307343.png)

我们先创建⼀个表：
mysql> CREATE TABLE page_demo(
-> c1 INT,
-> c2 INT,
-> c3 VARCHAR(10000),
-> PRIMARY KEY (c1)
-> ) CHARSET=ascii ROW_FORMAT=Compact;
Query OK, 0 rows affected (0.03 sec)
这个新创建的page_demo表有3个列，其中c1和c2列是⽤来存储整
数的，c3列是⽤来存储字符串的。需要注意的是，我们把 c1 列指定
为主键，所以在具体的⾏格式中InnoDB就没必要为我们去创建那个
所谓的 row_id 隐藏列了。⽽且我们为这个表指定了ascii字符集
以及Compact的⾏格式。所以这个表中记录的⾏格式示意图就是这
样的：

![image-20220413204331265](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220413204331265.png)

从图中可以看到，我们特意把记录头信息的5个字节的数据给标出来
了，说明它很重要，我们再次先把这些记录头信息中各个属性的⼤体意思浏览⼀下（我们⽬前使⽤Compact⾏格式进⾏演示）：

![image-20220413204703098](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220413204703098.png)

由于我们现在主要在唠叨记录头信息的作⽤，所以为了⼤家理解上的⽅便，我们只在page_demo表的⾏格式演示图中画出有关的头信息属性以及c1、c2、c3列的信息（其他信息没画不代表它们不存在
啊，只是为了理解上的⽅便在图中省略了～），简化后的⾏格式示意图就是这样：

![image-20220413205212557](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220413205212557.png)

下边我们试着向page_demo表中插⼊⼏条记录：
mysql> INSERT INTO page_demo VALUES(1, 100,'aaaa'), (2, 200, 'bbbb'), (3, 300, 'cccc'), (4,400, 'dddd');
Query OK, 4 rows affected (0.00 sec)
Records: 4 Duplicates: 0 Warnings: 0

为了⽅便⼤家分析这些记录在⻚的User Records部分中是怎么表
示的，我把记录中头信息和实际的列数据都⽤⼗进制表示出来了（其
实是⼀堆⼆进制位），所以这些记录的示意图就是：

![image-20220413213140426](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220413213140426.png)

delete_mask
这个属性标记着当前记录是否被删除，占⽤1个⼆进制位，值为0的时候代表记录并没有被删除，为1的时候代表记录被删除掉了。
啥？被删除的记录还在⻚中么？是的，摆在台⾯上的和背地⾥做的可能⼤相径庭，你以为它删除了，可它还在真实的磁盘上。这些被删除的记录之所以不⽴即从磁盘上移除，是因为移除它们之后把其他的记录在磁盘上重新排列需要性能消耗，所以只是打⼀个删除标记⽽已，所有被删除掉的记录都会组成⼀个所谓的垃圾链表，在这个链表中的记录占⽤的空间称之为所谓的可重⽤空间，之后如果有新记录插⼊到表中的话，可能把这些被删除的记录占⽤的存储空间覆盖掉。

record_type
这个属性表示当前记录的类型，⼀共有4种类型的记录，0表示普通记录，1表示B+树⾮叶节点记录，2表示最⼩记录，3表示最⼤记录。从图中我们也可以看出来，我们⾃⼰插⼊的记录就是普通记录，它们的record_type值都是0，⽽最⼩记录和最⼤记录的record_type值分别为2和3。

next_record
这玩意⼉⾮常重要，它表示从当前记录的真实数据到下⼀条记录的真实数据的地址偏移量。⽐⽅说第⼀条记录的next_record值为32，意味着从第⼀条记录的真实数据的地址处向后找32个字节便是下⼀条记录的真实数据。如果你熟悉数据结构的话，就⽴即明⽩了，这其实是个链表，可以通过⼀条记录找到它的下⼀条记录。但是需要注意注意再注意的⼀点
是，下⼀条记录指得并不是按照我们插⼊顺序的下⼀条记录，⽽是按照主键值由⼩到⼤的顺序的下⼀条记录。⽽且规定Infimum 记录（也就是最⼩记录） 的下⼀条记录就本⻚中主键值最⼩的⽤户记录，⽽本⻚中主键值最⼤的⽤户记录的下⼀条记录就是 Supremum 记录（也就是最⼤记录）  

## Page Header（⻚⾯头部）

设计InnoDB的⼤叔们为了能得到⼀个数据⻚中存储的记录的状态信
息，⽐如本⻚中已经存储了多少条记录，第⼀条记录的地址是什么，
⻚⽬录中存储了多少个槽等等，特意在⻚中定义了⼀个叫Page
Header的部分，它是⻚结构的第⼆部分，这个部分占⽤固定的56个
字节，专⻔存储各种状态信息，具体各个字节都是⼲嘛的看下表：

![image-20220414192057214](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220414192057214.png)

![image-20220416001701911](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220416001701911.png)

PAGE_DIRECTION
假如新插⼊的⼀条记录的主键值⽐上⼀条记录的主键值⽐上⼀
条记录⼤，我们说这条记录的插⼊⽅向是右边，反之则是左
边。⽤来表示最后⼀条记录插⼊⽅向的状态就是PAGE_DIRECTION。

PAGE_N_DIRECTION
假设连续⼏次插⼊新记录的⽅向都是⼀致的，InnoDB会把沿
着同⼀个⽅向插⼊记录的条数记下来，这个条数就⽤PAGE_N_DIRECTION这个状态表示。当然，如果最后⼀条
记录的插⼊⽅向改变了的话，这个状态的值会被清零重新统计。

**File Header（⽂件头部）**
上边唠叨的Page Header是专⻔针对<u>数据⻚</u>记录的各种状态信息，
⽐⽅说⻚⾥头有多少个记录了呀，有多少个槽了呀。我们现在描述的
File Header针对各种类型的⻚都通⽤，也就是说不同类型的⻚都
会以File Header作为第⼀个组成部分，它描述了⼀些针对各种⻚
都通⽤的⼀些信息，⽐⽅说这个⻚的编号是多少，它的上⼀个⻚、下
⼀个⻚是谁

![image-20220416004914505](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220416004914505.png)

FIL_PAGE_SPACE_OR_CHKSUM
这个代表当前⻚⾯的校验和（checksum）。啥是个校验和？
就是对于⼀个很⻓很⻓的字节串来说，我们会通过某种算法来计算⼀个⽐较短的值来代表这个很⻓的字节串，这个⽐较短的值就称为校验和。这样在⽐较两个很⻓的字节串之前先⽐较这两个⻓字节串的校验和，如果校验和都不⼀样两个⻓字节串肯定是不同的，所以省去了直接⽐较两个⽐较⻓的字节串的时间损耗。
FIL_PAGE_OFFSET
每⼀个⻚都有⼀个单独的⻚号，就跟你的身份证号码⼀样，InnoDB通过⻚号来可以唯⼀定位⼀个⻚。

FIL_PAGE_TYPE
这个代表当前⻚的类型，我们前边说过，InnoDB为了不同的⽬的⽽把⻚分为不同的类型，我们上边介绍的其实都是存储记录的数据⻚，其实还有很多别的类型的⻚，具体如下表：

![image-20220416005058263](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220416005058263.png)

![image-20220416005133764](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220416005133764.png)

FIL_PAGE_PREV和FIL_PAGE_NEXT
我们前边强调过，InnoDB都是以⻚为单位存放数据的，有时候我们存放某种类型的数据占⽤的空间⾮常⼤（⽐⽅说⼀张表中可以有成千上万条记录），InnoDB可能不可以⼀次性为这么多数据分配⼀个⾮常⼤的存储空间，如果分散到多个不连续的⻚中存储的话需要把这些⻚关联起来，FIL_PAGE_PREV和FIL_PAGE_NEXT就分别代表本⻚的上⼀个和下⼀个⻚的⻚号。这样通过建⽴⼀个双向链表把许许多多的⻚就都串联起来了，⽽⽆需这些⻚在物理上真正连着。需要注意的是，并不是所有类型的⻚都有上⼀个和下⼀个⻚的属性，不过我们本集中唠叨的数据⻚（也就是类型为FIL_PAGE_INDEX的⻚）是有这两个属性的，所以所有的数据⻚其实是⼀个双链表，就像这样：

![image-20220416005234982](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220416005234982.png)

1. InnoDB为了不同的⽬的⽽设计了不同类型的⻚，我们把⽤于存
放记录的⻚叫做数据⻚。
2. ⼀个数据⻚可以被⼤致划分为7个部分，分别是File Header，表示⻚的⼀些通⽤信息，占固定的38字节。
Page Header，表示数据⻚专有的⼀些信息，占固定的56个字节。
Infimum + Supremum，两个虚拟的伪记录，分别表示⻚中的最⼩和最⼤记录，占固定的26个字节。
User Records：真实存储我们插⼊的记录的部分，⼤⼩不固定。
Free Space：⻚中尚未使⽤的部分，⼤⼩不确定。
Page Directory：⻚中的某些记录相对位置，也就是
各个槽在⻚⾯中的地址偏移量，⼤⼩不固定，插⼊的记录
越多，这个部分占⽤的空间越多。
File Trailer：⽤于检验⻚是否完整的部分，占⽤固定的8个字节。
3. 每个记录的头信息中都有⼀个next_record属性，从⽽使⻚中的所有记录串联成⼀个单链表。

## 《mysql是怎样运行的》索引

为了故事的顺利发展，我们先建⼀个表：
mysql> CREATE TABLE index_demo(
-> c1 INT,
-> c2 INT,
-> c3 CHAR(1),
-> PRIMARY KEY(c1)
-> ) ROW_FORMAT = Compact;
Query OK, 0 rows affected (0.03 sec)
这个新建的index_demo表中有2个INT类型的列，1个CHAR(1)类
型的列，⽽且我们规定了c1列为主键，这个表使⽤Compact⾏格式
来实际存储记录的。为了我们理解上的⽅便，我们简化了⼀
下index_demo表的⾏格式示意图：

![image-20220417221106567](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220417221106567.png)

record_type：记录头信息的⼀项属性，表示记录的类型，0
表示普通记录、2表示最⼩记录、3表示最⼤记录、1：⽬录项记录（InnoDB区分⼀条记录是普通的⽤户记录还是⽬录项记录）
next_record：记录头信息的⼀项属性，表示下⼀条地址相
对于本条记录的地址偏移量，为了⽅便⼤家理解，我们都会⽤
箭头来表明下⼀条记录是谁。
各个列的值：这⾥只记录在index_demo表中的三个列，分别
是c1、c2和c3。

其他信息：除了上述3种信息以外的所有信息，包括其他隐藏
列的值以及记录的额外信息。

![image-20220417221233061](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220417221233061.png)

![image-20220417231005502](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220417231005502.png)

⽬录项记录的record_type值是1，⽽普通⽤户记录的
record_type值是0。
⽬录项记录只有主键值和⻚的编号两个列，⽽普通的⽤户记录
的列是⽤户⾃⼰定义的，可能包含很多列，另外还有InnoDB
⾃⼰添加的隐藏列。

，它们⽤的是⼀样的数据⻚（⻚⾯类型都是0x45BF，这个属性在File Header中，忘了的话可以翻到前边的⽂章看），⻚的组成结构也是⼀样⼀样的，都会为主键值⽣成Page Directory（⻚⽬录），从⽽在按照主键值进⾏查找时可以使⽤⼆分法来加快查询速度。

![image-20220417231839087](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220417231839087.png)

如图，我们⽣成了⼀个存储更⾼级⽬录项的⻚33，这个⻚中的两条
记录分别代表⻚30和⻚32，如果⽤户记录的主键值在[1, 320)之
间，则到⻚30中查找更详细的⽬录项记录，如果主键值不⼩于320的
话，就到⻚32中查找更详细的⽬录项记录。



# 脏页

不能直接修改硬盘上的数据，而是先将数据从硬盘读入到内存的data cache，然后在内存中修改（被修改过的页称为脏数据页），最后再从内存回写到硬盘。

**脏页**－linux内核中的概念，因为硬盘的读写速度远赶不上内存的速度，系统就把读写比较频繁的数据事先放到内存中，以提高读写速度，这就叫高速缓存，linux是以页作为高速缓存的单位，当进程修改了高速缓存里的数据时，该页就被内核标记为脏页，内核将会在合适的时间把脏页的数据写到磁盘中去，以保持高速缓存中的数据和磁盘中的数据是一致的。

这里插入一个小点：在一个面试题【慢查询如何解决时】有一个可能的原因是脏页相关：
慢查询有两种情况：
**1、大多数情况是正常的，只是偶尔会出现很慢的情况。**

**2、在数据量不变的情况下，这条SQL语句一直以来都执行的很慢。**
脏页和第一种有关，这里只讲第一种：
一条 SQL 大多数情况正常，偶尔才能出现很慢的情况，针对这种情况，我觉得这条SQL语句的书写本身是没什么问题的，而是其他原因导致的，那会是什么原因呢？

1、数据库在刷新脏页（flush）

当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在**内存**中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到**磁盘**中去，而是把这些更新的记录写入到 redo log 日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到**磁盘**中去。

【当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。】

**刷脏页有下面4种场景（后两种不用太关注“性能”问题）：**

- **redolog写满了：**redo log 里的容量是有限的，如果数据库一直很忙，更新又很频繁，这个时候 redo log 很快就会被写满了，这个时候就没办法等到空闲的时候再把数据同步到磁盘的，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，**就会导致我们平时正常的SQL语句突然执行的很慢**，所以说，数据库在在同步数据到磁盘的时候，就有可能导致我们的SQL语句执行的很慢了。

即：

(1) 当 redo log写满了。这时候系统就会停止所有的更新操作，将更新的这部分日志对应的脏页同步到磁盘中，此时所有的更新全部停止，此时写的性能变为0，必须待刷一部分脏页后才能更新,这时就会导致 sql语句 执行的很慢。



- **内存不够用了：**如果一次查询较多的数据，恰好碰到所查数据页不在内存中时，需要申请内存，而此时恰好内存不足的时候就需要淘汰一部分内存数据页，如果是干净页，就直接释放，如果恰好是脏页就需要刷脏页。
- **MySQL 认为系统“空闲”的时候：**这时系统没什么压力。
- **MySQL 正常关闭的时候：**这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。

2、拿不到锁

# 数据写入磁盘的流程
首先明确一下InnoDB修改数据的基本流程。当我们想要修改DB上某一行数据的时候，InnoDB是把数据从磁盘读取到内存的缓冲池上进行修改。这个时候数据在内存中被修改，与磁盘中相比就存在了差异，我们称这种有差异的数据为脏页。InnoDB对脏页的处理不是每次生成脏页就将脏页刷新回磁盘，因为这样会产生海量的I/O操作，严重影响InnoDB的处理性能。既然脏页与磁盘中的数据存在差异，那么如果在此期间DB出现故障就会造成数据丢失。为了解决这个问题，redo log就应运而生了。 我们着重看看redo log是怎么一步步写入磁盘的。

# redo log
redo log本身由两部分所构成，即**重做日志缓存**（redo log buffer）和**重做日志文件**（redo log file）。这样的设计同样也是为了调和内存与磁盘的速度差异。InnoDB写入磁盘的策略可以通过innodb_flush_log_at_trx_commit这个参数来控制。 
具体如下：

- 当设置参数为1时，（默认为1），表示事务提交时必须调用一次 `fsync` 操作，最安全的配置，保障持久性
- 当设置参数为2时，则在事务提交时只做 **write** 操作，只保证将redo log buffer写到系统的页面缓存（或者叫文件系统的缓存）中，不进行fsync操作，因此如果MySQL数据库宕机时 不会丢失事务，但操作系统宕机则可能丢失事务
- 当设置参数为0时，表示事务提交时不进行写入redo log操作，这个操作仅在master thread 中完成，而在master thread中每1秒进行一次重做日志的fsync操作，因此实例 crash 最多丢失1秒钟内的事务。（master thread是负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性）

因此为了保证事务的ACID中的持久性，必须将innodb_flush_log_at_trx_commit设置为1，也就是每当有事务提交时，就必须确保事务都已经写入重做日志文件。那么当数据库因为意外发生宕机时，可以通过重做日志文件恢复，并保证可以恢复已经提交的事务。而将重做日志文件设置为0或2，都有可能发生恢复时部分事务的丢失。不同之处在于，设置为2时，当MySQL数据库发生宕机而操作系统及服务器并没有发生宕机时，由于此时未写入磁盘的事务日志保存在文件系统缓存中，当恢复时同样能保证数据不丢失。 

为了保证每次日志都写入redo log file，在每次将redo buffer写入redo log file之后，默认情况下，InnoDB存储引擎都需要调用一次 **fsync操作**,因为重做日志打开并没有 O_DIRECT选项，所以重做日志先写入到文件系统缓存。**为了确保重做日志写入到磁盘，必须进行一次 fsync操作**。fsync是一种系统调用操作，其fsync的效率取决于磁盘的性能，因此磁盘的性能也影响了事务提交的性能，也就是数据库的性能。
关于`fsync`和`write`：

`fsync`和`write`操作实际上是系统调用函数，在很多持久化场景都有使用到，比如 Redis 的AOF持久化中也使用到两个函数。`fsync`操作 将数据提交到硬盘中，强制硬盘同步，将一直阻塞到写入硬盘完成后返回，大量进行`fsync`操作就有性能瓶颈，而`write`操作将数据写到系统的页面缓存后立即返回，因此比fsync快。

DB宕机后重启，InnoDB会首先去查看数据页中LSN的数值，即数据页被刷新回磁盘的LSN（LSN实际上就是InnoDB使用的一个版本标记的计数）的大小，然后去查看redo log的LSN大小。如果数据页中的LSN值大，就说明数据页领先于redo log刷新回磁盘，不需要进行恢复；反之，需要从redo log中恢复数据。 

**当一个日志文件写满后，InnoDB会自动切换到另一个日志文件，但切换时会触发数据库检查点checkpoint**（checkpoint所做的事就是把脏页刷新回磁盘，当DB重启恢复时只需要恢复checkpoint之后的数据即可），导致InnoDB缓存脏页的小批量刷新，明显降低InnoDB的性能。可以通过增大log file size避免一个日志文件过快被写满，但是如果日志文件设置得过大，恢复时将需要更长的时间，同时也不便于管理。一般来说，平均每半个小时写满一个日志文件比较合适。 

## innodb_log_buffer_size
redo log buffer 

redo log buffer是一块内存区域，存放将要写入redo log文件的数据。redo log buffer大小是通过设置innodb_log_buffer_size参数来实现的。redo log buffer会周期性地刷新到磁盘的redo log file中。一个大的redo log buffer允许大事务在提交之前不写入磁盘的redo log文件。因此，如果有事务需要update、insert、delete许多记录，则可增加redo log buffer来节省磁盘I/O。 参数innodb_flush_log_at_trx_commit选项控制redo log buffer的内容何时写入redo log file，即控制redo log flush的频率。
即：对于可能产生大量更新记录的大事务，增加innodb_log_buffer_size的大小，可以避免InnoDB在事务提交前就执行不必要的日志写入磁盘操作。**因此，对于会在一个事务中更新、插入或删除大量记录的应用，可以通过增大innodb_log_buffer_size来减少日志写磁盘操作**，提高事务处理性能。 
附一张图：
![在这里插入图片描述](https://img-blog.csdnimg.cn/50649d900faa427ba445760f2d801fdb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAbWFyY2ggb2YgVGltZQ==,size_20,color_FFFFFF,t_70,g_se,x_16)


## 为什么Mysql不能直接更新磁盘上的数据而且设置这么一套复杂的机制来执行SQL了？

因为来一个请求就直接对磁盘文件进行随机读写，然后更新磁盘文件里的数据性能可能相当差。磁盘随机读写的性能是非常差的，所以直接更新磁盘文件是不能让数据库抗住很高并发的。
Mysql这套机制看起来复杂，但它可以保证每个更新请求都是更新内存BufferPool，然后顺序写日志文件，同时还能保证各种异常情况下的数据一致性。
更新内存的性能是极高的，然后顺序写磁盘上的日志文件的性能也是非常高的，要远高于随机读写磁盘文件。
正是通过这套机制，才能让我们的MySQL数据库在较高配置的机器上每秒可以抗下几干的读写请求。

随机IO顺序IO
第一种：假设我们所需要的数据是随机分散在磁盘的不同页的不同扇区中的，那么找到相应的数据需要等到磁臂（寻址作用）旋转到指定的页，然后盘片寻找到对应的扇区，才能找到我们所需要的一块数据，一次进行此过程直到找完所有数据，这个就是随机IO，读取数据速度较慢。

第二种：假设我们已经找到了第一块数据，并且其他所需的数据就在这一块数据后边，那么就不需要重新寻址，可以依次拿到我们所需的数据，这个就叫顺序IO。





## redo log 与binlog什么区别

1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。

2. redo log 是物理日志，**记录的是“在某个数据页上做了什么修改”**；binlog 是逻辑日志，**记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。**

3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。



redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数我建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。

sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。

也许有人会问，既然同样是记录事务日志，和之前介绍的二进制日志有什么区别？
 首先，二进制日志会记录所有与MySQL数据库有关的日志记录，包括InnoDB、MyISAM、Heap等其他存储引擎的日志。**而InnoDB存储引擎的重做日志只记录有关该存储引擎本身的事务日志**（是innodb独有redolog）。 其次，记录的内容不同，无论用户将二进制日志文件记录的格式设为STATEMENT还是ROW，又或者是MIXED，其记录的都是关于一个事务的具体操作内容，即该日志是逻辑日志。**而InnoDB存储引擎的重做日志文件记录的是关于每个页（Page）的更改的物理情况**。 此外，写入的时间也不同，二进制日志文件仅在事务提交前进行提交，即只写磁盘一次，不论这时该事务多大。而在事务进行的过程中，却不断有重做日志条目（redo entry）被写入到重做日志文件中。 在InnoDB存储引擎中，对于各种不同的操作有着不同的重做日志格式。到InnoDB 1.2.x版本为止，总共定义了51种重做日志类型。虽然各种重做日志的类型不同但格式类似：
重做日志条目是由4个部分组成： 
redo_log_type占用1字节，表示重做日志的类型 
space表示表空间的ID，但采用压缩的方式，因此占用的空间可能小于4字节 
page_no表示页的偏移量，同样采用压缩的方式 redo_log_body表示每个重做日志的数据部分，恢复时需要调用相应的函数进行解析

**从重做日志缓冲往磁盘写入时，是按512个字节，也就是一个扇区的大小进行写入**。因为扇区是写入的最小单位，因此可以保证写入必定是成功的。因此在重做日志的写入过程中不需要有doublewrite。 前面提到了从日志缓冲写入磁盘上的重做日志文件是按一定条件进行的，那这些条件有哪些呢？

在主线程中每秒会将重做日志缓冲写入磁盘的重做日志文件中，**不论事务是否已经提交**。
另一个触发写磁盘的过程是由参数innodb_flush_log_at_trx_commit控制，表示在提交（commit）操作时，处理重做日志的方式。 　　 

## MySQL的binlog有有几种录入格式？

mysql复制主要有三种方式：基于SQL语句的复制(statement-based replication, SBR)，基于行的复制(row-based replication, RBR)，混合模式复制(mixed-based replication, MBR)。对应的，binlog的格式也有三种：STATEMENT，ROW，MIXED

statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。缺点是在某些情况下会导致master-slave中的数据不一致( 如sleep()函数， last_insert_id()，以及user-defined functions(udf)等会出现问题)
row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。
mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。以上两种模式的混合使用，一般的复制使用STATEMENT模式保存binlog，对于STATEMENT模式无法复制的操作使用ROW模式保存binlog，MySQL会根据执行的SQL语句选择日志保存方式。

**在RC级用别下，主从复制用什么binlog格式：row格式，是基于行的复制！

## 设置binlog文件

在MySQL 5.1之前，所有的binlog都是基于SQL语句级别的。应用这种格式的binlog进行数据恢复时，如果SQL语句带有rand或uuid等函数，可能导致恢复出来的数据与原始数据不一致。从MySQL 5.1版本开始，MySQL引入了binlog_format参数。**这个参数有可选值statement和row**：statement就是之前的格式，基于SQL语句来记录；row记录的则是行的更改情况，可以避免之前提到的数据不一致的问题。做MySQL主从复制，statement格式的binlog可能会导致主备不一致，所以要使用row格式。我们还需要借助mysqlbinlog工具来解析和查看binlog中的内容。如果需要用binlog来恢复数据，标准做法是用mysqlbinlog工具把binlog中的内容解析出来，然后把解析结果整个发给MySQL执行。 

（2）redo重做日志文件：ib_logfile0、ib_logfile1是InnoDB引擎特有的、用于记录InnoDB引擎下事务的日志，它记录每页更改的物理情况。首先要搞明白的是已经有binlog了为什么还需要redo log，因为两者分工不同。**binlog主要用来做数据归档，但是并不具备崩溃恢复的能力，也就是说如果系统突然崩溃，重启后可能会有部分数据丢失**。Innodb将所有对页面的修改操作写入一个专门的文件，并在数据库启动时从此文件进行恢复操作，这个文件就是redo log file。redo log的存在可以完美解决这个问题。默认情况下，每个InnoDB引擎至少有一个重做日志组，每组下至少有两个重做日志文件，例如前面提到的ib_logfile0和ib_logfile1。重做日志组中的每个日志文件大小一致且循环写入，也就是说先写iblogfile0，写满了之后写iblogfile1，一旦iblogfile1写满了，就继续写iblogfile0。当innodb log设置过大的时候，可能会导致系统崩溃后恢复需要很长的时间；当innodb log设置过小的时候，在一个事务产生大量日志的情况下，需要多次切换重做日志文件。 

（3）共享表空间和独立表空间：在MySQL 5.6.6之前的版本中，InnoDB默认会将所有的数据库InnoDB引擎的表数据存储在一个共享表空间ibdata1中，这样管理起来很困难，增删数据库的时候，ibdata1文件不会自动收缩，单个数据库的备份也将成为问题。为了优化上述问题，在MySQL 5.6.6之后的版本中，独立表空间innodb_file_per_table参数默认开启，每个数据库的每个表都有自已独立的表空间，每个表的数据和索引都会存在自己的表空间中。即便是innnodb表指定为独立表空间，用户自定义数据库中的某些元数据信息、回滚（undo）信息、插入缓冲（change buffer）、二次写缓冲（double writebuffer）等还是存放在共享表空间，所以又称为系统表空间。 这个系统表空间由一个或多个数据文件组成，默认情况下其包含一个叫ibdata1的系统数据文件，位于MySQL数据目录（datadir）下。系统表空间数据文件的位置、大小和数目由参数innodb_data_home_dir和innodb_data_file_path启动选项控制。 当开启独立表空间参数innodb_file_per_table选项时，该表创建于自己的数据文件中，而非创建于系统表空间中。这样的好处在于在drop或者truncate时空间可以被收回至操作系统用作其他用途，而且进行单表空间在不同实例间移动，而不必处理整个数据库表空间。 如图1-8所示，参数innodb_file_per_table独立表空间选项开启，同时通过“show variables like 'innodb_data%';”命令可以查询共享表空间（系统表空间）的文件信息。 





默认情况下，二进制日志是关闭的，可以通过修改MySQL的配置文件来启动和设置二进制日志。 my.ini中［mysqld］组下面有几个设置是关于二进制日志的： log-bin定义开启二进制日志；path表明日志文件所在的目录路径；filename指定日志文件的名称，如文件的全名为filename.000001、filename.000002等，除了上述文件之外，还有一个名称为filename.index的文件，文件内容为所有日志的清单，可以使用记事本打开该文件。 expire_logs_days定义了MySQL清除过期日志的时间，即二进制日志自动删除的天数。默认值为0，表示 “没有自动删除”。当MySQL启动或刷新二进制日志时，可能删除该文件。 max_binlog_size定义了单个文件的大小限制，如果二进制日志写入的内容大小超出给定值，日志就会发生滚动（关闭当前文件，重新打开一个新的日志文件）。不能将该变量设置为大于1GB或小于4096B，默认值是1GB。 如果正在使用大的事务，二进制日志文件大小还可能会超过max_binlog_size定义的大小。 

![image-20210906202922439](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210906202922439.png)

在my.ini配置文件中的［mysqld］组下，添加以下几个参数与参数值： 添加完毕之后，关闭并重新启动MySQL服务进程，即可打开二进制日志，然后可以通过SHOW VARIABLES LIKE 'log_%'来查询

修改路径：

log-bin = "D:/MYSQL/log/binlog"

show binary logs语句可以查看当前的二进制日志文件的个数及其文件名。MySQL二进制日志并不能直接查看，如果要查看日志内容，可以通过MySQLbinlog命令查看。 

1．使用RESET MASTER语句删除所有二进制日志文件 RESTE MASTER语法如下： 执行完该语句后，所有二进制日志将被删除，MySQL会重新创建二进制日志，新的日志文件扩展名将重新从000001开始编号。 

2．使用PURGE MASTER LOGS语句删除指定日志文件 



## redolog详解

![image-20220314091609708](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220314091609708.png)

每个mini-transaction对应每一条DML操作，比如一条update语句，其由一个mini-transaction来保证，对数据修改后，产生redo1，首先将其写入mini-transaction私有的Buffer中，update语句结束后，将redo1从私有Buffer拷贝到公有的Log Buffer中。当整个外部事务提交时，将redo log buffer再刷入到redo log file中。



重做日志(redo log)用来保证事务的持久性，即事务ACID中的D。实际上它可以分为以下两种类型：

- 物理Redo日志
- 逻辑Redo日志

在InnoDB存储引擎中，**大部分情况下 Redo是物理日志，记录的是数据页的物理变化**。而逻辑Redo日志，不是记录页面的实际修改，而是记录修改页面的一类操作，比如新建数据页时，需要记录逻辑日志。关于逻辑Redo日志涉及更加底层的内容，这里我们只需要记住绝大数情况下，Redo是物理日志即可，DML对页的修改操作，均需要记录Redo.



Redo log的主要作用是用于数据库的崩溃恢复

Redo 的组成

Redo log可以简单分为以下两个部分：

- 一是内存中重做日志缓冲 (redo log buffer),是易失的，在内存中
- 二是重做日志文件 (redo log file)，是持久的，保存在磁盘中

什么时候写Redo?



- 在数据页修改完成之后，在脏页刷出磁盘之前，写入redo日志。注意的是先修改数据，后写日志
- **redo日志比数据页先写回磁盘**
- 聚集索引、二级索引、undo页面的修改，均需要记录Redo日志。

下面以一个更新事务为例，宏观上把握redo log 流转过程，如下图所示：

![img](https:////upload-images.jianshu.io/upload_images/5652417-a5f90ca64ed10d4d.png?imageMogr2/auto-orient/strip|imageView2/2/w/662/format/webp)

mysql_redo

- 第一步：先将原始数据从磁盘中读入内存中来，修改数据的内存拷贝
- 第二步：生成一条重做日志并写入redo log buffer，记录的是数据被修改后的值
- 第三步：当事务commit时，将redo log buffer中的内容刷新到 redo log file，对 redo log file采用追加写的方式（这里有误，实际上刷新到file的时机应该取决于redo log buffer的大小）
- 第四步：定期将内存中修改的数据刷新到磁盘中



**redo如何保证 事务的持久性？**

InnoDB是事务的存储引擎，其通过**Force Log at Commit 机制**实现事务的持久性，即当事务提交时，先将 redo log buffer 写入到 redo log file 进行持久化，待事务的commit操作完成时才算完成。这种做法也被称为 **Write-Ahead Log(预先日志持久化)**，<font color="red">在持久化一个数据页之前，先将内存中相应的日志页持久化。</font>

为了保证每次日志都写入redo log file，在每次将redo buffer写入redo log file之后，默认情况下，InnoDB存储引擎都需要调用一次 **fsync操作**,因为重做日志打开并没有 O_DIRECT选项，所以重做日志先写入到文件系统缓存。**为了确保重做日志写入到磁盘，必须进行一次 fsync操作**。fsync是一种系统调用操作，其fsync的效率取决于磁盘的性能，因此磁盘的性能也影响了事务提交的性能，也就是数据库的性能。
 **(O_DIRECT选项是在Linux系统中的选项，使用该选项后，对文件进行直接IO操作，不经过文件系统缓存，直接写入磁盘)**

上面提到的**Force Log at Commit机制**就是靠InnoDB存储引擎提供的参数 `innodb_flush_log_at_trx_commit`来控制的，该参数可以控制 redo log刷新到磁盘的策略，设置该参数值也可以允许用户设置非持久性的情况发生，具体如下：

- 当设置参数为1时，（默认为1），表示事务提交时必须调用一次 `fsync` 操作，最安全的配置，保障持久性
- 当设置参数为2时，则在事务提交时只做 **write** 操作，只保证将redo log buffer写到系统的页面缓存中，不进行fsync操作，因此如果MySQL数据库宕机时 不会丢失事务，但操作系统宕机则可能丢失事务
- 当设置参数为0时，表示事务提交时不进行写入redo log操作，这个操作仅在master thread 中完成，而在master thread中每1秒进行一次重做日志的fsync操作，因此实例 crash 最多丢失1秒钟内的事务。（master thread是负责将缓冲池中的数据异步刷新到磁盘，保证数据的一致性）

`fsync`和`write`操作实际上是系统调用函数，在很多持久化场景都有使用到，比如 Redis 的AOF持久化中也使用到两个函数**。`fsync`操作 将数据提交到硬盘中，强制硬盘同步，将一直阻塞到写入硬盘完成后返回，大量进行`fsync`操作就有性能瓶颈，而`write`操作将数据写到系统的页面缓存后立即返回**，后面依靠系统的调度机制将缓存数据刷到磁盘中去,其顺序是user buffer——> page cache——>disk。

![img](https:////upload-images.jianshu.io/upload_images/5652417-d150cdad4d6d53f8.png?imageMogr2/auto-orient/strip|imageView2/2/w/364/format/webp)

usebuffer_pagecache_disk

**除了上面谈到的Force Log at Commit机制保证事务的持久性，实际上重做日志的实现还要依赖于mini-transaction。**

**重做日志的写入流程**

![img](https:////upload-images.jianshu.io/upload_images/5652417-e1d3736a71806d90.png?imageMogr2/auto-orient/strip|imageView2/2/w/711/format/webp)

重做日志写入流程

上图表示了重做日志的写入流程，每个mini-transaction对应每一条DML操作，比如一条update语句，其由一个mini-transaction来保证，对数据修改后，产生redo1，首先将其写入mini-transaction私有的Buffer中，update语句结束后，将redo1从私有Buffer拷贝到公有的Log Buffer中。当整个外部事务提交时，将redo log buffer再刷入到redo log file中。



redo log buffer 

redo log buffer是一块内存区域，存放将要写入redo log文件的数据。redo log buffer大小是通过设置innodb_log_buffer_size参数来实现的。redo log buffer会周期性地刷新到磁盘的redo log file中。一个大的redo log buffer允许大事务在提交之前不写入磁盘的redo log文件。因此，如果有事务需要update、insert、delete许多记录，则可增加redo log buffer来节省磁盘I/O。 参数innodb_flush_log_at_trx_commit选项控制redo log buffer的内容何时写入redo log file，即控制redo log flush的频率。

 innodb_flush_log_at_trx_commit的参数取值及其说明如下： 

设置为0：在提交事务时，InnoDB不会立即触发将缓存日志log buffer写到磁盘文件的操作，而是每秒触发一次缓存日志回写磁盘操作，并调用操作系统fsync刷新I/O缓存。 

设置为1：每次事务提交时MySQL都会立即把log buffer的数据写入redo log file，并且调用操作系统fsync刷新I/O缓存（刷盘操作）。值为1时，每次事务提交都持久化一次，当然是最安全的，但是数据库性能会受影响，I/O负担重，适合对安全要求极高的交易系统场景（建议配置SSD硬盘提高I/O能力）。 

设置为2：每次事务提交时MySQL都会把redo log buffer的数据写入redo log file，但是flush（刷到磁盘）操作并不会同时进行，而是每秒执行一次flush（磁盘I/O缓存刷新）。注意，由于进程调度策略问题，并不能保证百分之百的“每秒”。 刷写其实是两个操作，即刷flush）和写（write）。区分这两个概念（两个系统调用）是很重要的。在大多数的操作系统中，把InnoDB的log buffer（内存）写入日志（调用系统调用write），只是简单地把数据移到操作系统缓存（内存）中，并没有实际持久化数据。 ==通常设置为0的时候，mysqld进程的崩溃会导致上一秒钟的所有事务数据丢失==。当该值为2时，表示事务提交时不写入重做日志文件，而是写入文件系统缓存中，当DB数据库发生故障时能恢复数据，如果操作系统也出现宕机，就会丢失掉文件系统没有及时写入磁盘的数据。 设为1当然是最安全的，适合数据安全性要求非常高的而且磁盘I/O写能力足够支持业务，比如订单、交易、充值支付消费系统。如果对数据一致性和完整性要求不高，完全可以设为2，推荐使用带蓄电池后备电源的缓存cache，防止系统断电异常。如果只要求性能，例如高并发写的日志服务器，就设置为0来获得更高性能。 



引入Redo日志是为了更好地提升数据库整体性能，其实Redo日志还有以下作用： 

·　快速提交。 

·　恢复实例。 

·　增量备份，以及恢复到某一时间点。 

·　复制（在MySQL中使用Binlog复制，也有一些大厂商实现了基于Redo日志的复制）。 

## Redo日志落盘时间点 

综合前面介绍的知识，我们再来看一下Redo日志的落盘时间点。Redo日志的写入在事务开始后就会一直进行，首先会写入日志缓冲区（Log Buffer）中，缓冲区的大小由innodb_log_buffer_size参数控制，随后从日志缓冲区中刷新到磁盘上，而并不是在事务提交之后再刷新到磁盘上的，在MySQL内部有一些触发条件会自动将内存中的脏数据刷新到磁盘上（MySQL后台会有一些线程每隔一定的时间或者当日志缓冲区写入达到一定比例时，执行刷新操作）。这样设计的好处也是显而易见的，如果都等提交之后再将日志缓冲区中的数据刷新到磁盘上，那么几个很大的事务就会将日志缓冲区撑满。 根据上面所述，我们会想到一种情况：虽然事务没有提交，但相关的操作日志是有可能会被刷新到磁盘上的。事实上确实存在这种情况，解决的办法是：在提交事务时在Redo日志文件中添加一个commit标记表示对应的记录已经提交，这样即可实现快速提交。 

**Redo日志格式**

 Redo日志格式分为两种：物理日志和逻辑日志。 

1．物理日志 物理日志即将变化对象的新旧状态都记录到日志中。物理日志记录看起来如下所示。 

![image-20210923162632549](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923162632549.png)

从结构体中可以看出，当对象发生变化时将其新旧值都记录下来了，这样在做实例恢复时操作也非常简单，只需要用新值覆盖旧值即可。但是当对象中的内容很多并且改变比较少时，则日志可以进行压缩，只记录变化的部分。例如，当一个较大的对象文件中某一个页面发生了变化时，则日志只记录变化的页面，而不需要记录整个对象的新旧值。类似的，如果页面中只有一个字段发生了变化，则日志记录看起来如下所示。 

![image-20210923162717728](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923162717728.png)

压缩后可以通过pageno和offset定位到被修改的字段，用新值覆盖旧值。这样可以减少日志量，但是存在问题，在数据库中插入一条数据可能会造成数据页分裂，这部分索引的变更也需要记录到日志中，所以日志量还是有可能会很大。 



## undolog

在InnoDB存储引擎中，undo log分为：

- insert undo log
- update undo log

insert undo log是指在insert 操作中产生的undo log，因为insert操作的记录，只对事务本身可见，对其他事务不可见。故该undo log可以在事务提交后直接删除，不需要进行purge操作。

而update undo log记录的是对delete 和update操作产生的undo log，该undo log可能需要提供MVCC机制，因此不能再事务提交时就进行删除。提交时放入undo log链表，等待purge线程进行最后的删除。

补充：purge线程两个主要作用是：清理undo页和清除page里面带有Delete_Bit标识的数据行。在InnoDB中，事务中的Delete操作实际上并不是真正的删除掉数据行，而是一种Delete Mark操作，在记录上标识Delete_Bit，而不删除记录。是一种"假删除",只是做了个标记，真正的删除工作需要后台purge线程去完成。

undo log 是否是redo log的逆过程？其实从前文就可以得出答案了，undo log是逻辑日志，对事务回滚时，只是将数据库逻辑地恢复到原来的样子，而redo log是物理日志，记录的是数据页的物理变化，显然undo log不是redo log的逆过程。

```bash
假设有A、B两个数据，值分别为1,2.
1. 事务开始
2. 记录A=1到undo log
3. 修改A=3
4. 记录A=3到 redo log
5. 记录B=2到 undo log
6. 修改B=4
7. 记录B=4到redo log
8. 将redo log写入磁盘
9. 事务提交
```





（4）undo log：undo log是回滚日志，如果事务回滚，则需要依赖undo日志进行回滚操作。MySQL在进行事务操作的同时，会记录事务性操作修改数据之前的信息，就是undo日志，确保可以回滚到事务发生之前的状态。innodb的undo log存放在ibdata1共享表空间中，当开启事务后，事务所使用的undo log会存放在ibdata1中，即使这个事务被关闭，undo log也会一直占用空间。为了避免ibdata1共享表空间暴涨，建议将undo log单独存放。innodb_undo_directory参数指定单独存放undo表空间的目录，该参数实例初始化之后不可直接改动（可以通过先停库，修改配置文件，然后移动undo表空间文件的方式去修改该参数）；innodb_undo_tablespaces参数指定单独存放的undo表空间个数，推荐设置为大于等于3；innodb_undo_logs参数指定回滚段的个数，默认为128个。MySQL 5.7引入了新的参数innodb_undo_log_truncate，这个参数开启后可在线收缩拆分出来的undo表空间。

（5）临时表空间：存储临时对象的空间，比如临时表对象等，参数innodb_temp_data_file_path可以看到临时表空间的信息，上限设置为5GB。

show variables like '%undo%';

 

## MySQL的binlog有有几种录入格式?分别有什么区别?

有三种格式,statement,row和mixed.
statement模式下,记录单元为语句.即每一个sql造成的影响会记录.由于sql的执行是有上下文的,因此在保存的时候需要保存相关的信息,同时还有一些使用了函数之类的语句无法被记录复制.
row级别下,记录单元为每一行的改动,基本是可以全部记下来但是由于很多操作,会导致大量行的改动(比如alter table),因此这种模式的文件保存的信息太多,日志量太大.
mixed. 一种折中的方案,普通操作使用statement记录,当无法使用statement的时候使用row.此外,新版的MySQL中对row级别也做了一些优化,当表结构发生变化的时候,会记录语句而不是逐行记录.





# 查询日志-log_output参数

log_output参数是什么意思？表示慢日志输出到文件还是表中。

show global variables like 'log_output'；

 

默认参数，log_output='FILE'，表示慢日志输出到了文件中。

![img](https://img2020.cnblogs.com/blog/1262014/202005/1262014-20200521142556537-1480144464.png)

 

 

set global log_output='TABLE';



此时慢日志就输出到了mysql数据库的系统表中：select * from mysql.slow_log; 可以查看到。也可以select * from mysql.general_log;

 ![image-20210923110908821](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923110908821.png)

set global log_output='FILE,TABLE';

此时慢日志可以同时记录到文件，和mysql.slow_log系统表中。

 SELECT CONNECTION_ID()
[MySQL的这个函数返回的是这个连接的连接ID或者thread ID。对于已经建立的连接的客户端，都有一个唯一的连接ID]

-![image-20210923103909422](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210923103909422.png)

🐄🐄🐄查询日志会在语句一开始执行时就进行记录，而慢查询语句会在语句执行完成并释放完所有的锁之后进行记录，两者的记录语句都与事务是否提交无关。一条语句是否被判定为慢查询语句，**要看该语句真正的执行时间（排除锁等待时间）是否超过系统变量long_query_time的值，超过了就被判定为慢查询语句**，并记录到慢查询日志中（记录到慢查询日志中的语句执行时间包含了锁等待时间），没超过就不会被记录到慢查询日志中。通过上述示例过程也可以证实：当会话3中的DML语句执行完之后，慢查询日志中的记录总是包含锁等待时间的（也测试过log_output=FILE，结果一样）。 

# 数据量大的表优化

1.限制查询的范围

务必禁止不带任何限制数据范围条件的查询

2.读写分离

拆分为主库和从库

3.垂直分区 

比如一个表里有用户id和用户详细信息、登录信心，把详细信息这一列分到另一个表中

4.水平分区

![image-20220211210641340](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220211210641340.png)

![image-20220211210713877](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220211210713877.png)

水平拆分的优点是:

- 不存在单库大数据和高并发的性能瓶颈
- 应用端改造较少
- 提高了系统的稳定性和负载能力

缺点是：

- 分片事务一致性难以解决
- 跨节点Join性能差，逻辑复杂
- 数据多次扩展难度跟维护量极大

分片原则

- 能不分就不分，参考单表优化
- 分片数量尽量少，分片尽量均匀分布在多个数据结点上，因为一个查询SQL跨分片越多，则总体性能越差，虽然要好于所有数据在一个分片的结果，只在必要的时候进行扩容，增加分片数量
- 分片规则需要慎重选择做好提前规划，分片规则的选择，需要考虑数据的增长模式，数据的访问模式，分片关联性问题，以及分片扩容问题，最近的分片策略为范围分片，枚举分片，一致性Hash分片，这几种分片都有利于扩容
- 尽量不要在一个事务中的SQL跨越多个分片，分布式事务一直是个不好处理的问题
- 查询条件尽量优化，尽量避免Select * 的方式，大量数据结果集下，会消耗大量带宽和CPU资源，查询尽量避免返回大量结果集，并且尽量为频繁使用的查询语句建立索引。
- 通过数据冗余和表分区赖降低跨库Join的可能

水平拆分解决方案

由于水平拆分牵涉的逻辑比较复杂，当前也有了不少比较成熟的解决方案。这些方案分为两大类：客户端架构和代理架构。

客户端架构

通过修改数据访问层，如JDBC、Data Source、MyBatis，通过配置来管理多个数据源，直连数据库，并在模块内完成数据的分片整合，一般以Jar包的方式呈现

代理架构

通过独立的中间件来统一管理所有数据源和数据分片整合，后端数据库集群对前端应用程序透明，需要独立部署和运维代理组件。如MyCat或者Atlas

# 慢查询如何优化

慢查询的优化首先要搞明白慢的原因是什么，是查询条件没有命中索引？是load了不需要的数据列？还是数据量太大？所以优化也是针对这三个方向：

首先分析语句看是否load了额外的数据，可能是查询了多余的并且抛弃掉了，可能是加载了许多结果中并不需要的列；

分析语句中的执行计划然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引；

还可以考虑表中数据量是否太大

# 一条SQL语句执行得很慢的原因有哪些



我们还得分以下两种情况来讨论。

**1、大多数情况是正常的，只是偶尔会出现很慢的情况。**

**2、在数据量不变的情况下，这条SQL语句一直以来都执行的很慢。**

一条 SQL 大多数情况正常，偶尔才能出现很慢的情况，针对这种情况，我觉得这条SQL语句的书写本身是没什么问题的，而是其他原因导致的，那会是什么原因呢？

1、数据库在刷新脏页（flush）

当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在**内存**中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到**磁盘**中去，而是把这些更新的记录写入到 redo log 日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到**磁盘**中去。

【当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。】

**刷脏页有下面4种场景（后两种不用太关注“性能”问题）：**

- **redolog写满了：**redo log 里的容量是有限的，如果数据库一直很忙，更新又很频繁，这个时候 redo log 很快就会被写满了，这个时候就没办法等到空闲的时候再把数据同步到磁盘的，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，**就会导致我们平时正常的SQL语句突然执行的很慢**，所以说，数据库在在同步数据到磁盘的时候，就有可能导致我们的SQL语句执行的很慢了。
- **内存不够用了：**如果一次查询较多的数据，恰好碰到所查数据页不在内存中时，需要申请内存，而此时恰好内存不足的时候就需要淘汰一部分内存数据页，如果是干净页，就直接释放，如果恰好是脏页就需要刷脏页。
- **MySQL 认为系统“空闲”的时候：**这时系统没什么压力。
- **MySQL 正常关闭的时候：**这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。

2、拿不到锁

这个就比较容易想到了，我们要执行的这条语句，刚好这条语句涉及到的**表**，别人在用，并且加锁了，我们拿不到锁，只能慢慢等待别人释放锁了。或者，表没有加锁，但要使用到的某个一行被加锁了，这个时候，我也没办法啊。

如果要判断是否真的在等待锁，我们可以用 **show processlist**这个命令来查看当前的状态哦，这里我要提醒一下，有些命令最好记录一下，反正，我被问了好几个命令，都不知道怎么写，呵呵。

 *SHOW* *PROCESSLIST*显示哪些线程正在运行。您也可以使用mysqladmin *processlist*语句得到此信息。

**针对一直都这么慢的情况**

如果在数据量一样大的情况下，这条 SQL 语句每次都执行的这么慢，那就就要好好考虑下你的 SQL 书写了，下面我们来分析下哪些原因会导致我们的 SQL 语句执行的很不理想。

例如：没有用到索引（字段没有索引或字段有索引但却没有用到索引）

也可能是数据库没有选择索引：

我们知道，主键索引和非主键索引是有区别的，主键索引存放的值是**整行字段的数据**，而非主键索引上存放的值不是整行字段的数据，而且存放**主键字段的值**。

比如

select * from t where 100 < c and c < 100000;

也就是说，我们如果走 c 这个字段的索引的话，最后会查询到对应主键的值，然后，再根据主键的值走主键索引，查询到整行数据返回。

总之就算你在 c 字段上有索引，系统也并不一定会走 c 这个字段上的索引，而是有可能会直接扫描扫描全表，找出所有符合 100 < c and c < 100000 的数据。

**为什么会这样呢？**

其实是这样的，系统在执行这条语句的时候，会进行预测：究竟是走 c 索引扫描的行数少，还是直接扫描全表扫描的行数少呢？显然，扫描行数越少当然越好了，因为扫描行数越少，意味着I/O操作的次数越少。

如果是扫描全表的话，那么扫描的次数就是这个表的总行数了，假设为 n；而如果走索引 c 的话，我们通过索引 c 找到主键之后，还得再通过主键索引来找我们整行的数据，也就是说，需要走两次索引。而且，我们也不知道符合 100 c < and c < 10000 这个条件的数据有多少行，万一这个表是全部数据都符合呢？这个时候意味着，走 c 索引不仅扫描的行数是 n，同时还得每行数据走两次索引。

**所以呢，系统是有可能走全表扫描而不走索引的。那系统是怎么判断呢？**

判断来源于系统的预测，也就是说，如果要走 c 字段索引的话，系统会预测走 c 字段索引大概需要扫描多少行。如果预测到要扫描的行数很多，它可能就不走索引而直接扫描全表了。

**系统是怎么预测的呢？**

系统是通过**索引的区分度**来判断的，一个索引上不同的值越多，意味着出现相同数值的索引越少，意味着索引的区分度越高。我们也把区分度称之为**基数**，即区分度越高，基数越大。所以呢，基数越大，意味着符合 100 < c and c < 10000 这个条件的行数越少。

所以呢，一个索引的基数越大，意味着走索引查询越有优势。

**那么问题来了，怎么知道这个索引的基数呢？**

系统当然是不会遍历全部来获得一个索引的基数的，代价太大了，索引系统是通过遍历部分数据，也就是通过**采样**的方式，来预测索引的基数的。

**扯了这么多，重点的来了**，居然是采样，那就有可能出现**失误**的情况，也就是说，c 这个索引的基数实际上是很大的，但是采样的时候，却很不幸，把这个索引的基数预测成很小。例如你采样的那一部分数据刚好基数很小，然后就误以为索引的基数很小。**然后就呵呵，系统就不走 c 索引了，直接走全部扫描了**。

不过呢，我们有时候也可以通过强制走索引的方式来查询，例如

```
select * from t force index(c) where c < 100 and c < 100000;
```

force index() 方法强制使用这个索引

我们也可以通过

```
show index from t;
```

来查询索引的基数和实际是否符合，如果和实际很不符合的话，我们可以重新来统计索引的基数，可以用这条命令

```
analyze table t;
```

来重新统计分析。

**既然会预测错索引的基数，这也意味着，当我们的查询语句有多个索引的时候，系统有可能也会选错索引哦**，这也可能是 SQL 执行的很慢的一个原因。

# 数据库 SQL 开发规范

1. 建议使用预编译语句进行数据库操作

预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题。

只传参数，比传递 SQL 语句更高效。

相同语句可以一次解析，多次使用，提高处理效率。

2. 避免数据类型的隐式转换

隐式转换会导致索引失效如:

- 

```
select name,phone from customer where id = '111';
```

3. 充分利用表上已经存在的索引

避免使用双%号的查询条件。如：`a like '%123%'`，（如果无前置%,只有后置%，是可以用到列上的索引的）

一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。

在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧，使用 left join 或 not exists 来优化 not in 操作，因为 not in 也通常会使用索引失效。

3. 禁止使用 SELECT * 必须使用 SELECT <字段列表> 查询

**原因：**

•消耗更多的 CPU 和 IO 以网络带宽资源•无法使用覆盖索引•可减少表结构变更带来的影响

4. 禁止使用不含字段列表的 INSERT 语句

如：

- 

```
insert into values ('a','b','c');
```

应使用：

- 

```
insert into t(c1,c2,c3) values ('a','b','c');
```

8. 避免使用子查询，可以把子查询优化为 join 操作

通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。

**子查询性能差的原因：**

子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。

由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。

9. 避免使用 JOIN 关联太多的表

对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。

在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。

如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。

同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。

10. 减少同数据库的交互次数

数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。

11. 对应同一列进行 or 判断时，使用 in 代替 or

in 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。`OR`的效率是n级别，`IN`的效率是log(n)级别

12. 禁止使用 order by rand() 进行随机排序

order by rand() 会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的 CPU 和 IO 及内存资源。

推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。

13. WHERE 从句中禁止对列进行函数转换和计算

对列进行函数转换或计算时会导致无法使用索引

**不推荐：**

- 

```
where date(create_time)='20190101'
```

**推荐：**

- 

```
where create_time >= '20190101' and create_time < '20190102'
```

14. 在明显不会有重复值时使用 UNION ALL 而不是 UNION

•UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作•UNION ALL 不会再对结果集进行去重操作

15. 拆分复杂的大 SQL 为多个小 SQL

•大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL•MySQL 中，一个 SQL 只能使用一个 CPU 进行计算•SQL 拆分后可以通过并行执行来提高处理效率



- 尽量避免在`WHERE`子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描
- 对于连续数值，使用`BETWEEN`不用`IN`：`SELECT id FROM t WHERE num BETWEEN 1 AND 5`
- 列表数据不要拿全表，要使用`LIMIT`来分页，每页数量也不要太大

# 执行计划

table | type | possible_keys | key | key_len | ref | rows | Extra

table：哪个表

type：这个很重要，是说类型，all（全表扫描），const（读常量，最多一条记录匹配），eq_ref（走主键，一般就最多一条记录匹配），index（扫描全部索引），range（扫描部分索引）

possible_keys：显示可能使用的索引，但不一定实际查询使用

key：实际使用的索引

key_len：使用索引的长度

ref：联合索引的哪一列被用了，如果可能的话是一个常量（如name = 'zhangsan')用库.表.字段来表示



rows：一共扫描和返回了多少行，大致估算出找到所需的记录需要读取的行数

extra：using filesort（需要额外进行排序，说明mysql会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取，mysql中无法利用索引完成的排序操作称为“文件排序”，这种情况尽量要优化，所以一般利用索引排序可以提高性能），using temporary（mysql构建了临时表，比如排序的时候，常见于order by ,group by），using where（就是对索引扫出来的数据再次根据where来过滤出了结果），using index 表示mysql使用覆盖索引

mysql创建索引

create index age index(索引名称) on `test`(age);age为表字段

id:(下面这个图中的s1即为衍生表)

![image-20210901173355081](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210901173355081.png)

select_type:

![image-20210901173952622](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210901173952622.png)

type:访问类型

性能顺序：

📣📣📣system>const>eq_ref>ref>range>index>all

system:表只有一行记录，这是Const的特例，且只能用于myisam和memory表。如果是 Innodb引擎表，type列在这个情况通常都是all或者index

![image-20210901174856402](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210901174856402.png)

const: select * from t1 where id = 1(1为常量，id为主键) 最多只有一行记录匹配。当联合主键或唯一索引的所有字段跟常量值比较时，join类型为const。其他数 据会也叫做唯一索引扫描

eq_ref:用到了索引且结果只有一条记录与索引匹配

例子：select * from t1,t2 where t1.id = t2.id(t2表是all类型，t1为eq_ref类型)

range的例子：explain select * from t1 where id between 30 and 20

select * from t1 where id in(1,2,6)

index例子：select id from t1



# 关于索引的知识点---索引为何使得查询变快？

❗❗❗❗❗❗❗❗【正确的创建合适的索引是数据库性能优化的基础】❗❗❗❗❗❗❗❗



![image-20210725014336303](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210725014336303.png)

索引是关系型数据库为了加速对表中行数据检索的（磁盘存储的）数据结构

一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往是存储在磁盘上的文件中的（可能存储在单独的索引文件中，也可能和数据一起存储在数据文件中）。
我们通常所说的索引，包括聚集索引、覆盖索引、组合索引、前缀索引、唯一索引等，没有特别说明，默认都是使用B+树结构组织（多路搜索树，并不一定是二叉的）的索引。

索引的基本原理
索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。
索引的原理：就是把无序的数据变成有序的查询

1. 把创建了索引的列的内容进行排序
2. 对排序结果生成倒排表
3. 在倒排表内容上拼上数据地址链
4. 在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据



![image-20210725014733507](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210725014733507.png)





索引的常用数据结构?
索引的数据结构和具体存储引擎的实现有关, 在MySQL中使用较多的索引有Hash索引,B+树索引等,而我们经常使用的InnoDB存储引擎的默认索引实现为:B+树索引



![image-20210818152405228](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210818152405228.png)



数据库访问速度快的一个很重要的原因就在于索引index的作用。也就是这篇文章的主要想介绍的内容，为什么索引可以让数据库查询变快？

# 普通索引 V.S 唯一索引

普通索引可重复，唯一索引和主键一样不能重复。

唯一索引可作为数据的一个合法验证手段，例如学生表的身份证号码字段，人为规定该字段不得重复，那么就使用唯一索引。（一般设置学号字段为主键）

**主键 V.S 唯一索引**

主键保证DB的每一行都是唯一、不重复，比如身份证，学号等，不重复。

唯一索引的作用跟主键一样。

但在一张表里面只能有一个主键，不能为空，唯一索引可有多个。唯一索引可有一条记录为null。

```
select id from T where k=4
```

通过B+树从root开始层序遍历到叶节点，数据页内部通过二分搜索：

- 普通索引
  查找到满足条件的第一个记录(4,400)后，继续查找下个记录，直到碰到第一个不满足k=4的记录
- 唯一索引
  查到第一个满足条件的，就停止搜索

看起来性能差距很小。

InnoDB数据按数据页单位读写。即读一条记录时，并非将该一个记录从磁盘读出，而以页为单位，将其整体读入内存。

所以普通索引，多了一次“查找和判断下一条记录”的操作，即一次指针寻找和一次计算。

若k=4记录恰为该数据页的最后一个记录，则此时要取下个记录，还得读取下个数据页。

对整型字段，一个数据页可存近千个key，因此这种情况概率其实也很低。因此计算平均性能差异时，可认为该操作成本对CPU开销忽略不计。

##  更新性能

往表中插入一个新记录(4,400)，InnoDB会有什么反应？

这要看该记录要更新的目标页是否在内存：

在内存

- 普通索引
  找到3和5之间的位置，插入值，结束。
- 唯一索引
  找到3和5之间的位置，判断到没有冲突，插入值，结束。

只是一个判断的差别，耗费微小CPU时间。

不在内存

- 唯一索引
  将数据页读入内存，判断到没有冲突，插入值，结束。
- 普通索引
  将更新记录在change buffer，结束。

将数据从磁盘读入内存涉及随机I/O访问，是DB里成本最高的操作之一。而change buffer可以减少随机磁盘访问，所以更新性能提升明显。唯一索引用不了change buffer

**5 索引选择最佳实践**

普通索引、唯一索引在查询性能上无差别，主要考虑更新性能。所以，推荐尽量选择普通索引。

若所有更新后面，都紧跟对该记录的查询，就该关闭change buffer。其它情况下，change buffer都能提升更新性能。

普通索引和change buffer的配合使用，对数据量大的表的更新优化还是明显的。

在使用机械硬盘时，change buffer收益也很大。所以，当你有“历史数据”库，且出于成本考虑用机械硬盘，应该关注这些表里的索引，尽量用普通索引，把change buffer开大，确保“历史数据”表的数据写性能。



# 百万级数据库优化方案

***\*
\****

1.对查询进行优化，要尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。

2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：

```
select id from t where num is null
```

最好不要给数据库留NULL，尽可能的使用 NOT NULL填充数据库.

备注、描述、评论之类的可以设置为 NULL，其他的，最好不要使用NULL。

不要以为 NULL 不需要空间，比如：char(100) 型，在字段建立时，空间就固定了， 不管是否插入值（NULL也包含在内），都是占用 100个字符的空间的，如果是varchar这样的变长字段， null 不占用空间。


可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：

```
select id from t where num = 0
```


3.应尽量避免在 where 子句中使用 != 或 <> 操作符，否则将引擎放弃使用索引而进行全表扫描。

4.应尽量避免在 where 子句中使用 or 来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描，如：

```
select id from t where num=10 or Name = 'admin'
```

可以这样查询：

```
select id from t where num = 10
union all
select id from t where Name = 'admin'
```


5.in 和 not in 也要慎用，否则会导致全表扫描，如：

```
select id from t where num in(1,2,3)
```

对于连续的数值，能用 between 就不要用 in 了：

```
select id from t where num between 1 and 3
```

很多时候用 exists 代替 in 是一个好的选择：

```
select num from a where num in(select num from b)
```

用下面的语句替换：

```
select num from a where exists(select 1 from b where num=a.num)
```

 

6.下面的查询也将导致全表扫描：

```
select id from t where name like ‘%abc%’
```

若要提高效率，可以考虑全文检索。

7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：

```
select id from t where num = @num
```

可以改为强制查询使用索引：

```
select id from t with(index(索引名)) where num = @num
```

.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：

```
select id from t where num/2 = 100
```

应改为:

```
select id from t where num = 100*2
```


9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：

```
select id from t where substring(name,1,3) = ’abc’       -–name以abc开头的id
select id from t where datediff(day,createdate,’2005-11-30′) = 0    -–‘2005-11-30’    --生成的id
```

应改为:

```
select id from t where name like 'abc%'
select id from t where createdate >= '2005-11-30' and createdate < '2005-12-1'
```


10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。

11.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。

12.不要写一些没有意义的查询，如需要生成一个空表结构：

```
select col1,col2 into #t from t where 1=0
```

这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：
create table #t(…)

13.Update 语句，如果只更改1、2个字段，不要Update全部字段，否则频繁调用会引起明显的性能消耗，同时带来大量日志。

14.对于多张大数据量（这里几百条就算大了）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差。

15.select count(*) from table；这样不带任何条件的count会引起全表扫描，并且没有任何业务意义，是一定要杜绝的。


16.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有 必要。

17.应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。

18.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连 接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。

19.尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。

20.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。

21.尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。

\22. 避免频繁创建和删除临时表，以减少系统表资源的消耗。临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件， 最好使用导出表。

23.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。

24.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。

25.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。

26.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。

27.与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。

28.在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。

29.尽量避免大事务操作，提高系统并发能力。

30.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。

## 简述mysql中索引类型及对数据库的性能的影响

普通索引：允许被索引的数据列包含重复的值。
唯一索引：可以保证数据记录的唯一性。
主键：是一种特殊的唯一索引，在一张表中只能定义一个主键索引，主键用于唯一标识一条记录，使用关键字 PRIMARY KEY 来创建。
联合索引：索引可以覆盖多个数据列，如像INDEX(columnA, columnB)索引。
全文索引：通过建立倒排索引,可以极大的提升检索效率,解决判断字段是否包含的问题，是目前搜索引擎使用的一种关键技术。可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引
索引可以极大的提高数据的查询速度。
通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。但是会降低插入、删除、更新表的速度，因为在执行这些写操作时，还要操作索引文件
索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大，如果非聚集索引很多，一旦聚集索引改变，那么所有非聚集索引都会跟着变。





## 索引下推

**索引下推**（Index Condition Pushdown，ICP）, like KK%其实就是用到了索引下推优化

 EXPLAIN SELECT * FROM employees_copy WHERE name like 'LiLei%' AND age = 22 AND position ='manager';              

**什么是索引下推了？**

对于辅助的联合索引(name,age,position)，正常情况按照最左前缀原则，**SELECT \* FROM employees WHERE name like 'LiLei%' AND age = 22 AND position ='manager'**  这种情况只会走name字段索引，因为根据name字段过滤完，得到的索引行里的age和position是无序的，无法很好的利用索引。

在MySQL5.6之前的版本，这个查询只能在联合索引里匹配到名字是 **'LiLei' 开头**的索引，然后拿这些索引对应的主键逐个回表，到主键索引上找出相应的记录，再比对**age**和**position**这两个字段的值是否符合。

MySQL 5.6引入了索引下推优化，**可以在索引遍历过程中，对索引中包含的所有字段先做判断，过滤掉不符合条件的记录之后再回表，可以有效的减少回表次数**。使用了索引下推优化后，上面那个查询在联合索引里匹配到名字是 **'LiLei' 开头**的索引之后，同时还会在索引里过滤**age**和**position**这两个字段，拿着过滤完剩下的索引对应的主键id再回表查整行数据。

索引下推会减少回表次数，对于innodb引擎的表索引下推只能用于二级索引，innodb的主键索引（聚簇索引）树叶子节点上保存的是全行数据，所以这个时候索引下推并不会起到减少查询全行数据的效果。

索引下推（index condition pushdown ）简称ICP，在Mysql5.6的版本上推出，用于优化查询。

在不使用ICP的情况下，在使用非主键索引（又叫普通索引或者二级索引）进行查询时，存储引擎通过索引检索到数据，然后返回给MySQL服务器，服务器然后判断数据是否符合条件 。

在使用ICP的情况下，如果存在某些被索引的列的判断条件时，MySQL服务器将这一部分判断条件传递给存储引擎，然后由存储引擎通过判断索引是否符合MySQL服务器传递的条件，只有当索引符合条件时才会将数据检索出来返回给MySQL服务器 。

**索引条件下推优化可以减少存储引擎查询基础表的次数**，也可以减少MySQL服务器从存储引擎接收数据的次数

ICP的意思就是筛选字段**在索引中的where条件从服务器层下推到存储引擎**层，这样可以在存储引擎层过滤数据。由此可见，ICP可以减少存储引擎访问基表的次数和MySQL服务器访问存储引擎的次数。 ICP的使用场景如下： 

·　组合索引（a,b）where条件中的a字段是范围扫描，那么后面的索引字段b则无法使用到索引。在没有ICP时需要把满足a字段条件的数据全部提取到服务器层，并且会有大量的回表操作；而有了ICP之后，则会将b字段条件下推到存储引擎层，以减少回表次数和返回给服务器层的数据量。 

·　组合索引（a,b）的第一个字段的选择性非常低，第二个字段查询时又利用不到索引（%b%），在这种情况下，通过ICP也能很好地减少回表次数和返回给服务器层的数据量。 



**问题1** 当复合索引列为（name，age，address）时 以下SQL能使用索引吗？

```
select * from student where name like 'peng%' and age = 23;
```

可以，遇到like会中断后续元素的匹配，但只能使用name这个字段，mysql会一直向右匹配直到遇到范围查询（>、<、between、like）就停止匹配。范围列可以用到索引，但是范围列后面的列无法用到索引。即索引最多用于一个范围列，因此如果查询条件中有两个范围列则无法全用到索引。

**问题3** 索引下推在哪些情况下无法使用？

```
下推条件遇到子查询

下推条件遇到函数

非InnoDB表和MyISAM表
```



## 自适应哈希索引

哈希（hash）是一种非常快的查找方法，一般情况下查找的时间复杂度为O(1)，即一般仅需要一次查找就能准确定位。B+Tree的查找次数则取决于B+Tree的高度，在大多数的生产环境中，B+Tree的高度一般为3到5层，故需要3~5次的查询。 InnoDB存储引擎会监控对表上二级索引的查找。如果发现某二级索引被频繁访问，二级索引就成为热数据；如果观察到建立哈希索引可以带来速度的提升，则建立哈希索引，所以称之为自适应（adaptive）的，即自适应哈希索引（Adaptive Hash Index，AHI）。 经常访问的二级索引数据会自动被生成到hash索引里面（最近连续被访问3次的数据），自适应哈希索引通过缓冲池的B+Tree构造而来，因此建立的速度很快，而且不需要将整个表都建立哈希索引，InnoDB存储引擎会自动根据访问的频率和模式来为某些页建立哈希索引。 自适应哈希索引会占用InnoDB Buffer Pool，而且只适合搜索等值的查询，如select * from table where index_col='xxx'；对于其他查找类型，如范围查找，是不能使用的。MySQL自动管理，人为无法干预。 查看当前自适应哈希索引的使用状况可以使用show engine innodb status\G命令，通过hash searches、non-hash searches计算自适应哈希索引带来的收益以及付出，确定是否开启自适应哈希索引。 对于某些工作负载，如使用like和%的范围查询以及高并发的joins，不适合使用自适应哈希索引，维护哈希索引结构的额外开销会带来严重性能损耗。这种情况更适合于禁用自适应哈希索引，建议关掉，尽管默认情况下仍然启用。可以通过“set global innodb_adaptive_hash_index=off/on” 

## b树和b+树

B树的优势是当你要查找的值恰好处在一个非叶子节点时，查找到该节点就会成功并结束查询，而B+树由于非叶节点只是索引部分，这些节点中只含有其子树中的最大(或最小)关键字，当非终端节点上的关键字等于给点值时，查找并不终止，而是继续向下直到叶子节点。因此在B+树中，无论查找成功与否，都是走了一条从根到叶子节点的路径。b树任何一个关键字出现且只出现在一个结点中；

**有很多基于频率的搜索是选用B树，越频繁query的结点越往根上走，前提是需要对query做统计，而且要对key做一些变化**。
另外B树也好B+树也好，根或者上面几层因为被反复query，所以这几块基本都在内存中，不会出现读磁盘IO，一般已启动的时候，就会主动换入内存。 mysql底层存储是用B+树实现的，因为内存中B+树是没有优势的，但是一到磁盘，B+树的威力就出来了。

查询单条数据的时候**，B树的查询效率不固定，最好的情况是O(1**)。我们可以认为**在做单一数据查询的时候，使用B树平均性能更好**。但是，由于B树中各节点之间没有指针相邻，因此B树不适合做一些数据遍历操作。

 mysql底层存储是用B+树实现的，因为内存中B+树是没有优势的，但是一到磁盘，B+树的威力就出来了。

B+树的数据只出现在叶子节点上，因此在查询单条数据的时候，查询速度非常稳定。因此，在做单一数据的查询上，其平均性能并不如B树。但是，**B+树的叶子节点上有指针进行相连，因此在做数据遍历的时候，只需要对叶子节点进行遍历即可**，这个特性使得B+树非常适合做范围查询。

B+树 是 B-树的一个升级版本，在存储结构上的变化，由于磁盘页的大小限制，只能读取少量的B-树结点到内存中（因为B-树结点就带有数据，占用更多空间，所以说是 少量）；而B+树就不一样了。因为非叶子结点不带数据，能够一次性读取更多结点进去处理，所以对于同样的数据量， B+树更加 "矮胖"， 性能更好。但是两者在查找、插入和删除等操作的时间复杂度的量级是一致的，均为o(logn)



## B+树索引和哈希索引

<font color ="red">索引有三个优点：减少了服务器需要扫描的数据量，帮助服务器避免排序， 将随机IO变为顺序IO （因为b+树索引是有序的，将相邻数据都存储在一起）</font>

b+树一个结点的大小设为一页或页的倍数最合适。因为如果一个结点的大小小于一页，那么读这个结点的时候其实读取的还是一页，这样就造成了资源的浪费。

![image-20220413215837381](C:\Users\heziyi6\AppData\Roaming\Typora\typora-user-images\image-20220413215837381.png)

INNODB用b+树索引，先通过b+树找到数据所在的页，然后将页读到内存，在内存中找到要查找的数据。b+树拥有B树的特点，而且叶子结点之间有指针，非叶子结点的元素在叶子结点上都冗余了，也就是叶子结点中存储了所有的元素并且排好顺序

在mysql中一个innodb页就是一个B+树节点

b+树比b树的优点：

1.顺序查找 范围查找 排序查找能力强

2.IO的效率更高，因为非叶结点不存储数据

3.基于索引树的全量数据扫描能力更高



hash索引底层就是hash表，进行查找时，==调用一次hash函数就可以获取到相应的键值,之后进行回表查询获得实际数据==.B+树底层实现是多路平衡查找树.对于每一次的查询都是从根节点出发,查找到叶子节点方可 以获得所查键值，然后根据查询判断是否需要回表查询数据.

-hash索引不支持使用索引进行排序，原理同上.

-hash索引不支持模糊查询以及多列索引的最左前缀匹配.原理也是因为hash函数的不可预测.**AAAA**和 **AAAAB**的索引没有相关性.

-hash索引任何时候都避免不了回表查询数据，而B+树在符合某些条件（聚簇索引,覆盖索引等）的时候可 以只通过索引完成查询.

-hash索引虽然在等值查询上较快，但是不稳定.性能不可预测，当某个键值存在大量重复的时候，发生hash 碰撞，此时效率可能极差.而B+树的查询效率比较稳定,对于所有的查询都是从根节点到叶子节点，且树 的高度较低.

在大多数情况下，直接选择B+树索引可以获得稳定且较好的查询速度.而不需要使用hash索引.

![image-20210821110207997](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210821110207997.png)





由于主键是聚族索引的缘故，Innodb 的基于主键的查询效率非常高。



innodb的叶子节点直接存放数据、

innodb通过b+树结构对主键创建索引，然后叶子节点中存储记录，如果没有主键，那么会选择唯一键，如果没有唯一键那么会生成一个六字节的row_id作为主键。

如果创建索引的键是其他字段，那么在叶子节点中存储的是该记录的主键，然后再通过主键索引找到对应的记录，叫做回表。

![image-20210818210816620](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210818210816620.png)

![image-20210818210833069](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210818210833069.png)



注意：b+树上有两个头指针，一个指向根节点，一个指向关键字最小的叶子结点，而且所有叶子结点之间是一种链式环结构，因此可以用b+树进行两种查找运算：一种是对于主键的范围查找和分类查找，一种是从根节点开始进行随机查找。

B+树：
==B+树是一个平衡的多叉树，从根节点到每个叶子节点的高度差值不超过1，而且同层级的节点间有指针相互链接==。在B+树上的常规检索，从根节点到叶子节点的搜索效率基本相当，不会出现大幅波动，而且基于索引的顺序扫描时，也可以利用双向指针快速左右移动，效率非常高。因此，B+树索引被广泛应用于数据库、文件系统等场景。

![image-20210724164900206](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210724164900206.png)

B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在哈希碰撞问题。

哈希索引：

![image-20210724165149452](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210724165149452.png)

==如果是等值查询，那么哈希索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值；前提是键值都是唯一的。如果键值不是唯一的，就需要先找到该键所在位置，然后再根据链表往后扫描，直到找到相应的数据；==
如果是范围查询检索，这时候哈希索引就毫无用武之地了，因为原先是有序的键值，经过哈希算法后，有可能变成不连续的了，就没办法再利用索引完成范围查询检索；（另一种说法：因为在hash索引中经过hash函数建立索引之后,索引的顺序与原顺序无法保持一致,不能支持范围查询.而B+树的的所有节点皆遵循(左节点小于父节点,右节点大于父节点,多叉树也类似),天然支持范围.hash索引不支持使用索引进行排序,原理同上.）
哈希索引也没办法利用索引完成排序，以及like ‘xxx%’ 这样的部分模糊查询（这种部分模糊查询，其实本质上也是范围查询）；

哈希索引也不支持多列联合索引的最左匹配规则；

B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键值情况下，哈希索引的效率也是极低的，因为存在哈希碰撞问题。（❓为什么会有大量重复键值？）



哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快

hash索引任何时候都避免不了回表查询数据,而B+树在符合某些条件(聚簇索引,覆盖索引等)的时候可以只通过索引完成查询.（❓回表查询数据）
hash索引虽然在等值查询上较快,但是不稳定.性能不可预测,当某个键值存在大量重复的时候,发生hash碰撞,此时效率可能极差.而B+树的查询效率比较稳定,对于所有的查询都是从根节点到叶子节点,且树的高度较低

因此,在大多数情况下,直接选择B+树索引可以获得稳定且较好的查询速度.而不需要使用hash索引.



一般来说3-4层的B+树就足够存储上千万的数据

磁盘扇区、文件系统、InnoDB存储引擎都有各自的最小存储单元。在MySQL中我们的InnoDB页的大小默认是16k， `当然也可以通过参数设置`。

`假设一行数据的大小是1k`，那么`一个页可以存放16行这样的数据`。

## InnoDB B+树能存放多少数据

一次查询的效率取绝于磁盘io的次数，如果我们能够在一次查询中尽可能地降低磁盘io的次数，那么我们就能加快查询的速度。

B-树（B类树）的特定就是每层节点数目非常多，层数很少，目的就是为了就少磁盘IO次数，当查询数据的时候，最好的情况就是很快找到目标索引，然后读取数据，使用B+树就能很好的完成这个目的，但是B-树的每个节点都有data域（指针），这无疑增大了节点大小，说白了增加了磁盘IO次数（在B树的情况下，由于非叶子节点使用了大量空间存储数据，存放的索引指针肯定就少，最终整棵树如果想要存储和B+树一样多的数据就必须要增加树高，这样一来就增加了磁盘io，所以说B+树作为索引的性能比B树高），而B+树除了叶子节点其它节点并不存储数据，节点小，磁盘IO次数就少。这是优点之一。另一个优点是什么，B+树所有的Data域在叶子节点，一般来说都会进行一个优化，就是将所有的叶子节点用指针串起来。这样遍历叶子节点就能获得全部数据，这样就能进行区间访

网上各种博客说一个2层的B+树可以存18720条数据，一个3层的B+树，大约可以存2000W条数据。

这种说法真的是正确的吗？

在一定的前提下估算结果是正确的。

基本上博客中都是以主键为bigint类型、一行数据大小为1K，为前提得到这个结果的。

在这个前提下，咱们来计算一下：

首先，Mysql存储数据是按页进行存储的，一页默认大小为16K。bigint在mysql中占8B，指针在mysql中占6B。

我们都知道B+树的特点，非叶子节点存储的是 主键-节点页指针这样的“键值对”。也就是说一个主键和指针占用14B，所以所有的非叶子节点就可以存放16*1024/14=1170个这样的“键值对”。

那么，这个3层的B+树就是这样存储的：

![img](https://img-blog.csdnimg.cn/20201126225948725.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2F1ZXJqZHM=,size_16,color_FFFFFF,t_70)



由此，证明了一个3层的B+树，约可以存放2000W条数据。但是得注意咱们说的前提条件。

`先假设B+树高为2`，即存在一个根节点和若干个叶子节点，那么这棵B+树的存放总记录数为：根节点指针数*单个叶子节点记录行数。

上文我们已经说明单个叶子节点（页）中的记录数=16K/1K=16。（这里假设一行记录的数据大小为1k，`实际上`现在很多互联网业务`数据记录大小通常就是1K左右`）。

那么现在我们需要计算出`非叶子节点能存放多少指针`，其实这也很好算，我们假设`主键ID为bigint类型，长度为8字节`，而`指针大小`在InnoDB源码中设置为`6字节`，这样一共14字节，我们一个页中能存放多少这样的单元，其实就代表有多少指针，即`16384/14=1170`。那么可以算出一棵`高度为2的B+树`，能存放`1170*16=18720条`这样的数据记录。

根据同样的原理我们可以算出一个`高度为3的B+树`可以存放：`1170*1170*16=21902400条`这样的记录。所以在InnoDB中`B+树高度一般为1-3层`，它就能`满足千万级的数据存储`。在查找数据时 **`一次页的查找代表一次IO`**， 所以通过主键索引查询通常 **`只需要1-3次IO操作`** 即可查找到数据

上面都是理想化的计算方式，实际上一页不会完全的存用户数据，还会存放一些其他字段，比如page level，index number等等。一般一页存满15/16，就另起一页存放其他数据了。还有其他因素。

估算的算法： (叶子节点存放数量的平均值)*(非叶子节点存放的数量的平均值)^(层数-1)

## 什么是聚簇（集）索引？

一个innodb页内的记录按照主键的大小顺序排成一个单向链表，各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表（页与页之间双向链表，一个页内部单向链表）

innodb存储引擎在进行数据插入的时候数据必须要跟某一个索引列存储在一起，这个索引列可以是主键，如果没有主键则选择唯一键，如果没有唯一键选择6字节的rowid来进行存储。

innodb中有聚簇索引也有非聚簇索引，myisam只有非聚簇索引

非聚簇索引的叶子结点存储的数据不再是整行的记录而是聚簇索引的id值

表数据文件本身就是按b+树组织的一个索引结构文件

聚集索引-指叶结点包含了完整的数据记录

在B+树的索引中,叶子节点可能存储了当前的key值**,也可能存储了当前的key值以及整行的数据,这就是聚簇索引和非聚簇索引.** 在InnoDB中,只有主键索引是聚簇索引,如果没有主键,则挑选一个唯一键建立聚簇索引.如果没有唯一键,则隐式的生成一个键来建立聚簇索引.当查询使用聚簇索引时,在对应的叶子节点,可以获取到整行数据,因此不用再次进行回表查询.

对于InnoDB表而言，MySQL的非聚簇索引统称为“辅助索引”（secondary index），辅助索引的“表记录指针”称为“书签”（bookmark），实际上是主键值，如图3-32所示，可以看到，所有的辅助索引都包含主键列，所有的InnoDB表都是通过主键来聚簇的。 

![image-20210826094051031](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210826094051031.png)

​                                                                   3-32



●非聚簇索引一定会回表查询吗?
不一定,**这涉及到查询语句所要求的字段是否全部命中了索引,如果全部命中了索引,那么就不必再进行回表查询**.
举个简单的例子,假设我们在员工表的年龄上建立了索引,那么当进行select age from employee where age < 20的查询时,在索引的叶子节点上,已经包含了age信息,不会再次进行回表查询.

● InnoDB存储引擎的表支持聚簇索引。由于创建聚簇索引时需要对“索引”中的数据以及表中的数据进行排序，**为了避免更新数据（例如插入数据）耗费过多的时间，建议将InnoDB表的主键设置为自增型字段。** 

为什么非主键索引叶子节点存储的是主键值： 一致性和节省存储空间



## 二级索引

聚簇索引只有在搜索条件是主键时才发挥作用

为了让新插入的记录能够找到自己在哪个页中，就需要保证b+树同一层内节点的目录项记录除页号这个字段以外是唯一的，所以二级索引的内节点的目录项记录的内容实际上是由3部分构成：索引列的值；主键值；页号。

对于二级索引来说，先是按照二级索引列的值进行排序，在二级索引列相同的情况下再按照主键值进行排序，所以为c2列建立索引相当于为(c2,c1)建立了一个联合索引（c1是主键）

二级索引：叶子节点中存储主键值，每次查找数据时，根据索引找到叶子节点中的主键值，根据主键值再到聚簇索引中得到完整的一行记录。

问题：

1.相比于叶子节点中存储行指针，二级索引存储主键值会占用更多的空间，那为什么要这样设计呢？

InnoDB在移动行时，无需维护二级索引，因为叶子节点中存储的是主键值，而不是指针。



## 二级索引存储主键值而不是存储行指针的优点与缺点

优点

> 减少了出现行移动或者数据页分裂时二级索引的维护工作（当数据需要更新的时候，二级索引不需要修改，只需要修改聚簇索引，一个表只能有一个聚簇索引，其他的都是二级索引，这样只需要修改聚簇索引就可以了，不需要重新构建二级索引）

缺点

> 1. 二级索引体积可能会变大，因为二级索引中存储了主键的信息
> 2. 二级索引的访问需要两次索引查找。第一次通过查找 *二级索引* 找二级索引中叶子节点存储的 *主键的值*；第二次通过这个主键的值去 ***聚簇索引*** 中查找对应的行



二级索引的叶子节点存储了索引值+rowid（主键值）。熟悉MySQL的读者在MySQL中创建表时最好自己指定一个显式的自增主键，这样做的好处是：显式指定的主键可以是普通的int类型，这样存储的空间就是4字节，在二级索引的叶子节点中存储主键值所占用的空间就会变小。这时可能有人会问：**二级索引的叶子节点为何不存储主键的指针呢**？原因是：如果主键位置发生了变化，则需要修改二级索引的叶子节点对应存储的指针；但是如果二级索引的叶子节点本身存储的是主键的值，则不会出现这种情况。 

**二级索引为什么存储主键值而不直接存储整条记录**：这样的话虽然不用回表，但是太占地方了，相当于每建立一棵b+树都要把所有用户记录复制一遍，太浪费存储空间了。

2.那么InnoDB有了聚簇索引，为什么还要有二级索引呢？

　　聚簇索引的叶子节点存储了一行完整的数据，而二级索引只存储了主键值，相比于聚簇索引，占用的空间要少。当我们需要为表建立多个索引时，如果都是聚簇索引，那将占用大量内存空间，所以InnoDB中主键所建立的是聚簇索引，而唯一索引、普通索引、前缀索引等都是二级索引。

## 为什么一般情况下，我们建表的时候都会使用一个自增的id来作为我们的主键？

　　InnoDB中表中的数据是直接存储在主键聚簇索引的叶子节点中的，每插入一条记录，其实都是增加一个叶子节点，如果主键是顺序的，只需要把新增的一条记录存储在上一条记录的后面，当页达到最大填充因子的时候，下一跳记录就会写入新的页中，这种情况下，**主键页就会近似于被顺序的记录填满。**

　　若表的主键不是顺序的id，而是无规律数据，比如字符串，InnoDB无法简单的把一行记录插入到索引的最后，而是需要找一个合适的位置（已有数据的中间位置），甚至产生大量的页分裂并且移动大量数据，在寻找合适位置进行插入时，目标页可能不在内存中，这就导致了大量的随机IO操作，影响插入效率。除此之外，大量的页分裂会导致大量的内存碎片。





## 计算机存储原理

在理解索引这个概念之前，我们需要先了解一下计算机存储方面的基本知识。

我们知道数据持久化之后存在了数据库里,那么我现在的问题是数据库将数据存在了哪里？答案显然是存在了计算机的存储设备上。就个人电脑而言，数据被存在了我们的电脑存储设备上。

计算机的存储设备有很多种，其中速度越快的越贵，因此容量也往往越小例如我们的RAM随机存储器，也就是大家平时说的内存条，速度慢的就相对便宜例如我们的硬盘。而我们的数据往往都是被存在最慢的存储设备硬盘上的，因为存在当中的数据在断电之后依然存在。

计算机的存储介质有多种，例如硬盘，例如告诉缓存，不同的存储介质的数据读取速度是不一样的。例如，像RAM这样的易失性存储设备的读写操作就非常快，访问其中的数据几乎没有延迟性。由于这个原因，计算机操作系统的设计是这样的：数据永远不会直接从硬盘等机械设备中取出，而是首先从硬盘转移到更快的存储设备，例如RAM，从RAM当中应用程序直接按需获取数据。 

这个时候我们就能直接回答上述问题了，建立了索引的数据，就是通过事先排好序，从而在查找时可以应用二分查找来提高查询效率。这也解释了为什么索引应当尽可能的建立在主键这样的字段上，因为主键必须是唯一的，根据这样的字段生成的二叉查找树的效率无疑是最高的。



为什么索引不能建立的太多？
如果一个表中所有字段的索引很大，也会导致性能下降。想象一下，如果一个索引和一个表一样长，那么它将再次成为一个需要检查的开销。这就好比字典的目录非常详细，但是其长度已经和所有的文字一样长，这个时候目录本身的效率就大大下降了。

索引有弊端吗？
肯定是有的，索引可以提高查询读取性能，而它将降低写入性能。当有索引时，如果更改一条记录，或者在数据库中插入一条新的记录，它将执行两个写入操作（一个操作是写入记录本身，另一个操作是将更新索引）。因此，在定义索引时，必须牢记以下几点：

索引表中的每个字段将降低写入性能。
建议使用表中的唯一值为字段编制索引。
在关系数据库中充当外键的字段必须建立索引，因为它们有助于跨多个表进行复杂查询。
索引还使用磁盘空间，因此在选择要索引的字段时要小心。
什么是聚集索引

![image-20210712005510862](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210712005510862.png)聚集索引clustered index也叫聚簇索引，它的定义是：聚集索引的表中数据行的物理顺序与列值（一般是主键的那一列)的逻辑顺序相同，一个表中只能拥有一个聚集索引。 

## 索引设计的原则？

通常在创建索引时要考虑以上内容（回表、基数、选择性），在MySQL中可以通过系统表innodb_index_stats来查看索引选择性如何，并且可以看到组合索引中每一个字段的选择性如何，还可以计算索引的大小

。例如，某一字段的基数是几十万条，但是表中数据有几十亿条，在这个字段上创建索引就不是很合适，因为选择性比较低，通过索引查询在索引中可能就要扫描上亿条数据。

正确创建合适索引是数据库优化性能的基础

索引空间上的代价：每建立一个索引，都要为它建立一颗b+树，每一棵b+树的每一个节点都是一个数据页，一个数据页默认会占用16kb的存储空间，一棵很大的b+树由许多数据页组成，将占用很大一片空间

索引时间上的代价：每当对表中的数据进行增删改查操作时都需要修改各个b+树索引，可能会对节点和记录的排序造成破坏，所以存储引擎需要额外时间进行页面分裂、页面回收等操作，以维护节点和记录的排序

### 

建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。

•区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数）•尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好）•使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）

![image-20210826145617719](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210826145617719.png)

![image-20210826150204601](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210826150204601.png)

离散度：即这个列取值的重复度，如果离散度达到一定大小可能即使建立了索引查询时也不会走索引

![image-20210826150951821](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210826150951821.png)



在添加联合索引时的顺序：先放最常使用的列，其次是离散度高的列，再次是最少空间的列

索引确实可以提高检索效率，但要记住**，索引是冗余数据，冗余数据不仅需要额外的存储空间，而且还需要额外的维护**（虽然不需要人为的维护）。 如果索引过多，在更新数据（添加、修改或者删除）时，除了需要修改表中的数据外，还需要对该表的所有索引进行维护，以维持表字段值和索引关键字值之间的一致性，反而降低了数据的更新速度。 实践表明，当修改表记录的操作特别频繁时，过多的索引会导致硬盘I/O次数明显增加，反而会显著地降低服务器性能，甚至可能会导致服务器宕机。不恰当的索引不但于事无补，反而会降低系统性能。因此，索引是把双刃剑，并不是越多越好

原则1：表的某个字段值的离散度越高，该字段越适合选作索引的关键字。主键字段以及唯一性约束字段适合选作索引的关键字，原因就是这些字段的值非常离散。尤其是在主键字段创建索引时，Cardinality的值就等于该表的行数。MySQL在处理主键约束以及唯一性约束时，考虑得比较周全。数据库用户创建主键约束的同时，MySQL会自动创建主索引（primary idex），且索引名称为 PRIMARY；数据库用户创建唯一性约束的同时，MySQL 会自动地创建唯一性索引（unique index），默认情况下，索引名为唯一性约束的字段名。 

原则2：占用储存空间少的字段更适合选作索引的关键字。 如果索引中关键字的值占用的存储空间较多，那么检索效率势必会造成影响。例如，与字符串字段相比，整数字段占用的存储空间较少，因此，较为适合选作索引的关键字。 

原则3：储存空间固定的字段更适合选作索引的关键字。 与text类型的字段相比，char类型的字段较为适合选作索引的关键字。 

原则 4：where 子句中经常使用的字段应该创建索引，分组字段或者排序字段应该创建索引，两个表的连接字段应该创建索引。 引入索引的目的是提高数据的检索效率，因此索引关键字的选择与select语句息息相关。这句话有两个方面的含义：select语句的设计可以决定索引的设计；索引的设计也同样影响着select语句的设计。例如原则1 与原则2，可以影响select语句的设计；而select语句中的where子句、group by子句以及order by子句，又可以影响索引的设计。两个表的连接字段应该创建索引，外键约束一经创建，MySQL会自动地创建与外键相对应的索引，这是由于外键字段通常是两个表的连接字段。 

原则5：更新频繁的字段不适合创建索引，不会出现在where子句中的字段不应该创建索引。 

原则6：最左前缀原则。 复合索引还有另外一个优点，它通过被称为“最左前缀”（leftmost prefixing）的概念体现出来。假设向一个表的多个字段（例如 firstname、lastname、address）创建复合索引（索引名为fname_lname_address）。当 where 查询条件是以下各种字段的组合时，MySQL 将使用 fname_lname_address索引。其他情况将无法使用fname_lname_address索引。 firstname，lastname，address firstname，lastname firstname 可以这样理解：一个复合索引（firstname、lastname、address）等效于(firstname，lastname，age)、(firstname，lastname)以及(firstname)三个索引。基于最左前缀原则，应该尽量避免创建重复的索引，例如创建了fname_lname_address索引后，就无需在first_name字段上单独创建一个索引。 

原则7：尽量使用前缀索引。 

例如，仅仅在姓名（例如“张三”）中的姓氏部分（“张”）创建索引，从而可以节省索引的存储空间，提高检索效率。 

==约束==主要用于保证业务逻辑操作数据库时数据的完整性；而索引则是将关键字数据以某种数据结构的方式存储到外存，用于提升数据的检索性能。约束是逻辑层面的概念；而索引既有逻辑上的概念，更是一种物理存储方式，且事实存在，需要耗费一定的存储空间。 对于一个MySQL数据库表而言，主键约束、唯一性约束以及外键约束是基于索引实现的。因此，创建主键约束的同时，会自动创建一个主索引，且主索引名与主键约束名相同（PRIMARY）；创建唯一性约束的同时，会自动创建一个唯一性索引，且唯一性索引名与唯一性约束名相同；创建外键约束的同时，会自动创建一个普通索引，且索引名与外键约束名相同。 在MySQL数据库中，删除了唯一性索引，对应的唯一性约束也将自动删除。若不考虑存储空间方面的因素，唯一性索引就是唯一性约束。 

MySQL还支持全文索引（fulltext），当查询数据量大的字符串信息时，使用全文索引可以大幅提升字符串的检索效率。需要注意的是，**全文索引只能创建在char、varchar或者text字符串类型的字段上，且全文索引不支持前缀索引**。 



1. 适合索引的列是出现在where子句中的列，或者连接子句中指定的列
2. 基数较小的表，索引效果较差，没有必要在此列建立索引
3. 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间，如果搜索词超过索引前缀长度，则使用索引排除不匹配的行，然后检查其余行是否可能匹配。
4. 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。
5. 定义有外键的数据列一定要建立索引。
6. 更新频繁字段不适合创建索引
7. 若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低)
8. 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。
9. 对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。
10. 对于定义为text、image和bit的数据类型的列不要建立索引

在已有表上创建索引：

语法格式一： create [ unique | fulltext ] index索引名on 表名 ( 字段名[(长度)] [ asc | desc ] ) 

语法格式二： alter table表名add [ unique | fulltext ] index索引名 ( 字段名[(长度)] [ asc | desc ] ) 



对于频繁的查询优先考虑使用覆盖索引

> 覆盖索引：就是包含了所有查询字段 (where,select,ordery by,group by 包含的字段) 的索引

**覆盖索引的好处：**

•**避免 Innodb 表进行索引的二次查询:** Innodb 是以聚集索引的顺序来存储的，对于 Innodb 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。

•**可以把随机 IO 变成顺序 IO 加快查询效率:** 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。

下面是图灵笔记中的几个索引设计原则：[有道云笔记 (youdao.com)](https://note.youdao.com/ynoteshare/index.html?id=d2e8a0ae8c9dc2a45c799b771a5899f6&type=note&_time=1648129406997)

**索引设计原则**

**1、代码先行，索引后上**

不知大家一般是怎么给数据表建立索引的，是建完表马上就建立索引吗？

这其实是不对的，一般应该等到主体业务功能开发完毕，把涉及到该表相关sql都要拿出来分析之后再建立索引。

**2、联合索引尽量覆盖条件**

比如可以设计一个或者两三个联合索引(尽量少建单值索引)，让每一个联合索引都尽量去包含sql语句里的where、order by、group by的字段，还要确保这些联合索引的字段顺序尽量满足sql查询的最左前缀原则。

**3、不要在小基数字段上建立索引**

索引基数是指这个字段在表里总共有多少个不同的值，比如一张表总共100万行记录，其中有个性别字段，其值不是男就是女，那么该字段的基数就是2。

如果对这种小基数字段建立索引的话，还不如全表扫描了，因为你的索引树里就包含男和女两种值，根本没法进行快速的二分查找，那用索引就没有太大的意义了。

一般建立索引，尽量使用那些基数比较大的字段，就是值比较多的字段，那么才能发挥出B+树快速二分查找的优势来。

**4、长字符串我们可以采用前缀索引**

尽量对字段类型较小的列设计索引，比如说什么tinyint之类的，因为字段类型较小的话，占用磁盘空间也会比较小，此时你在搜索的时候性能也会比较好一点。

当然，这个所谓的字段类型小一点的列，也不是绝对的，很多时候你就是要针对varchar(255)这种字段建立索引，哪怕多占用一些磁盘空间也是有必要的。

对于这种varchar(255)的大字段可能会比较占用磁盘空间，可以稍微优化下，比如针对这个字段的前20个字符建立索引，就是说，对这个字段里的每个值的前20个字符放在索引树里，类似于 KEY index(name(20),age,position)。

此时你在where条件里搜索的时候，如果是根据name字段来搜索，那么此时就会先到索引树里根据name字段的前20个字符去搜索，定位到之后前20个字符的前缀匹配的部分数据之后，再回到聚簇索引提取出来完整的name字段值进行比对。

但是假如你要是order by name，那么此时你的name因为在索引树里仅仅包含了前20个字符，所以这个排序是没法用上索引的， group by也是同理。所以这里大家要对前缀索引有一个了解。

**5、where与order by冲突时优先where**

在where和order by出现索引设计冲突时，到底是针对where去设计索引，还是针对order by设计索引？到底是让where去用上索引，还是让order by用上索引?

一般这种时候往往都是让where条件去使用索引来快速筛选出来一部分指定的数据，接着再进行排序。

因为大多数情况基于索引进行where筛选往往可以最快速度筛选出你要的少部分数据，然后做排序的成本可能会小很多。

**6、基于慢sql查询做优化**

可以根据监控后台的一些慢sql，针对这些慢sql查询做特定的索引优化。

关于慢sql查询不清楚的可以参考这篇文章：https://blog.csdn.net/qq_40884473/article/details/89455740



##  ==索引失效的典型例子==

如果在SQL的WHERE条件中，字段类型为字符串，而其值为数值，那么MySQL不会使用索引，这个规则和Oracle是一致的，所以，字符类型的字段值应该加上引号。例如，表t_base_user的telephone列是一个字符类型的索引列，下面的语句在执行的时候就不会选择索引：

select * from user where phone=123

应该改为select *from user where phone='123'

全表扫描往往发生在下面几种情况:

SQL的on子句或者where子句涉及到的列上没有索引；
表数据量很小，走索引查询比全表扫描更麻烦；这对于少于10行且行长度较短的表来说很常见



索引不生效的情况？

-使用不等于查询

-NULL值

.where中索引列有运算;
where中索引列使用了函数;

■在字符串like时左边是通配符.比如%xxx

■当mysql分析全表扫描比使用索引快的时候不使用索引.

-当使用联合索引，前面一个条件为范围查询，后面的即使符合最左前缀原则，也无法使用索引.



- 存在索引列的数据类型隐形转换，则用不上索引，比如列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引

![img](https://img2018.cnblogs.com/blog/1623038/201906/1623038-20190619181326223-1654473887.png)

- where 子句里对索引列上有数学运算，用不上索引

![img](https://img2018.cnblogs.com/blog/1623038/201906/1623038-20190619181436583-1773123023.png)

2.避免索引失效
不在索引列上做任何操作（计算，函数、自动or手动类型转换），这样会导致索引失效而转向全表扫描。条件中用or，即使其中有条件带索引，也不会使用索引查询，这就是查询尽量不要用or的原因，用in吧。 

存储引擎不能使用索引中范围条件右边的列。这个是因为age中查询时范围查询了，pos列的索引就没有生效了

尽量使用覆盖索引（只访问索引的查询（索引列和查询列一致）），减少select *。

对于MySQL而言

mysql在使用不等于（!=或者<>）的时候无法使用索引会导致全表扫描
is null,is not null也无法使用索引
like 通配符开头'%abc..'，mysql索引会失效会变成全表扫描的操作 

3.避免排序，不能避免，尽量选择索引排序
4.避免查询不必要的字段
5.避免临时表的创建，删除 



**查询语句的查询条件中只有OR关键字，且OR前后的两个条件中的列都是索引时，查询中才使用索引。**否则，查询将不使用索引。







## 避免select *

效率低的原因

\1. 不需要的列会增加数据传输时间和网络开销

\2. 对于无用的大字段，如 varchar、blob、text，会增加 io 操作

\3. 失去MySQL优化器“覆盖索引”策略优化的可能性

阿里java开发手册(泰山版)》中 MySQL 部分描述：

4 - 1. 【强制】在表查询中，一律不要使用 * 作为查询的字段列表，需要哪些字段必须明确写明。说明：增加查询分析器解析成本。增减字段容易与 resultMap 配置不一致。无用字段增加网络 消耗，尤其是 text 类型的字段。

开发手册中比较概括的提到了几点原因，让我们深入一些看看：

\1. 不需要的列会增加数据传输时间和网络开销

用“SELECT * ”数据库需要解析更多的对象、字段、权限、属性等相关内容，在 SQL 语句复杂，硬解析较多的情况下，会对数据库造成沉重的负担。增大网络开销；* 有时会误带上如log、IconMD5之类的无用且大文本字段，数据传输size会几何增涨。如果DB和应用程序不在同一台机器，这种开销非常明显。即使 mysql 服务器和客户端是在同一台机器上，使用的协议还是 tcp，通信也是需要额外的时间。

2. 对于无用的大字段，如 varchar、blob、text，会增加 io 操作 ，准确来说，长度超过 728 字节的时候，会先把超出的数据序列化到另外一个地方，因此读取这条记录会增加一次 io 操作。(MySQL InnoDB)

3. 失去MySQL优化器“覆盖索引”策略优化的可能性

**SELECT * 杜绝了覆盖索引的可能性，而基于MySQL优化器的“覆盖索引”策略又是速度极快，效率极高，业界极为推荐的查询优化方式。**

例如，有一个表为t(a,b,c,d,e,f)，其中，a为主键，b列有索引。

那么，在磁盘上有两棵 B+ 树，即聚集索引和辅助索引(包括单列索引、联合索引)，分别保存(a,b,c,d,e,f)和(a,b)，如果查询条件中where条件可以通过b列的索引过滤掉一部分记录，查询就会先走辅助索引，如果用户只需要a列和b列的数据，直接通过辅助索引就可以知道用户查询的数据。 如果用户使用select *，获取了不需要的数据，则首先通过辅助索引过滤数据，然后再通过聚集索引获取所有的列，这就多了一次b+树查询，速度必然会慢很多。 

由于辅助索引的数据比聚集索引少很多，很多情况下，通过辅助索引进行覆盖索引(通过索引就能获取用户需要的所有列)，都不需要读磁盘，直接从内存取，而聚集索引很可能数据在磁盘(外存)中(取决于buffer pool的大小和命中率)，这种情况下，一个是内存读，一个是磁盘读，速度差异就很显著了，几乎是数量级的差异。 



## 联合索引 (a,b,c)

联合索引 (a,b,c) 实际建立了 (a)、(a,b)、(a,b,c) 三个索引

我们可以将组合索引想成书的一级目录、二级目录、三级目录，如index(a,b,c)，相当于a是一级目录，b是一级目录下的二级目录，c是二级目录下的三级目录。要使用某一目录，必须先使用其上级目录，一级目录除外。

![图片](https://mmbiz.qpic.cn/mmbiz_png/a7wPU9Eqe9sMBBA3zT893lnCo0ibZ0OhKOxbBMUKX1XkAASic92YrDMc4pEOdgzgic6ATpvKmprib4ZvPibN7sZQE7w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 联合索引的优势

1） 减少开销

建一个联合索引 (a,b,c) ，实际相当于建了 (a)、(a,b)、(a,b,c) 三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！

2）覆盖索引

对联合索引 (a,b,c)，如果有如下 sql 的，

```
SELECT a,b,c from table where a='xx' and b = 'xx';
```

那么 MySQL 可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机 io 操作。减少 io 操作，特别是随机 io 其实是 DBA 主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。

3）效率高

索引列多，通过联合索引筛选出的数据越少。比如有 1000W 条数据的表，有如下SQL:

```
select col1,col2,col3 from table where col1=1 and col2=2 and col3=3;
```

假设：假设每个条件可以筛选出 10% 的数据。

- A. 如果只有单列索引，那么通过该索引能筛选出 1000W10%=100w 条数据，然后再回表从 100w 条数据中找到符合 col2=2 and col3= 3 的数据，然后再排序，再分页，以此类推（递归）；
- B. 如果是（col1,col2,col3）联合索引，通过三列索引筛选出 1000w10% 10% *10%=1w，效率提升可想而知！

## 索引覆盖是什么

索引覆盖就是一个SQL在执行时,可以利用索引来快速查找,并且此SQL所要查询的字段在当前索对应的字段中都包含了,那么就表示此SQL走完索引后不用回表了,所需要字段都在当前索引的叶子节点上存在,可以直接作为结果返回了

![image-20210826151430920](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210826151430920.png)

上面的语句中第一句、第三句、第五句是覆盖索引

##  覆盖索引(Using index)、文件排序(Using filesort)

Using index 覆盖索引

概念：一个索引（B+树）中包含所有需要查询的字段的值，称为覆盖索引。覆盖索引的一个特点是无需回表。
覆盖索引不是一种索引类型，不是一个名词，而是一个动词。

下面举一个渐进的例子来描述覆盖索引在底层上是怎么做的：
比如，我给 col1 字段设置了一个普通索引，给id设置了主键索引，使用的是innodb的表。

1.Select * from t;
全表扫描，在底层表现为在聚集索引这棵树中把所有叶子节点一个个的读取到内存中，获取每个叶节点内的行数据。

2.Select col1 from t;
Mysql检查到 col1 字段是索引，所以会直接去索引获取col1字段，因为col1索引B+树中就包含col1字段的值，无需拿到叶子节点中存储的主键id值再跳到聚集索引获取col1的数据。此时直接找到二级索引的B+树将所有叶子节点的col1的值获取到。这个过程只查了col1二级索引的B+树，没有去查主键索引的B+树，因此这个例子就用到了覆盖索引。

Using FileSort 文件排序

在Sql优化中，我们希望尽可能不要出现文件排序，因为出现了文件排序意味着没有使用到索引构建好的排序，而是需要在内存中对字段进行重新排序，排序的过程是计算的过程比较消耗cpu。

多字段排序要尽量遵循最左前缀原则，而且不要对一个字段升序对另一个字段降序，否则也会使用到Using filesort

**Using filesort文件排序原理详解**

**filesort文件排序方式**

- 单路排序：是一次性取出满足条件行的所有字段，然后在sort buffer中进行排序；用trace工具可以看到sort_mode信息里显示< sort_key, additional_fields >或者< sort_key, packed_additional_fields >
- 双路排序（又叫**回表**排序模式）：是首先根据相应的条件取出相应的**排序字段**和**可以直接定位行数据的行 ID**，然后在 sort buffer 中进行排序，排序完后需要再次取回其它需要的字段；用trace工具可以看到sort_mode信息里显示< sort_key, rowid >

MySQL 通过比较系统变量 max_length_for_sort_data(**默认1024字节**) 的大小和需要查询的字段总大小来判断使用哪种排序模式。

- 如果 字段的总长度小于max_length_for_sort_data ，那么使用 单路排序模式；
- 如果 字段的总长度大于max_length_for_sort_data ，那么使用 双路排序模·式。

比如：              

  mysql> **select * from employees where name = 'zhuge' order by position;**              

（开启trace的命令：               

 mysql> set session optimizer_trace="enabled=on",end_markers_in_json=on;  --开启trace mysql> select * from information_schema.OPTIMIZER_TRACE;     ）         

从上面的sql语句我们先看**单路排序**的详细过程：

1. 从索引name找到第一个满足 name = ‘zhuge’ 条件的主键 id
2. 根据主键 id 取出整行，**取出所有字段的值，存入 sort_buffer 中**
3. 从索引name找到下一个满足 name = ‘zhuge’ 条件的主键 id
4. 重复步骤 2、3 直到不满足 name = ‘zhuge’ 
5. 对 sort_buffer 中的数据按照字段 position 进行排序
6. 返回结果给客户端

我们再看下**双路排序**的详细过程：

1. 从索引 name 找到第一个满足 name = ‘zhuge’  的主键id
2. 根据主键 id 取出整行，**把排序字段 position 和主键 id 这两个字段放到 sort buffer 中**
3. 从索引 name 取下一个满足 name = ‘zhuge’  记录的主键 id
4. 重复 3、4 直到不满足 name = ‘zhuge’ 
5. 对 sort_buffer 中的字段 position 和主键 id 按照字段 position 进行排序
6. 遍历排序好的 id 和字段 position，按照 id 的值**回到原表**中取出 所有字段的值返回给客户端

其实对比两个排序模式，<font color="red">单路排序会把所有需要查询的字段都放到 sort buffer 中，而双路排序只会把主键和需要排序的字段放到 sort buffer 中进行排序，然后再通过主键回到原表查询需要的字段。</font>

如果 MySQL **排序内存** **sort_buffer** 配置的比较小并且没有条件继续增加了，可以适当把 max_length_for_sort_data 配置小点，让优化器选择使用**双路排序**算法，可以在sort_buffer 中一次排序更多的行，只是需要再根据主键回到原表取数据。

如果 MySQL 排序内存有条件可以配置比较大，可以适当增大 max_length_for_sort_data 的值，让优化器优先选择全字段排序(**单路排序**)，把需要的字段放到 sort_buffer 中，这样排序后就会直接从内存里返回查询结果了。

所以，MySQL通过 **max_length_for_sort_data** 这个参数来控制排序，在不同场景使用不同的排序模式，从而提升排序效率。

**注意**，如果全部使用sort_buffer内存排序一般情况下效率会高于磁盘文件排序，但不能因为这个就随便增大sort_buffer(默认1M)，mysql很多参数设置都是做过优化的，不要轻易调整。

如果一定会发生 Using filesort，那么我们要了解的文件排序有两种方式：双路排序和单路排序。

举个例子：

一个表建立的联合索引 index age_salary (age, salary)

Select * from t where id>500 and id<1000 order by  salary, age;

上面的例子中：

双路排序会在二级索引的B+树取出满足where条件的行(501~999行)的salary和age字段（不会取其他字段），和501~999行的地址指针，然后在sort buffer内存中排序。如果sort buffer不够（要排序的salary和age太多了），此时会创建一个 temporary table 存储结果（临时表的出现意味着更多次的io）。排完序之后再根据行指针（这是的行指针也是排好序的）回表（回到主键索引）取记录（排序的时候只取了要排序的字段，现在回表是要取整行的所有字段）。

 

单路排序会取出满足where条件的行(501~999行)的所有字段（不过这样更容易生成临时表，这样的话io反而会比双路排序高），然后再sort buffer中根据salary 和 age字段排序，然后直接输出结果。

 

由于双路排序发生了回表，所以大大增加了io次数（是单路排序的两倍，如果单路排序不生成临时表的话）， 但是单路排序的内存开销更大，更容易在排序过程中生成临时表，从而增加io次数。

Mysql会根据情况选择其中当然一种算法来进行文件排序filesort。但无论是哪种排序我们都可以通过提高 sort_buffer_size 和 max_length_for_sort_data来增大排序缓冲区的大小，减小创建临时表的可能。

 

结论：在非要出现文件排序不可的情况下，可以通过增大排序缓冲区的大小来优化



## 什么是前缀索引？ 

 有时候需要索引很长的字符列，这会让索引变得大且慢，此时可以考虑前缀索引。MySQL目前还不支持函数索引，但是支持前缀索引，即对索引字段的前N个字符创建索引，这个特性可以大大缩小索引文件的大小，从而提高索引效率。用户在设计表结构的时候也可以对文本列根据此特性进行灵活设计。前缀索引是一种能使索引更小、更快的有效办法。 前缀索引的缺点是，在排序ORDER BY和分组GROUP BY操作的时候无法使用，也无法使用前缀索引做覆盖扫描，并且前缀索引降低了索引的选择性。索引的选择性是指不重复的索引值（也称为基数，Cardinality）和数据表的记录总数（COUNT(*)）的比值，范围为(0,1]。索引的选择性越高则查询效率越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的行。唯一索引的选择性是1，这是最好的索引选择性，性能也是最好的。 一般情况下某个前缀的选择性也是足够高的，足以满足查询性能。对于BLOB、TEXT，或者很长的VARCHAR类型的列，必须使用前缀索引，因为MySQL不允许索引这些列的完整长度。 使用前缀索引的诀窍在于要选择足够长的前缀以保证较高的选择性，同时又不能太长（以便节约空间）。**前缀应该足够长，以使得前缀索引的选择性接近于索引的整个列**。换句话说，前缀的“基数”应该接近于完整的列的“基数”。 为了决定前缀的合适长度，需要找到最常见值的列表，然后和最常见的前缀列表进行比较。





## 最左前缀原则、联合索引是什么

当一个SQL想要利用索引是,就一定要提供该索引所对应的字段中最左边的字段,也就是排在最前面的字段,比如针对a,bc三个字段建立了一个联合索引,那么在写一个sql时一定要提供a字段的条件,这样才能用到联合索引,这是由于在建立abc三个字段的联合索引时,==底层的B+树是按照a,b,c三个字段从左往右去比较大小进行排序的==,所以如果想要利用B+树进行快速查找也得符合这个规则

联合索引 (a,b,c) 实际建立了 (a)、(a,b)、(a,b,c) 三个索引

我们可以将组合索引想成书的一级目录、二级目录、三级目录，如index(a,b,c)，相当于a是一级目录，b是一级目录下的二级目录，c是二级目录下的三级目录。要使用某一目录，必须先使用其上级目录，一级目录除外。

如下：

● 联合索引的优势

1）减少开销

建一个联合索引 (a,b,c) ，实际相当于建了 (a)、(a,b)、(a,b,c) 三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！ 

2)覆盖索引

对联合索引 (a,b,c)，如果有如下 sql 的，

SELECT a,b,c from table where a='xx' and b = 'xx';

那么 MySQL 可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机 io 操作。减少 io 操作，特别是随机 io 其实是 DBA 主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。

3)效率高

索引列多，通过联合索引筛选出的数据越少。比如有 1000W 条数据的表，有如下SQL:

select col1,col2,col3 from table where col1=1 and col2=2 and col3=3; 

假设：假设每个条件可以筛选出 10% 的数据。

A. 如果只有单列索引，那么通过该索引能筛选出 1000W10%=100w 条数据，然后再回表从 100w 条数据中找到符合 col2=2 and col3= 3 的数据，然后再排序，再分页，以此类推(递归)；

B. 如果是(col1,col2,col3)联合索引，通过三列索引筛选出 1000w10% 10% *10%=1w，效率提升可想而知！ 





# 高并发情况下，我们系统是如何支撑大量的请求的

有个每秒钟5k个请求，查询手机号所属地的笔试题(记得不完整，没列
出)，如何设计算法?请求再多，比如5w，如何设计整个系统?尽量使用缓存，包括用户缓存，信息缓存等，多花点内存来做缓存，可以大量减少与
数据库的交互，提高性能。
用jprofiler等工具找出性能瓶颈，减少额外的开销。
优化数据库查询语句，减少直接使用hibernate等工具的直接生成语句（仅耗时较长的查询做优化）。
优化数据库结构，多做索引，提高查询效率。
统计的功能尽量做缓存，或按每天一统计或定时统计相关报表，避免需要时进行统计的功能。
能使用静态页面的地方尽量使用，减少容器的解析（尽量将动态内容生成静态html来显示）。
解决以上问题后，使用服务器集群来解决单台的瓶颈问题。

# mysql中的日志

MySQL中有以下7种类型日志文件：

重做日志（redo log）：保证事务的持久性。事务开始后，将执行过程中修改的数据逐步写入log buffer缓冲区，然后刷到磁盘。可以使用double write?buffer两次写保证数据页的完整性。
回滚日志（undo log）：保证事务的原子性。提供多版本并发控制读。每条修改记录都会记录undo log，并通过roll_pointer将undo log串成一个链表,形成版本链。
二进制日志（binlog）：用于实现主从复制。主库数据修改后会记录binlog，给从库读取。
中继日志（relay log）：用于实现主从复制。从库读取主库的binlog，然后写入到relay log，SQL线程通过重放relay log，将数据写入本地数据。
一般查询日志（general log）：默认关闭，记录了服务器接收到的每一个查询或是命令。
慢查询日志（slow query log）：记录执行时间过长和没有使用索引的成功的查询语句
错误日志（errorlog）：默认关闭，错误日志记录着mysqld启动和停止，以及服务器在运行过程中发生的错误的相关信息 

## Mysq慢查询该如何优化?

1.检查是否走了索引,如果没有则优化SQL利用索引
2.检查所利用的索引,是否是最优素引

3.检查是否所查字段都是必须的，是否查询了过多字段，查出了多余数据

4、检查表中数据是否过多，是否要分库分表了

5、检查数据库实例所在的机器性能配置，是否太低，是否可以适当增加资源

- 可通过开启慢查询日志来找出较慢的SQL
- 不做列运算：`SELECT id WHERE age + 1 = 10`，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边
- sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库
- 不用`SELECT *`
- `OR`改写成`IN`：`OR`的效率是n级别，`IN`的效率是log(n)级别，in的个数建议控制在200以内
- 不用函数和触发器，在应用程序实现
- 避免`%xxx`式查询
- 少用`JOIN`

## mysql聚簇和非聚簇索引的区别

都是B+树的数据结构
聚簇索引：将数据存储与索引放到了一块、并且是按照一定的顺序组织的，找到索引也就找到了数据，数据的物理存放顺序与索引顺序是一致的，即：只要索引是相邻的，那么对应的数据一定也是相邻地存放在磁盘上的
非聚簇索引：**叶子节点不存储数据、存储的是数据行地址**，也就是说根据索引查找到数据行的位置再取磁盘查找数据，这个就有点类似一本树的目录，比如我们要找第三章第一节，那我们先在这个目录里面找，找到对应的页码后再去对应的页码看文章。

详细解释：

1. 使⽤记录主键值的⼤⼩进⾏记录和⻚的排序，这包括三个⽅⾯
的含义：
1、⻚内的记录是按照主键的⼤⼩顺序排成⼀个单向链表。
2、各个存放⽤户记录的⻚也是根据⻚中⽤户记录的主键⼤⼩顺序排成⼀个双向链表。
3、存放⽬录项记录的⻚分为不同的层次，在同⼀层次中的⻚也是根据⻚中⽬录项记录的主键⼤⼩顺序排成⼀个双向链表。
2. B+树的叶⼦节点存储的是完整的⽤户记录。
所谓完整的⽤户记录，就是指这个记录中存储了所有列的值
（包括隐藏列）

我们把具有这两种特性的B+树称为聚簇索引，所有完整的⽤户记录
都存放在这个聚簇索引的叶⼦节点处。这种聚簇索引并不需要我们
在MySQL语句中显式的使⽤INDEX语句去创建（后边会介绍索引相关
的语句），InnoDB存储引擎会⾃动的为我们创建聚簇索引。另外有
趣的⼀点是，在InnoDB存储引擎中，聚簇索引就是数据的存储⽅式
（所有的⽤户记录都存储在了叶⼦节点），也就是所谓的索引即数
据，数据即索引。

优势：
1、查询通过聚簇索引可以直接获取数据，相比非聚簇索引需要第二次查询（非覆盖索引的情况下）效率要高
2、聚簇索引对于范围查询的效率很高，因为其数据是按照大小排列的
3、聚簇索引适合用在排序的场合，非聚簇索引不适合
劣势：
1、维护索引很昂贵，特别是插入新行或者主键被更新导至要分页(page split)的时候。建议在大量插入新行后，选在负载较低的时间段，通过OPTIMIZE TABLE优化表，因为必须被移动的行数据可能造成碎片。使用独享表空间可以弱化碎片
2、表因为使用UUId（随机ID）作为主键，使数据存储稀疏，这就会出现聚簇索引有可能有比全表扫面更慢，所以建议使用int的auto_increment作为主键
3、如果主键比较大的话，那辅助索引将会变的更大，因为辅助索引的叶子存储的是主键值；过长的主键值，会导致非叶子节点占用占用更多的物理空间

📣📣【innodb中怎么设置索引】InnoDB中一定有主键，主键一定是聚簇索引，不手动设置、则会使用unique索引，判断表中是否有非空的*唯一*索引(Unique NOT NULL),若有,则该列即为主键(当表中有多个非空*唯一*索引时,*InnoDB* 存储引擎将*选择*建表时第一个定义的非空*唯一*索引,没有unique索引，则会使用数据库内部的一个行的隐藏id来当作主键索引。在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，**非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，**

**辅助索引叶子节点存储的不再是行的物理位置，而是主键值**

MyISM使用的是非聚簇索引，没有聚簇索引，非聚簇索引的两棵B+树看上去没什么不同，节点的结构完全一致只是存储的内容不同而已，主键索引B+树的节点存储了主键，辅助键索引B+树存储了辅助键。表数据存储在独立的地方，这两颗B+树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。由于索引树是独立的，通过辅助键检索无需访问主键的索引树。

如果涉及到大数据量的排序、全表扫描、count之类的操作的话，还是MyISAM占优势些，因为索引所占空间小，这些操作是需要在内存中完成的。

InnoDB存储引擎的默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择B+Tree索引。

(1)可以看到，**InnoDB的B+树索引的结点就是InnoDB的数据页，这些结点通过File Header中的上一页、下一页左右相连成为一个双向链表；**

(2)B+树只有叶子结点才存放数据，非叶子结点的记录头的record_type字段都置为1，叶子节点的记录的record_type字段则是0(除了系统插入的最大记录、最小记录)；

(3)非叶子结点的只有两个字段有效：页号+该页号的页内记录的最小主键id(注意图中红字部分)。这样根据要查的目标记录的id就可以找到它属于拿个页了，然后在页内根据要查的记录主键二分找到最相近的槽号，通过槽号到记录组后就个位数的记录了，直接遍历即可。

(4)二级索引或叫辅助索引也是类似的结构，只是把按照主键寻找改成按照索引字段+主键寻找结点而已(加主键不仅是为了回访主键索引，也是为了保证非叶子结点中记录的唯一性，因为索引字段可能重复)。 

二级索引的叶子结点不记录所有数据，只有索引字段和主键，确定查找目标后需要拿主键回访主键索引。

(5)为c1、c2建立联合索引，也就是二级索引。优先按照建立索引时的左边字段进行排序，即c1。当c1字段相同时，在这些c1相同的连续一串结点里再根据c2字段进行索引。这也是左前缀原则的原理。

B树和B+树的不同点及思考

(1)我们知道从磁盘搬运数据到内存一般都是几KB的搬，避免频繁磁盘IO。所以InnoDB选择一个页作为一个结点，一次性运一个结点进内存，然后在内存中二分查找、遍历找到对应记录。B树的每个结点里的每条记录都是带有完整的一行数据的，这就导致了一个结点中可存储的记录条数变少了，就意味着一个结点开的叉变少了很多，就意味着整个索引树的高度变大了，这显然是不好的。基于此，B+树让全部数据存到叶子结点中，其他结点不存放数据，每个非叶子结点的分叉大大增多，代价就是每次查找都必须走到叶子结点。4层B+树已经可以存很多很多记录了，5到6层已经是极限了。

(2)B+树的所以数据都存到叶子结点，还有双向的指针将这些结点组成双向链表，非常适合范围查找。 



## binlog是什么

### 

binlog是一个二进制格式的文件，用于记录用户对数据库**更新的SQL语句信息**，例如更改数据库表和更改内容的[SQL语句](https://so.csdn.net/so/search?q=SQL语句&spm=1001.2101.3001.7020)都会记录到binlog里，但是对库表等内容的**查询不会记录**。

默认情况下，binlog日志是二进制格式的，不能使用查看文本工具的命令（比如，cat，vi等）查看，而使用mysqlbinlog解析查看。

binlog的作用：

 当有数据写入到数据库时，还会同时把更新的SQL语句写入到对应的binlog文件里，这个文件就是上文说的binlog文件。使用mysqldump备份时，只是对一段时间的数据进行全备，但是如果备份后突然发现数据库服务器故障，这个时候就要用到binlog的日志了

## Redo日志的设置 

在MySQL中，Redo日志文件的大小及个数可以通过innodb_log_file_size和innodb_log_ files_in_group参数来控制调整，那么这两个参数调整的意义及作用是什么呢？在什么情况下做调整呢？在MySQL中Redo日志是循环写的，什么是循环写呢？

![image-20211030172524440](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20211030172524440.png)

如图所示，有三个Redo日志文件，它们之间是循环写入使用的，但这当中存在一个问题，就是当第三个Redo日志文件写完之后要覆盖写第一个文件时不能直接写入，因为此时第一个文件中可能存在一部分Redo日志对应的数据还没有从内存（Buffer Pool）刷新到数据文件中，如果直接覆盖写入，在宕机之后就会出现数据不一致的情况，此时就需要等待数据刷新到数据文件中后才能覆盖写入Redo日志。 图19-3 在MySQL中会有一些机制触发后台线程的异步刷新来避免这种情况的发生，但如果innodb_log_file_size设置得非常小还是会经常发生这种情况的，所以我们要合理设置Redo日志文件的大小和个数。



那么如何监控Redo日志的使用情况呢？ 我们可以将Redo日志的使用情况作为评估数据库繁忙程度的一个指标，通过以下几种方式查看其使用情况。

先选择一个数据库 如use english;

 1．执行show engine innodb status;





2．INNODB_METRICS 

information_schema.innodb_metrics表，就像是一个InnoDB性能和资源相关项的计数器。

show name,count from information schema.innodb metrics where name in ('log lsn current','loglsn last checkpoint');

【命令暂时还无法正常执行】

3．sys schema（系统库） 有了log_lsn_current和log_lsn_last_checkpoint的值，我们就可以计算出Redo日志的使用量。 有时候我们还需要查看每分钟产生的Redo日志量，可以通过以下方式：

=====================================================================

　　自然主键：就是充当主键的字段本身具有一定的含义，是构成记录的组成部分，比如学生的学号，除了充当主键之外，同时也是学生记录的重要组成部分。

　　代理主键：就是充当主键的字段本身不具有业务意义，只具有主键作用，比如自动增长的ID 

======================================================================



## redo日志与binlog协调工作

MySQL是插件式数据库，按结构来讲分为服务器层与存储引擎层，Binlog是服务器层所产生的，Redo日志是InnoDB存储引擎层所产生的。Binlog是纯逻辑日志，用于MySQL中的主从复制和数据恢复，并不是Oracle中的归档日志。这里大家可能会有以下疑问： ·　

Binlog与Redo日志哪个先写？ 

这两个日志以哪个为准呢？ 这里我们就要讲一下MySQL中的两阶段提交的概念。 

通过图19-4可以看到，当会话发起COMMIT（提交）动作时先进行存储引擎的Prepare（准备）工作，这里的Prepare工作只是在对应的Redo日志记录上打上prepare标记。随后会写入Binlog并执行fsync（系统调用，对Binlog执行磁盘同步），最后在Redo日志记录上打上commit标记表示记录已提交完成。那么为什么会先写入Binlog而不是存储引擎的Redo日志呢？
由于MySQL是插件式数据库，可能会同时使用多个存储引擎，像InnoDB这种存储引擎有Redo日志可以做实例恢复，但是其他的存储引擎可能没有Redo日志而就是依赖Binlog做实例恢复。那么如果先写Redo日志并打上commit标记，这时若发生宕机，使用InnoDB存储引擎就可以做实例恢复，将数据恢复过来，而其他的存储引擎由于Binlog没有写成功可能就无法恢复数据了，那么可能就会出现数据不一致的情况。所以，在MySQL中做实例恢复时会依赖Binlog，以Binlog中的内容为准。 现在我们就再讲一下MySQL中的实例恢复方式。对于实例恢复相信Oracle DBA肯定不会陌生，先前滚Redo日志，再通过Undo日志回滚未提交的事务。 但是在MySQL中却有些不同，在MySQL中做实例恢复时先进行前滚，回滚时会判断对应记录的事务状态： ·　

对于活跃状态的事务，可直接进行回滚。 ·　

对于Prepare状态的事务，如果该事务对应的Binlog已经记录则提交，否则回滚事务。



![image-20211030192857483](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20211030192857483.png)

Prepare状态就是图19-4中在Redo日志中所打上的prepare标记的状态。 有时候大家可能会发现数据库启动失败，在错误日志中查看报错原因是找不到Binlog的，这就是因为在做实例恢复时，需要查询Binlog来判断对应的事务在Binlog中是否已执行fsync成功。 

redolog的写入过程：

start transaction;
update t_user set name = '路人甲Java' where user_id = 666;
update t_user set name = 'javacode2018' where user_id = 888;
commit;

1. mysql收到start transaction后，生成一个全局的事务编号trx_id，比如trx_id=10 user_id=666这个记录我们就叫r1，user_id=888这个记录叫r2
2. 找到r1记录所在的数据页p1，将其从磁盘中加载到内存中
3. 在内存中找到r1在p1中的位置，然后对p1进行修改（这个过程可以描述为：将p1中的pos_start1到pos_start2位置的值改为v1），这个过程我们记为rb1(内部包含事务编号trx_id)，将rb1放入
    redo log buffer数组中，此时p1的信息在内存中被修改了，和磁盘中p1的数据不一样了
4. 找到r2记录所在的数据页p2，将其从磁盘中加载到内存中
5. 在内存中找到r2在p2中的位置，然后对p2进行修改（这个过程可以描述为：将p2中的pos_start1到pos_start2位置的值改为v2），这个过程我们记为rb2(内部包含事务编号trx_id)，将rb2放入
    redo log buffer数组中，此时p2的信息在内存中被修改了，和磁盘中p2的数据不一样了
6. 此时redo log buffer数组中有2条记录[rb1,rb2]
7. mysql收到commit指令
8. 将redo log buffer数组中内容写入到redo log文件中，写入的内容：
9. 返回给客户端更新成功。
   上面过程执行完毕之后，数据是这样的：
10. 内存中p1、p2页被修改了，还未同步到磁盘中，此时内存中数据页和磁盘中数据页是不一致的，此时内存中数据页我们称为脏页
11. **对p1、p2页修改被持久到磁盘中的redolog文件中了，不会丢失**
      认真看一下上面过程中第9步骤，一个成功的事务记录在redo log中是有start和end的，redo log文件中如果一个trx_id对应start和end成对出现，说明这个事务执行成功了，如果只有start没有end说明是有问题的。
      那么对p1、p2页的修改什么时候会同步到磁盘中呢？
      redo log是mysql中所有连接共享的文件，对mysql执行insert、delete和上面update的过程类似，都是先在内存中修改页数据，然后将修改过程持久化到redo log所在的磁盘文件中，然后返回成功。redolog文件是有大小的，需要重复利用的，当redo log满了，或者系统比较闲的时候，会对redo log文件中的内容进行处理，处理过程如下：
12. 读取redo log信息，读取一个完整的trx_id对应的信息，然后进行处理
13. 比如读取到了trx_id=10的完整内容，包含了start end，表示这个事务操作是成功的，然后继续向下判断p1在内存中是否存在，如果存在，则直接将p1信息写到p1所在的磁盘中；如果p1在内存中不存在，则将p1从磁盘加载到内存，通过redo log中的信息在内存中对p1进行修改，然后将其写到磁盘中上面的update之后，p1在内存中是存在的，并且p1是已经被修改过的，可以直接刷新到磁盘中。
      1.start trx=10;
      2.写入rb1
      3.写入rb2
      4.end trx=10;





# Mysql锁有哪些,如何理解

[MySQL各种锁机制撸一遍 (qq.com)](https://mp.weixin.qq.com/s/Tknaz9s17AKymAgWpkyXkA)

加锁的目的：

数据库是一个多用户使用的共享资源。当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据库的一致性。**锁是用于管理对公共资源的并发控制。**也就是说在并发的情况下，会出现资源竞争，所以需要加锁。
加锁解决了 多用户环境下保证数据库完整性和一致性。
使用锁的对象是事务，事务用来锁定数据库的对象是表、页、行。并且一般锁定的对象仅在事务commit或rollback后进行释放（不同事务隔离级别释放的时间可能不同）。

锁的分类

![图片](https://mmbiz.qpic.cn/mmbiz/OKUeiaP72uRwuLCoPlbWbTm8QfA2Jv5ZSic68HYO3xUaVcG4p9vuB1kCmtO2C1T2gfK9vN1bMwstOncBopBeRy4A/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

锁粒度

一、行锁

行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。有可能会出现死锁的情况，出现死锁的解决办法就是必须有一方事务回滚或者同时回滚。另外，行级锁按照使用方式分为共享锁和排他锁。

二、表锁

表级锁是mysql锁中粒度最大的一种锁，表示当前的操作对整张表加锁，资源开销比行锁少，不会出现死锁的情况，但是发生锁冲突的概率很大。被大部分的mysql引擎支持，MyISAM和InnoDB都支持表级锁，但是InnoDB默认的是行级锁。
Mysql的表级别锁分为两类：元数据锁（Metadata Lock，MDL）、表锁。

元数据锁（Metadata Lock，MDL）

元数据锁(MDL) 不需要显式使用，在访问一个表的时候会被自动加上。这个特性需要MySQL5.5版本以上才会支持，当对一个表做增删改查的时候，该表会被加MDL读锁当对表做结构变更的时候，加MDL写锁

- MDL锁规则：
  1、读锁之间不互斥
  2、读写锁、写锁之间是互斥的，为了保证表结构变更的安全性，所以如果要多线程对同一个表加字段等表结构操作，就会变成串行化，需要进行锁等待
  3、MDL的写锁优先级比MDL读锁的优先级高
  4、MDL的锁释放必须要等到事务结束才会释放

MDL锁的例子

![图片](https://mmbiz.qpic.cn/mmbiz/OKUeiaP72uRwuLCoPlbWbTm8QfA2Jv5ZSzL17RWClRdheJf9Ljfhsco0E61oZlwyNPco0gMCUMKr3jR83bXDibUA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

若没有MDL锁的保护，则事务2可以直接执行DDL操作，并且导致事务1出错，5.1版本即是如此。5.5版本加入MDL锁就在于保护这种情况的发生，由于事务1开启了查询，那么获得了MDL锁，锁的模式为SHARED_READ(读锁模式)，事务2要执行DDL，则需获得EXCLUSIVE锁（写锁），两者互斥，所以事务2需要等待。

页锁

页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。



声明数据库行锁
begin tran tran1
select * from MPHead with(rowlock,updlock) where id=1;
waitfor delay '00:00:10';
commit tran;

按锁粒度分类:
1.行锁:锁某行数据,锁粒度最小,并发度高
2.表锁:锁整张表,锁粒度最大,并发度低
3.间隙锁:锁的是一个区间
还可以分为
1.共享锁:也就是读锁,一个事务给某行数据加了读,其他事务也可以读,但是不能写
2.排它锁:也就是写锁,一个事务给某行数据加了锁,其他事务不能读,也不能写
还可以分为:
1.乐观锁:并不会真正的去锁某行记录,而是通过一个版本号来实现的
2.悲观锁:上面所的行锁、表锁等都是悲观锁
在事务的隔离级别实现中,就需要利用所来解决幻读



## for update

 

1.for update的使用场景 

2.for update如何使用 

3.for update的锁表 

4.for update的注意点 

5.for update的疑问点

 

for update的使用场景

如果遇到存在高并发并且对于数据的准确性很有要求的场景，是需要了解和使用for update的。

 

比如涉及到金钱、库存等。一般这些操作都是很长一串并且是开启事务的。如果库存刚开始读的时候是1，而立马另一个进程进行了update将库存更新为0了，而事务还没有结束，会将错的数据一直执行下去，就会有问题。所以需要for upate 进行数据加锁防止高并发时候数据出错。

 

记住一个原则：一锁二判三更新

 

for update如何使用

使用姿势：

```
select` `* from table where xxx ``for` `update
```

 

for update的锁表

InnoDB默认是行级别的锁，当有明确指定的主键时候，是行级锁。否则是表级别。

 例子: 假设表foods ，存在有id跟name、status三个字段，id是主键，status有索引。

 例1: (明确指定主键，并且有此记录，行级锁) 

```
SELECT * FROM foods WHERE ``id``=1 FOR UPDATE; ``SELECT * FROM foods WHERE ``id``=1 and name=’咖啡色的羊驼’ FOR UPDATE;
```



例2: (明确指定主键/索引，若查无此记录，无锁) 

SELECT * FROM foods WHERE id=-1 FOR UPDATE;

例3: (无主键/索引，表级锁) 

```
SELECT * FROM foods WHERE name=’咖啡色的羊驼’ FOR UPDATE;
```

 例4: (主键/索引不明确，表级锁) 

```
SELECT * FROM foods WHERE ``id``<>’3’ FOR UPDATE; ``SELECT * FROM foods WHERE ``id` `LIKE ‘3’ FOR UPDATE;
```

 for update的注意点

1.for update 仅适用于InnoDB，并且必须开启事务，在begin与commit之间才生效。

 2.要测试for update的锁表情况，可以利用MySQL的Command Mode，开启二个视窗来做测试。

 for update的疑问点

当开启一个事务进行for update的时候，另一个事务也有for update的时候会一直等着，直到第一个事务结束吗？

 答：会的。除非第一个事务commit或者rollback或者断开连接，第二个事务会立马拿到锁进行后面操作。

 如果没查到记录会锁表吗？

 答：会的。表级锁时，不管是否查询到记录，都会锁定表。

 

获取innodb行级锁的争用情况：

show status like '%innodb_row_lock%';



# 简述mysql中索引类型有哪些，以及对数据库的性能的影响？

　　普通索引：允许被索引的数据列包含重复的值
　　唯一索引：可以保证数据记录的唯一性
　　主键索引：是一种特殊的唯一索引，在一张表中只能定义一个主键索引，主键用于唯一标识一条记录，使用关键字primary key:来创建
　　联合索引：索引可以覆盖多个数据列
　　全文索引：通过建立倒排索引，可以极大的提升检索效率，解决判断字段是否包含的问题，是目前搜索引擎使用的一种关键技术(用的很少，一般在比如es搜索引擎中有成熟的方案)
　　索引可以极大地提高数据的查询速度
　　通过使用索引，**可以在查询的过程中，使用优化隐藏器，提高系统的性能**，但是会降低插入、删除、更新表的速度，因为在执行这些写操作的时候，还要操作索引文件索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要简历聚簇索引，那么需要的空间就会更大，如果非聚簇索引很多，一旦聚簇索引改变，那么所有非聚簇索引都会跟着变 



# Innodb是如何实现事务的

Innodbi通过 Buffer Pool,LogBuffer,. Redo Log,. Undo Log来实现事务,以一个 updatei语句为例:

1.Innodb在收到一个 updatei语句后,会先根条件找到数据所在的页,并将该页缓存在 Buffer Pool中

2.执行 updatei语句,修改 Buffer Pool中的数据,也就是内存中的数据
3.针对 updatei语句生成一个 RedoLog对象,并存入 LogBuffer中
4.针对 updatei语句生成 undolog日志,用于事务回滚
5.如果事务提交,那么则把 RedoLogX对象进行久化,后续还有其他机制将Buffer Pool中所改的数据页持久化到磁盘中
6.如果事务回滚,则利用 undolog日志进行回滚

# 回表是什么

非主键索引，我们先通过索引找到主键索引的键值，再通过主键值查出索引里面没有的数据，**它比基于主键索引的查询多扫描了一棵索引树，这个过程就叫回表。**

建一个表：

```java
create table xttblog(
    id int primary key, 
    k int not null, 
    name varchar(16),
    index (k)
)engine = InnoDB;
 

```

假设，现在我们要查询出 id 为 2 的数据。那么执行 select * from xttblog where ID = 2; 这条 SQL 语句就不需要回表。==原因是根据主键的查询方式，则只需要搜索 ID 这棵 B+ 树==。主键是唯一的，根据这个唯一的索引，MySQL 就能确定搜索的记录。

但当我们使用 k 这个索引来查询 k = 2 的记录时就要用到回表。select * from xttblog where k = 2; 原因是通过 k 这个普通索引查询方式，==则需要先搜索 k 索引树，然后得到主键 ID 的值为 1，再到 ID 索引树搜索一次==。这个过程虽然用了索引，但实际上底层进行了两次索引查询，这个过程就称为回表。 





# mysql server系统架构

MySQL 可以看成是二层架构，第一层我们通常叫做SQL Layer，在MySQL 数据库系统处理底层数据之前的所有工作都是在这一层完成的，包括权限判断，sql 解析，执行计划优化，query cache 的处理等等；第二层就是存储引擎层，我们通常叫做 Storage Engine Layer，也就是底层数据存取操作实现部分，由多种存储引擎共同组成。所以，可以用如下一张最简单的架构示意图来表示MySQL 的基本架构

![image-20210723125443966](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210723125443966.png)

SQL Layer 中包含了多个子模块，下面我将逐个做一下简单的介绍： 1、初始化模块
顾名思议，初始化模块就是在MySQL Server 启动的时候，对整个系统做各种各样的初始化操作，比如各种 buffer，cache 结构的初始化和内存空间的申请，各种系统变量的初始化设定，各种存储引擎的初始化设置，等等。
2、核心API
核心API 模块主要是为了提供一些需要非常高效的底层操作功能的优化实现，包括各种底层数据结构的实现，特殊算法的实现，字符串处理，数字处理等，小文件I/O，格式化输出，以及最重要的内存管理部分。核心 API 模块的所有源代码都集中在 mysys 和 strings 文件夹下面，有兴趣的读者可以研究研究。
3、网络交互模块
底层网络交互模块抽象出底层网络交互所使用的接口api，实现底层网络数据的接收与发送，以方便其他各个模块调用，以及对这一部分的维护。所有源码都在vio 文件夹下面。
4、Client & Server 交互协议模块
任何C/S 结构的软件系统，都肯定会有自己独有的信息交互协议，MySQL 也不例外。MySQL的 Client & Server 交互协议模块部分，实现了客户端与 MySQL 交互过程中的所有协议。当然这些协议都是建立在现有的OS 和网络协议之上的，如TCP/IP 以及Unix Socket。
5、用户模块
用户模块所实现的功能，主要包括用户的登录连接权限控制和用户的授权管理。他就像MySQL 的大门守卫一样，决定是否给来访者“开门”。

6、访问控制模块
造访客人进门了就可以想干嘛就干嘛么？为了安全考虑，肯定不能如此随意。这时候就需要访问控制模块实时监控客人的每一个动作，给不同的客人以不同的权限。访问控制模块实现的功能就是根据用户模块中各用户的授权信息，以及数据库自身特有的各种约束，来控制用户对数据的访问。用户模块和访问控制模块两者结合起来，组成了 MySQL 整个数据库系统的权限安全管理的功能。
7、连接管理、连接线程和线程管理
连接管理模块负责监听对MySQL Server 的各种请求，接收连接请求，转发所有连接请求到线程管理模块。每一个连接上MySQL Server 的客户端请求都会被分配（或创建）一个连接线程为其单独服务。而连接线程的主要工作就是负责 MySQL Server 与客户端的通信， 接受客户端的命令请求，传递Server 端的结果信息等。线程管理模块则负责管理维护这些连接线程。包括线程的创建，线程的cache 等。
8、Query 解析和转发模块
在MySQL 中我们习惯将所有Client 端发送给Server 端的命令都称为query，在 MySQL Server 里面，连接线程接收到客户端的一个 Query 后，会直接将该 query 传递给专门负责将各种Query 进行分类然后转发给各个对应的处理模块，这个模块就是 query 解析和转发模块。其主要工作就是将 query 语句进行语义和语法的分析，然后按照不同的操作类型进行分类，然后做出针对性的转发。
9、Query Cache 模块
Query Cache 模块在MySQL 中是一个非常重要的模块，他的主要功能是将客户端提交给MySQL 的Select 类query 请求的返回结果集cache 到内存中，与该 query 的一个hash 值做一个对应。该Query 所取数据的基表发生任何数据的变化之后，MySQL 会自动使该query 的Cache 失效。在读写比例非常高的应用系统中，Query Cache 对性能的提高是非常显著的。当然它对内存的消耗也是非常大的。
10、Query 优化器模块
Query 优化器，顾名思义，就是优化客户端请求的query，根据客户端请求的query 语句，和数据库中的一些统计信息，在一系列算法的基础上进行分析，得出一个最优的策略， 告诉后面的程序如何取得这个query 语句的结果。
11、表变更管理模块
表变更管理模块主要是负责完成一些DML 和 DDL 的query，如：update，delte，insert， create table，alter table 等语句的处理。
12、表维护模块
表的状态检查，错误修复，以及优化和分析等工作都是表维护模块需要做的事情。
13、系统状态管理模块
系统状态管理模块负责在客户端请求系统状态的时候，将各种状态数据返回给用户，像DBA 常用的各种show status 命令，show variables 命令等，所得到的结果都是由这个模块返回的。

14、表管理器
这个模块从名字上看来很容易和上面的表变更和表维护模块相混淆，但是其功能与变更及维护模块却完全不同。大家知道，每一个 MySQL 的表都有一个表的定义文件，也就是*.frm 文件。表管理器的工作主要就是维护这些文件，以及一个cache，该cache 中的主要内容是各个表的结构信息。此外它还维护table 级别的锁管理。
15、日志记录模块
日志记录模块主要负责整个系统级别的逻辑层的日志的记录，包括error log，binary log，slow query log 等。
16、复制模块
复制模块又可分为 Master 模块和 Slave 模块两部分， Master 模块主要负责在Replication 环境中读取Master 端的binary 日志，以及与 Slave 端的I/O 线程交互等工作 。Slave 模块比 Master 模块所要做的事情稍多一些，在系统中主要体现在两个线程上面。一个是负责从Master 请求和接受binary 日志，并写入本地 relay log 中的I/O 线程。另外一个是负责从relay log 中读取相关日志事件，然后解析成可以在 Slave 端正确执行并得到和Master 端完全相同的结果的命令并再交给Slave 执行的SQL 线程。
17、存储引擎接口模块
存储引擎接口模块可以说是 MySQL 数据库中最有特色的一点了。目前各种数据库产品中，基本上只有 MySQL 可以实现其底层数据存储引擎的插件式管理。这个模块实际上只是一个抽象类，但正是因为它成功地将各种数据处理高度抽象化，才成就了今天 MySQL 可插拔存储引擎的特色。

在了解了MySQL 的各个模块之后，我们再看看MySQL 各个模块间是如何相互协同工作的 。接下来，我们通过启动MySQL，客户端连接，请求query，得到返回结果，最后退出，这样一整个过程来进行分析。

当我们执行启动MySQL 命令之后，MySQL 的初始化模块就从系统配置文件中读取系统参数和命令行参数，并按照参数来初始化整个系统，如申请并分配buffer，初始化全局变量， 以及各种结构等。同时各个存储引擎也被启动，并进行各自的初始化工作。当整个系统初始化结束后，由连接管理模块接手。连接管理模块会启动处理客户端连接请求的监听程序，包括 tcp/ip 的网络监听，还有unix 的socket。这时候，MySQL Server 就基本启动完成，准备好接受客户端请求了。

当连接管理模块监听到客户端的连接请求（借助网络交互模块的相关功能），双方通过 Client & Server 交互协议模块所定义的协议“寒暄”几句之后，连接管理模块就会将连接请求转发给线程管理模块，去请求一个连接线程。
线程管理模块马上又会将控制交给连接线程模块，告诉连接线程模块：现在我这边有连接请求过来了，需要建立连接，你赶快处理一下。连接线程模块在接到连接请求后，首先会检查当前连接线程池中是否有被cache 的空闲连接线程，如果有，就取出一个和客户端请求连接上，如果没有空闲的连接线程，则建立一个新的连接线程与客户端请求连接。当然，连接线程模块并不是在收到连接请求后马上就会取出一个连接线程连和客户端连接，而是首先通过调用用户模块进行授权检查，只有客户端请求通过了授权检查后，他才会将客户端请求和负责请求的连接线程连上。

在 MySQL 中，将客户端请求分为了两种类型：一种是query，需要调用Parser 也就是Query 解析和转发模块的解析才能够执行的请求；一种是 command，不需要调用 Parser 就可以直接执行的请求。如果我们的初始化配置中打开了 Full Query Logging 的功能，那么Query 解析与转发模块会调用日志记录模块将请求计入日志，不管是一个 Query 类型的请求还是一个command 类型的请求，都会被记录进入日志，所以出于性能考虑，一般很少打开Full Query Logging 的功能。

![image-20210723130137911](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210723130137911.png)



# 查询缓冲区

如何使用查询缓冲区？ 

查询缓冲区可以提高查询的速度，但是这种方式只适合查询语句比较多、更新语句比较少的情况。默认情况下，查询缓冲区的大小为0，也就是不可用。可以修改query_cache_size以调整查询缓冲区大小；修改query_cache_type以调整查询缓冲区的类型。在my.ini中修改query_cache_size和query_cache_type的值如下： query_cache_type＝1表示开启查询缓冲区。只有在查询语句中包含SQL_NO_CACHE关键字时，才不会使用查询缓冲区。可以使用FLUSH QUERY CACHE语句来刷新缓冲区，清理查询缓冲区中的碎片。 

# 简单说一说drop、delete与truncate的区别
SQL中的drop、delete、truncate都表示删除，但是三者有一些差别
delete和truncate只删除表的数据不删除表的结构 速度,一般来说: drop> truncate >delete delete语句是dml,这个操作会放到rollback segement中,事务提交之后才生效; 如果有相应的trigger,执行的时候将被触发. truncate,drop是ddl, 操作立即生效,原数据不放到rollback segment中,不能回滚.
操作不触发trigger.



# 什么是内联接、左外联接、右外联接？

内联接（Inner Join）：匹配2张表中相关联的记录。
左外联接（Left Outer Join）：除了匹配2张表中相关联的记录外，还会匹配左表中剩余的记录，右表中未匹配到的字段用NULL表示。
右外联接（Right Outer Join）：除了匹配2张表中相关联的记录外，还会匹配右表中剩余的记录，左表中未匹配到的字段用NULL表示。在判定左表和右表时，要根据表名出现在Outer Join的左右位置关系。











 

# 配置主从服务器

MySQL从3.25.15版本开始提供数据库复制（Replication）功能。MySQL复制是指从一个MySQL主服务器（Master）将数据复制到另一台或多台MySQL从服务器（Slave）的过程，将主数据库的DDL和DML操作通过二进制日志传到从服务器上，然后在从服务器上对这些日志重新执行，从而使得主从服务器的数据保持同步。 在MySQL中，复制操作是异步进行的，Slave服务器不需要持续地保持连接用于接收Master服务器的数据。 MySQL支持一台主服务器同时向多台从服务器进行复制操作，从服务器同时可以作为其他从服务器的主服务器，如果MySQL主服务器访问量比较大，可以通过复制数据，然后在从服务器上进行查询操作，从而降低主服务器的访问压力，同时从服务器作为主服务器的备份，可以避免主服务器因为故障数据丢失的问题。 MySQL数据库复制操作大致可以分成三个步骤： 

步骤01　主服务器将数据的改变记录到二进制日志（binary log）中。 

步骤02　从服务器将主服务器的binary log events复制到它的中继日志（relay log）中。 

步骤03　从服务器重做中继日志中的事件，将数据的改变与从服务器保持同步。 首先，主服务器会记录二进制日志，每个事务更新数据完成之前，主服务器将这些操作的信息记录在二进制日志里面，在事件写入二进制日志完成后，主服务器通知存储引擎提交事务。 Slave上面的I/O进程连接上Master，并发出日志请求，Master接收到来自Slave的IO进程的请求，根据请求信息添加位置信息后，返回给Slave的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息、已经到Master端的bin-log文件的名称以及bin-log的位置。 Slave的I/O进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到Master端的bin-log的文件名和位置记录到master-info文件中。 Slave的SQL进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容，成为在Master端真实执行时的那些可执行的内容，并在自身执行。 MySQL复制环境90%以上都是一个Master带一个或者多个Slave的架构模式。如果Master和Slave的压力不是太大的话，异步复制的延时一般都很少。尤其是Slave端的复制方式改成两个进程处理之后，更是减小了Slave端的延时。 

grant replication slave on \*.* to repl@'%' identified by '123';

my.ini中：

server-id =1

binlog-do-db = test

binlog-ignore-db = mysql

·server-id表示服务器标识ID号，Master和Slave主机的server-id不能一样。 ·binlog-do-db表示需要复制的数据库，这里以test数据库为例。 ·binlog-ignore-db表示不需要复制的数据库。

　重启Master主机的MySQL服务，然后输入show master status命令查询Master主机的信息。 

# 分库分表后id主键如何处理

分成多个表后我们需要一个全局唯一id来支持

**全局唯一id介绍**

系统唯一id是我们在设计阶段常常遇到的问题。在复杂的分布式系统中，几乎都需要对大量的数据和消息进行唯一标识。在设计初期，我们需要考虑日后数据量的级别，如果可能会对数据进行分库分表，那么就需要有一个全局唯一id来标识一条数据或记录。生成唯一id的策略有多种，但是每种策略都有它的适用场景、优点以及局限性。

全局唯一id特点:

- **全局唯一性**：不能出现重复的ID号，既然是唯一标识，这是最基本的要求；
- **趋势递增**：在MySQL InnoDB引擎中使用的是聚集索引，由于多数RDBMS使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能；
- **单调递增**：保证下一个ID一定大于上一个ID，例如事务版本号、IM增量消息、排序等特殊需求；
- **信息安全**：如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定URL即可；如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要ID无规则、不规则；
- **高可用性**：同时除了对ID号码自身的要求，业务还对ID号生成系统的可用性要求极高，想象一下，如果ID生成系统瘫痪，这就会带来一场灾难。所以不能有单点故障；
- **分片支持**：可以控制ShardingId。比如某一个用户的文章要放在同一个分片内，这样查询效率高，修改也容易；
- **长度适中**。

**常见全局唯一id生成策略**

![图片](https://mmbiz.qpic.cn/mmbiz_png/eQPyBffYbuetyac8TibKV83bfb1DPqbpylO4wiasKEPywftUK3Y8vTgeM6g6ZLO5ibzs7ZXpbZesGXV45EGGvoPCg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 1、数据库自增长序列或字段生成id

最常见的一种生成id方式。利用数据库本身来进行设置，在全数据库内保持唯一。

**【优点】**

非常简单。利用现有数据库系统的功能实现，成本小，代码简单，性能可以接受。ID号单调递增。可以实现一些对ID有特殊要求的业务，比如对分页或者排序结果这类需求有帮助。

**【缺点】**

1. 强依赖DB。不同数据库语法和实现不同，数据库迁移的时候、多数据库版本支持的时候、或分表分库的时候需要处理，会比较麻烦。当DB异常时整个系统不可用，属于致命问题。
2. 单点故障。在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险。
3. 数据一致性问题。配置主从复制可以尽可能的增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号。
4. 难于扩展。在性能达不到要求的情况下，比较难于扩展。ID发号性能瓶颈限制在单台MySQL的读写性能。

**【部分优化方案】**

针对主库单点， 如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。比如：Master1 生成的是 `1,4,7,10`，Master2生成的是`2,5,8,11` Master3生成的是 `3,6,9,12`。这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。

### 2、UUID

常见的生成id方式，利用程序生成。

UUID (`Universally Unique Identifier`) 的目的，是让分布式系统中的所有元素，都能有唯一的辨识资讯，而不需要透过中央控制端来做辨识资讯的指定。如此一来，每个人都可以建立不与其它人冲突的 UUID。在这样的情况下，就不需考虑数据库建立时的名称重复问题。

UUID的标准形式包含32个16进制数字，以连字号分为五段，形式为8-4-4-4-12的36个字符，示例：`550e8400-e29b-41d4-a716-446655440000`，到目前为止业界一共有5种方式生成UUID。另外关注公众号码猿技术专栏，回复关键词“9527”送你一份阿里内部Spring Cloud 实战教程！

在Java中我们可以直接使用下面的API生成UUID:

```
UUID uuid  =  UUID.randomUUID(); String s = UUID.randomUUID().toString();
```

**【优点】**

- 非常简单，本地生成，代码方便，API调用方便。
- 性能非高。生成的id性能非常好，没有网络消耗，基本不会有性能问题。
- 全球唯一。在数据库迁移、系统数据合并、或者数据库变更的情况下，可以 从容应对。

**【缺点】**

- **存储成本高**。UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用。如果是海量数据库，就需要考虑存储量的问题。
- **信息不安全**。基于MAC地址生成UUID的算法可能会造成MAC地址泄露，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。
- **不适用作为主键**，ID作为主键时在特定的环境会存在一些问题，比如做DB主键的场景下，UUID就非常不适用。UUID往往是使用字符串存储，查询的效率比较低。
- **UUID是无序的**。不是单调递增的，而现阶段主流的数据库主键索引都是选用的B+树索引，对于无序长度过长的主键插入效率比较低。
- **传输数据量大**。
- **不可读**。

**【部分优化方案】**

- **为了解决UUID不可读**， 可以使用UUID to Int64的方法 。
- **为了解决UUID无序的问题**， NHibernate在其主键生成方式中提供了Comb算法（combined guid/timestamp）。保留GUID的10个字节，用另6个字节表示GUID生成的时间（DateTime）。

### 3、Redis生成ID

当使用数据库来生成ID性能不够要求的时候，我们可以尝试使用Redis来生成ID。这主要依赖于Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作 INCR和INCRBY来实现。

可以使用Redis集群来获取更高的吞吐量。假如一个集群中有5台Redis。可以初始化每台Redis的值分别是1,2,3,4,5，然后步长都是5。各个Redis生成的ID为：

- A：1,6,11,16,21
- B：2,7,12,17,22
- C：3,8,13,18,23
- D：4,9,14,19,24
- E：5,10,15,20,25

这个负载到哪台机器上需要提前设定好，未来很难做修改。但是3-5台服务器基本能够满足，都可以获得不同的ID。步长和初始值一定需要事先设定好。使用Redis集群也可以防止单点故障的问题。

比较适合使用Redis来生成日切流水号。比如订单号=日期+当日自增长号。可以每天在Redis中生成一个Key，使用INCR进行累加。

**【优点】**

- 不依赖于数据库，灵活方便，且性能优于数据库。。
- 数字ID天然排序，对分页或者需要排序的结果很有帮助。

**【缺点】**

- 如果系统中没有Redis，还需要引入新的组件，增加系统复杂度。。
- 需要编码和配置的工作量比较大。
- Redis单点故障，影响序列服务的可用性。

### 4、zookeeper生成ID

zookeeper主要通过其znode数据版本来生成序列号，可以生成32位和64位的数据版本号，客户端可以使用这个版本号来作为唯一的序列号。

很少会使用zookeeper来生成唯一ID。主要是由于需要依赖zookeeper，并且是多步调用API，如果在竞争较大的情况下，需要考虑使用分布式锁。因此，性能在高并发的分布式环境下，也不甚理想。

### 5、Twitter的snowflake算法

snowflake(雪花算法)是Twitter开源的分布式ID生成算法，结果是一个long型的ID。这种方案把64-bit分别划分成多段，分开来标示机器、时间等。如图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/eQPyBffYbuetyac8TibKV83bfb1DPqbpyrEpK1vDShZ7abmibjib2AcPrFeKE3jbsua07TYfxib1VyeBN6Twk4Io4g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其核心思想是：使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。具体实现的代码可以参看github。

snowflake算法可以根据自身项目的需要进行一定的修改。比如估算未来的数据中心个数，每个数据中心的机器数以及统一毫秒可以能的并发数来调整在算法中所需要的bit数。

**【优点】**

- **稳定性高**，不依赖于数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。
- **灵活方便**，可以根据自身业务特性分配bit位。
- **单机上ID单调自增**，毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。

**【缺点】**

- 强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。
- ID可能不是全局递增。在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况。



# 1、一张表，删除一条后再增添记录这条记录的ID是 ？

1、一张表，里面有ID自增主键，当insert了17条记录之后，删除了第15,16,17条记录，再把Mysql重启，再insert一条记录，这条记录的ID是18还是15 ？

(1)如果表的类型是MyISAM，那么是18
因为MyISAM表会把自增主键的最大ID记录到数据文件里，重启MySQL自增主键的最大ID也不会丢失
（2）如果表的类型是InnoDB，那么是15
InnoDB表只是把自增主键的最大ID记录到内存中，所以重启数据库或者是对表进行OPTIMIZE操作，都会导致最大ID丢失





# MySQL的自增 ID 用完了，怎么办

自增id

说到自增id，相信你的第一反应一定是在设计表结构的时候自定义一个自增id字段，那么就有一个问题啦，在插入数据时有可能唯一主键冲、sql事务回滚、批量插入的时候，批量申请自增值等原因导致自增id是不连续的。



表定义的自增值达到上线后的逻辑是：再申请下一个id的时候，获取的是同一个值（最大值）。大家可以插入sql设置id是最大值，再insert一条不主动设置id的语句就可以验证这一结论啦。这个时候如果再插入就是报主键冲突咯～

这里提醒一下：232-1（4294967295）不是一个特别大的数，对于一个频繁插入删除数据的表来说，是可能会被用完的。因此在建表的时候你需要考察你的表是否有可能达到这个上限，如果有可能，就应该创建成 8 个字节的 bigint unsigned。



## InnoDB系统自增row_id

如果你创建的 InnoDB 表没有指定主键，那么 InnoDB 会给你创建一个不可见的，长度为 6 个字节的 row_id。InnoDB 维护了一个全局的 dict_sys.row_id 值，所有无主键的 InnoDB 表，每插入一行数据，都将当前的 dict_sys.row_id 值作为要插入数据的 row_id，然后把 dict_sys.row_id 的值加 1。

实际上，在代码实现时 row_id 是一个长度为8字节的无符号长整型 (bigint unsigned)。但是，InnoDB 在设计时，给 row_id 留的只是 6 个字节的长度，这样写到数据表中时只放了最后 6 个字节，所以 row_id 能写到数据表中的值，就有两个特征：

row_id 写入表中的值范围，是从 0 到 248-1；

当 dict_sys.row_id=2^48时，如果再有插入数据的行为要来申请 row_id，拿到以后再取最后 6 个字节的话就是 0。

虽然，2^48这个数字已经很大了，但是大家要知道 一个系统是可以跑很久的，那么还是可能达到上限的，这时候再申请就会覆盖原来的记录了。因此，尽量不要选择这种方式！

## Xid

MySQL中redo log 和 binlog 相配合的时候，它们有一个共同的字段叫作 Xid。它在 MySQL 中是用来对应事务的。

MySQL 内部维护了一个全局变量 global_query_id，每次执行语句的时候将它赋值给 Query_id，然后给这个变量加 1。如果当前语句是这个事务执行的第一条语句，那么 MySQL 还会同时把 Query_id 赋值给这个事务的 Xid。而 global_query_id 是一个纯内存变量，重启之后就清零了。所以在同一个数据库实例中，不同事务的 Xid 也是有可能相同的。

## Innodb trx_id



InnoDB 内部维护了一个 max_trx_id 全局变量，每次需要申请一个新的 trx_id 时，就获得 max_trx_id 的当前值，然后并将 max_trx_id 加 1。

InnoDB 数据可见性的核心思想是：每一行数据都记录了更新它的 trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的 trx_id 做对比。但是这个过程有脏读存在，那么这个id就不会是原子性的，存在重复的可能性。

## thread_id

其实，线程 id 才是 MySQL 中最常见的一种自增 id。平时我们在查各种现场的时候，show processlist 里面的第一列，就是 thread_id。

thread_id 的逻辑很好理解：系统保存了一个全局变量 thread_id_counter，每新建一个连接，就将 thread_id_counter 赋值给这个新连接的线程变量。

thread_id_counter 定义的大小是 4 个字节，因此达到 232-1 后，它就会重置为 0，然后继续增加。结果跟row_id一样，就会覆盖原有记录了。

上面介绍了几种MySQL自身的一些自增id，其实，实际运用中，我们也可能会选择外部的自增主键，然后持久化到数据库，以此来代替数据库自身的自增id。下面来说说吧。

## Redis自增主键

其实外部自增主键的生成方式有很多，为什么我要介绍redis呢？因为我自己在实际应用中使用发现它的很多优点。

redis自身是原子性的，因此高并发也是线程安全的。假设主键字段长度20，我们以时间+自增数来构成主键，例如：8位日期+12自增数。那么，根据业务性质可以决定时间取年月日或者到毫秒级，那么在毫秒之间自增数的重复概率是极小极小的，基本的业务都能适用。

总结

上面介绍了好几种自增id，每种自增 id 有各自的应用场景，在达到上限后的表现也不同：

**1、** 表的自增 id 达到上限后，再申请时它的值就不会改变，进而导致继续插入数据时报主键冲突的错误
**2、** row_id 达到上限后，则会归 0 再重新递增，如果出现相同的 row_id，后写的数据会覆盖之前的数据
**3、** Xid 只需要不在同一个 binlog 文件中出现重复值即可。虽然理论上会出现重复值，但是概率极小，可以忽略不计
**4、** InnoDB 的 max_trx_id 递增值每次 MySQL 重启都会被保存起来，所以我们文章中提到的脏读的例子就是一个必现的 bug，好在留给我们的时间还很充裕
**5、** thread_id 是我们使用中最常见的，而且也是处理得最好的一个自增 id 逻辑了
**6、** redis外部自增，毫秒级别，理论上会出现重复值，但是概率极小，可以忽略不计
**7、** 其实，每种自增id都有各自的适用场景，大家在平时使用中可以根据具体场景再选择





# 数据库设计

几个要点：表关系 表结构 唯一索引 单索引以及组合索引 时间戳

将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 

1. 所有表必须使用 Innodb 存储引擎

没有特殊要求（即 Innodb 无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用 Innodb 存储引擎（MySQL5.5 之前默认使用 Myisam，5.6 以后默认的为 Innodb）。

Innodb 支持事务，支持行级锁，更好的恢复性，高并发下性能更好。

2. 数据库和表的字符集统一使用 UTF8

兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储 emoji 表情的需要，字符集需要采用 utf8mb4 字符集。

3. 所有表和字段都需要添加注释

使用 comment 从句添加表和列的备注，从一开始就进行数据字典的维护

4. 尽量控制单表数据量的大小,建议控制在 500 万以内。

500 万并不是 MySQL 数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。

可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小

## 数据库字段设计规范

1. 优先选择符合存储需要的最小的数据类型

**原因：**

列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多，索引的性能也就越差。

**方法：**

**a.将字符串转换成数字类型存储,如:将 IP 地址转换成整形数据**

MySQL 提供了两个方法来处理 ip 地址

•inet_aton 把 ip 转为无符号整型 (4-8 位)•inet_ntoa 把整型的 ip 转为地址

插入数据前，先用 inet_aton 把 ip 地址转为整型，可以节省空间，显示数据时，使用 inet_ntoa 把整型的 ip 地址转为地址显示即可。

**b.对于非负型的数据 (如自增 ID,整型 IP) 来说,要优先使用无符号整型来存储**

**原因：**

无符号相对于有符号可以多出一倍的存储空间

- 
- 

```
SIGNED INT -2147483648~2147483647UNSIGNED INT 0~4294967295
```

VARCHAR(N) 中的 N 代表的是字符数，而不是字节数，使用 UTF8 存储 255 个汉字 Varchar(255)=765 个字节。**过大的长度会消耗更多的内存。**

2. 避免使用 TEXT,BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据

**a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中**

MySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。

如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select * 而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。

**2、TEXT 或 BLOB 类型只能使用前缀索引**

因为MySQL[1] 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的

3. 避免使用 ENUM 类型

修改 ENUM 值需要使用 ALTER 语句

ENUM 类型的 ORDER BY 操作效率低，需要额外操作

禁止使用数值作为 ENUM 的枚举值

4. 尽可能把所有列定义为 NOT NULL

**原因：**

索引 NULL 列需要额外的空间来保存，所以要占用更多的空间

进行比较和计算时要对 NULL 值做特别的处理

5. 使用 TIMESTAMP(4 个字节) 或 DATETIME 类型 (8 个字节) 存储时间

TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07

TIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高

超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储

**经常会有人用字符串存储日期型的数据（不正确的做法）**

•缺点 1：无法用日期函数进行计算和比较•缺点 2：用字符串存储日期要占用更多的空间

6. 同财务相关的金额类数据必须使用 decimal 类型

•非精准浮点：float,double•精准浮点：decimal

Decimal 类型为精准浮点数，在计算时不会丢失精度

占用空间由定义的宽度决定，每 4 个字节可以存储 9 位数字，并且小数点要占用一个字节

可用于存储比 bigint 更大的整型数据

## mysql性能优化

Mysql 性能优化（2017-11-15-lyq）
1、当只要一行数据时使用limit 1
查询时如果已知会得到一条数据，这种情况下加上limit 1 会增加性能。因为mysql 数据库引擎会在找到一条结果停止搜索，而不是继续查询下一条是否符合标准直到所有记录查询完毕。
2、选择正确的数据库引擎
Mysql 中有两个引擎MyISAM 和InnoDB，每个引擎有利有弊。
MyISAM 适用于一些大量查询的应用，但对于有大量写功能的应用不是很好。甚至你只需要update 一个字段整个表都会被锁起来。而别的进程就算是读操作也不行要等到当前update 操作完成之后才能继续进行。另外，MyISAM 对于select count(*)这类操作是超级快的。
InnoDB 的趋势会是一个非常复杂的存储引擎，对于一些小的应用会比MyISAM 还慢，但是支持“行锁”，所以在写操作比较多的时候会比较优秀。

3、用not exists 代替not in
Not exists 用到了连接能够发挥已经建立好的索引的作用，**not in 不能使用索引。Not in 是最慢的方式要同每条记录比较**，在数据量比较大的操作时不建议使用这种方式。

4、对操作符的优化，尽量不采用不利于索引的操作符
如：in not in is null is not null <> 等，即避免在索引列上使用IS NULL 和IS NOT NULL,

应尽量避免在 where 子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描。
某个字段总要拿来搜索，为其建立索引：
Mysql 中可以利用alter table 语句来为表中的字段添加索引，语法为：alter table 表名 add index (字段名)；

5.优化数据类型

避免使用 NULL，NULL 需要特殊处理, 大多数时候应该使用 NOT NULL，或者使用一个特殊的值，如 0，-1 作为默认值。



仅可能使用更小的字段，MySQL 从磁盘读取数据后是存储到内存中的，然后使用 cpu 周期和磁盘 I/O 读取它，这意味着越小的数据类型占用的空间越小.

对查询进行优化，应尽量避免全表扫描，首先应考虑在where 及order by 涉及的列上建立索引。

6.小心字符集转换

客户端或应用程序使用的字符集可能和表本身的字符集不一样，这需要 MySQL 在运行过程中隐含地进行转换，此外，要确定字符集如 UTF-8 是否支持多字节字符，因此它们需要更多的存储空间。

7.优化子查询

遇到子查询时，MySQL 查询优化引擎并不是总是最有效的，这就是为什么经常将子查询转换为连接查询的原因了，优化器已经能够正确处理连接查询了，当然要注意的一点是，确保连接表 (第二个表) 的连接列是有索引的，在第一个表上 MySQL 通常会相对于第二个表的查询子集进行一次全表扫描，这是嵌套循环算法的一部分。

8.优化 UNION

在跨多个不同的数据库时使用 UNION 是一个有趣的优化方法，UNION 从两个互不关联的表中返回数据，这就意味着不会出现重复的行，同时也必须对数据进行排序，我们知道排序是非常耗费资源的，特别是对大表的排序。

==UNION ALL 可以大大加快速度==，如果你已经知道你的数据不会包括重复行，或者你不在乎是否会出现重复的行，在这两种情况下使用UNION ALL 更适合。此外，还可以在应用程序逻辑中采用某些方法避免出现重复的行，这样 UNION ALL 和 UNION 返回的结果都是一样的，但 UNION ALL 不会进行排序。

9.**应尽量避免在where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描**,、应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num is null 可以在num上设置默认值0，确保表中num列没有
null值，然后这样查询： select id from t where num=0

10.避免在索引列上使用计算

11.、Where 子句中：where 表之间的连接必须写在其他Where 条件之前，那些可以过滤掉最大数量记录的条件必须写在Where 子句的末尾。

MySQL从4.1版本开始支持子查询，使用子查询可以进行SELECT语句的嵌套查询，即一个SELECT查询的结果作为另一个SELECT语句的条件。子查询可以一次性完成很多逻辑上需要多个步骤才能完成的SQL操作。子查询虽然可以使查询语句很灵活，但执行效率不高。执行子查询时，MySQL需要为内层查询语句的查询结果建立一个临时表。然后外层查询语句从临时表中查询记录。查询完毕后，再撤销这些临时表。因此，子查询的速度会受到一定的影响。如果查询的数据量比较大，这种影响就会随之增大。 **在MySQL中，可以使用连接（JOIN）查询来代替子查询。连接查询不需要建立临时表**，其速度比子查询要快，如果查询中使用索引的话，性能会更好。连接之所以更有效率，是因为MySQL不需要在内存中创建临时表。尽量减少子查询，使用关联查询（left join,right join,inner join）替代



🔷**插入时可以进行的优化**：

对于InnoDB引擎的表，常见的优化方法如下： 

（1）禁用唯一性检查插入数据之前执行set unique_checks＝0来禁止对唯一索引的检查，数据导入完成之后再运行set unique_checks＝1。这和MyISAM引擎的使用方法一样。 

（2）禁用外键检查 插入数据之前执行禁止对外键的检查，数据插入完成之后再恢复对外键的检查。禁用外键检查的语句如下：

set foreign_key_checks  =0;

 恢复对外键检查的语句如下： 

set foreign_key_checks  =1;

（3）禁止自动提交 插入数据之前禁止事务的自动提交，数据导入完成之后，执行恢复自动提交操作。禁止自动提交的语句如下： 

set autocommit = 0;

数据库备份。

必须要在未登录状态下

导出整个数据库
mysqldump -u 用户名 -p 数据库名 > 导出的文件名

导出一个表
mysqldump -u 用户名 -p 数据库名 表名> 导出的文件名

导出一个数据库结构
mysqldump -u dbuser -p -d --add-drop-table

🔷MySQL中使用OPTIMIZE TABLE语句来优化表。该语句对InnoDB和MyISAM类型的表都有效。但是，OPTILMIZE TABLE语句只能优化表中的VARCHAR、BLOB或TEXT类型的字段。OPTILMIZE TABLE语句的基本语法如下： LOCAL | NO_WRITE_TO_BINLOG关键字的意义和分析表相同，都是指定不写入二进制日志；tbl_name是表名。 通过OPTIMIZE TABLE语句可以消除删除和更新造成的文件碎片。OPTIMIZE TABLE语句在执行过程中也会给表加上只读锁。 

提示 一个表使用了TEXT或者BLOB这样的数据类型，若已经删除了表的一大部分，或者已经对含有可变长度行的表（含有VARCHAR, BLOB或TEXT列的表）进行了很多更新，则应使用OPTIMIZE TABLE来重新利用未使用的空间，并整理数据文件的碎片。在多数的设置中，根本不需要运行OPTIMIZETABLE。即使对可变长度的行进行了大量的更新，也不需要经常运行，每周一次或每月一次即可，并且只需要对特定的表运行。 、



## 优化的几个层面

加索引

看执行计划

优化sql语句

分库分表

表结构设计

问题：之前有没有做过Sql优化？
　　工作中做过很多的优化，一般的优化我们并不是出现了问题才进行优化的，在进行数据库建模和数据库设计的时候回预先考虑到一些优化问题，比如表字段的类型，长度等等，包括创建合适的索引/等方式，但是这种方式只是提前的预防，并不一定能解决所有的问题，所以当我们生产环境中已经出现sq问题之后我会从数据库的性能监控，索引的创建和维护、s/语句的调整、参数的设置，架构的调整等多个方面去进行综合考虑，性能监控的会选择show profiles.performenace_schema来进行监控，索引.。。参数。。比如在我最近做的一个XX项目中，出现了XX的问题，我通过分析执行执行计划以及XXX的方式额利解决了这个问题并且在公司做了技术分享，详细了解了对应数据的知识，。。。



# 垂直分表和水平分表



**水平分表：**

![img](https://img.jbzj.com/file_images/article/201903/20193195437804.png?20192195449)

如上图所示：另外三张表表结构是一样的 只不过把数据进行分别存放在这三张表中，如果要insert 或者query 那么都需要对id进行取余 然后table名进行拼接，那么就是一张完整的table_name

但是如果我需要对name进行分表呢 或者对email呢？

那么就需要用MD5进行加密 因为MD5加密后是16进制 那么就就可以进行取余，思路同上。

**垂直分表：**

**为什么需要进行垂直分表？**

因为如果一张表中 有一个大字段 而且并不是必须要展示的或者不是当前需要用的 那么虽然没有刻意去查询 但是在根据id或者其他索引进行查询的时候就会把大字段一起查出来，会严重影响查询的性能，所以才有的垂直分表

**详细请看下图：**

![img](https://img.jbzj.com/file_images/article/201903/20193195508560.png?20192195518)

以上就是水平分表和垂直分表的一种思路，

水平分库：每个库**结构一样、数据不一样**，没有交集，库多了可以缓解io和cpu压力

垂直分库：每个库**结构、数据都不一样**，所有库的并集为全量数据

譬如：有一百万条数据，进行水平且切分，有一百个字段，进行垂直切分

# MySQL集群的解决方案

**读写分离、主从复制**

应用程序对于数据库而言都是**写少读多**，这样就可以这样设计：

主库：只负责写数据(写库，DML->insert\delete\update)

从库：只负责读数据(读库，select)

这样就可以解决如下问题：

- 主从分开后，在业务请求高并发时，只在从服务器上执行查询工作，降低主服务器的压力。
- 主从分开后，当主服务器有问题时，可迅速切换到从服务器，不会影响线上环境。
- 备份在从服务器进行，以避免备份期间影响主服务器服务。

# 拆分 VS 集群

1. 拆分：不同的多台服务器上面部署不同的服务模块，模块之间通过RPC通信和调用，用于拆分业务功能，独立部署，多个服务器共同组成一个整体对外提供服务。
2. 集群：不同的多台服务器上面部署相同的服务模块，通过分布式调度软件进行统一的调度，用于分流容灾，降低单个服务器的访问压力。





# MySQL中运算符“<=>”的作用是什么？ 

答案：比较运算符“<=>”表示安全的等于，这个运算符和“=”类似，都执行相同的比较操作，不过“<=>”可以用来判断NULL值，在两个操作数均为NULL时，其返回值为1而不为NULL，而当一个操作数为NULL时，其返回值为0而不为NULL。





# 真题17：MySQL数据类型有哪些属性？

 答案：数据类型的属性包括auto_increment、binary、default、index、not null、null、primary key、unique和zerofill，如下所示： 

| auto increment | 列<br/>l.auto increment能为新插入的行赋予一·个唯一的整数标识符，该属性只用于整数类型<br/>2.auto increment一般从1开始，每行增加1。可以通过“ALTER TABLE TB NAME AUTO INCREMENT=n；'<br/>语句强制设置自动增长列的初始值，但是该强制的默认值是保留在内存中的。如果该值在使用之前数据库重新<br/>启动，那么这个强制的默认值就会丢失，就需要在数据库启动以后重新设置<br/>3.可以使用LAST INSERT IDO查询当前线程最后插入记录使用的值。如果一次插入了多条记录，那么返<br/>回的是第一条记录使用的自动增长值<br/>4.MySQL要求将auto increment属性用于作为主键的列<br/>5.每个表只允许有一个auto increment列<br/>6.自动增长列可以手工插入，但是插入的值如果是空或者0，那么实际插入的将是自动增长后的值<br/>7.对于InnoDB表，自动增长列必须是索引.如果是组合索引，也必须是组合索引的第一列，但是对于MyISAM<br/>表，自动增长列可以是组合索引的其他列，这样插入记录后，自动增长列是按照组合索引的前儿列进行排序后<br/>递增的<br/>8.对于TRUNCATE操作，则表中的auto increment属性的值会被置为1，而DELETE并不会<br/>9，可以使用SQL语句“alter table ai.3add id0int auto increment primary key first；”来添加主键列<br/>l0.可以使用SQL语句“alter table ai4modify id int auto increment primary key；”来修改主键列<br/>11.如果达到最大值，那么继续插入会报错<br/><br/><br/> |
| -------------- | ------------------------------------------------------------ |
| binary         | binary属性只用于char和varchar值。当为列指定了该属性时，将以区分大小写的方式排序和比较 |
| default        | default属性确保在没有任何值可用的情况下，赋予某个常量值，这个值必须是常量，因为MySQL不允许插<br/>入函数或表达式值。此外，此属性无法用于BLOB或TEXT列。如果已经为此列指定了NULL属性，那么当<br/>没有指定默认值时默认值将为NULL，否则默认值将依赖于字段的数据类型 |
| primary key    | primary key属性用于确保指定行的唯一性。指定为主键的列中，值不能重复，也不能为空。为指定为主键的列赋予auto increment属性是很常见的，因为此列不必与行数据有任何关系，而只是作为一个唯一标识符。主键又分为以下两种：<br/>（1）单字段主键<br/>如果输入到数据库中的每行都已经有不可修改的唯一标识符，一般会使用单字段主键。注意，此主键一旦设置就不能再修改<br/>（2）多字段主键  如果记录中任何一一个字段都不可能保证唯一性，那么就可以使用多字段主键。这时，多个字段联合起来确保唯一性。如果出现这种情况，那么指定一个auto increment整数作为主键是更好的办法 |
| null           | 为列指定null属性时，该列可以保持为空，而不论行中其他列是否已经被填充。ull精确的说法是“无”，而不是空字符串或0 |
| unique         | 被赋予unique属性的列将确保所有值都有不同的值，只是null值可以重复。一般会指定一个列为unique，以确保该列的所有值都不同 |
| index          | 如果所有其他因素都相同，要加速数据库查询，那么使用索引通常是最重要的一个步骤。索引一个列会为该列创建一个有序的键数组，每个键指向其相应的表行。以后针对输入条件可以搜索这个有序的键数组，与搜索整个未索引的表相比，这将在性能方面得到极大的提升<br/>如果将一个列定义为not null，那么将不允许向该列插入null值。建议在重要情况下始终使用not null属性，因为它提供了一个基本验证，确保已经向查询传递了所有必要的值 |
| zerofill       | zerofill属性可用于任何数值类型，用0填充所有剩余字段空间。例如，无符号int的默认宽度是10；因此，当“零填充”的int值为4时，将表示它为0000000004 |



# 5.11 MySQL中Iimit的作用是什么？  

limit限制返回结果行数，主要用于查询之后要显示返回的前几条或者中间某几行数据，其写法如下所示： 

LIMIT 0,100;#从起始角标为0的位置，往后获取100条记录，也可简写为LIMIT 100; 

LIMIT 10,6;#从起始角标为10的位置，往后获取6条记录。 

可以直接使用limit来进行分页操作，但这个关键字在数据量和偏移量（offset）比较大时，却很低效。**所以，对limit优化，要么限制分页的数量，要么降低偏移量（offset）的大小**。一般解决方法是关联查询或子查询优化法，可以先查询出主键，然后利用主键进行关联查询。

优化示例如下： 原SQL：

SELECT*FROM MEMBER LIMIT 10000, 100; 

优化SQL：SELECT*FROM MEMBER WHERE MEMBERID>=(SELECT MEMBERID FROM MEMBER LIMIT 10000,1) LIMIT 100; 

## MySQL中的“LIMIT 0,100”与“LIMIT 100000,100”的执行效率是一样吗？为什么？

如何优化？ 答案：不一样，“LIMIT 100000,100”的效率更低。在语句“LIMIT 100000,100”中，实际上MySQL扫描了100100行记录，然后只返回100条记录，将前面的100000条记录抛弃掉。 MySQL的“limit m,n”工作原理就是先读取前m条记录，然后抛弃前m条，再读取n条想要的记录，所以m越大，性能会越差。优化思路是，在索引上完成排序分页的操作，最后根据主键关联回原表查询所需要的其他列内容。示例如下： 

优化前：

select * from memory order by last_active limit 50,5;

优化后：

select * from memory inner join(select memory_id from memory order by last_active limit 50,5) using(memory_id)

区别在于优化前的sql需要更多io浪费，因为先读索引再读数据，然后抛弃无用的行，优化后的sql只读索引就可以了，然后通过memory_id读取需要的列

using()用于两张表的join查询，要求using()指定的列在两个表中均存在，并使用之用于join的条件。
 示例：
 select a.*, b.* from a left join b using(colA);
 等同于：
 select a.*, b.* from a left join b on a.colA = b.colA; 





# 如何提高MySQL 的安全性

1.如果MySQL 客户端和服务器端的连接需要跨越并通过不可信任的网络，那么需要使用ssh 隧道来加密该连接的通信。
2.使用set password 语句来修改用户的密码，先“mysql -u root”登陆数据库系统，然后“mysql> update mysql.user set password=password(’newpwd’)”，最后执行“flush privileges”。
3.MySQL 需要提防的攻击有，防偷听、篡改、回放、拒绝服务等，不涉及可用性和容错方面。对所有的连接、查询、其他操作使用基于ACL（ACL（访问控制列表）是一种路由器配置和控制网络访问的一种有力的工具，它可控制路由器应该允许或拒绝数据包通过，可监控流量，可自上向下检查网络的安全性，可检查和过滤数据和限制不必要的路由更新，因此让网络资源节约成本的ACL 配置技术在生活中越来越广泛应用。）即访问控制列表的安全措施来完成。
4.设置除了root 用户外的其他任何用户不允许访问mysql 主数据库中的user 表；

5.使用grant 和revoke 语句来进行用户访问控制的工作;
6.不要使用明文密码，而是使用md5()和sha1()等单向的哈系函数来设置密码;

7.不要选用字典中的字来做密码;
8.采用防火墙可以去掉50%的外部危险，让数据库系统躲在防火墙后面工作，或放置在DMZ（DMZ 是英文“demilitarized zone”的缩写，隔离区，它是为了解决安装防火墙后外部网络的访问用户不能访问内部网络服务器的
问题，而设立的一个非安全系统与安全系统之间的缓冲区。）区域中;
9.从因特网上用nmap 来扫描3306 端口，也可用telnet server_host 3306 的方法测试，不允许从非信任网络中访问数据库服务器的3306 号tcp 端口，需要在防火墙或路由器上做设定;
10.服务端要对SQL 进行预编译，避免SQL 注入攻击，例如where id=234，别人却输入where id=234 or 1=1。
11.在传递数据给mysql 时检查一下大小;
12.应用程序连接到数据库时应该使用一般的用户帐号，开放少数必要的权限给该用户;
13.学会使用tcpdump 和strings 工具来查看传输数据的安全性，例如tcpdump -l -i eth0 -w -src or dst port
3306 strings。以普通用户来启动mysql 数据库服务;
14.确信在mysql 目录中只有启动数据库服务的用户才可以对文件有读和写的权限;
15.不许将process 或super 权限付给非管理用户，该mysqladmin processlist 可以列举出当前执行的查询文
本;super 权限可用于切断客户端连接、改变服务器运行参数状态、控制拷贝复制数据库的服务器;
16.如果不相信dns 服务公司的服务，可以在主机名称允许表中只设置ip 数字地址;
17.使用max_user_connections 变量来使mysqld 服务进程，对一个指定帐户限定连接数;
18.grant 语句也支持资源控制选项;
19.启动mysqld 服务进程的安全选项开关，–local-infile=0 或1，若是0 则客户端程序就无法使用local load
data 了，赋权的一个例子grant insert(user) on mysql.user to ‘user_name’@'host_name’;若使用–skip-granttables系统将对任何用户的访问不做任何访问控制，但可以用 mysqladmin flush-privileges 或mysqladmin reload来开启访问控制;默认情况是show databases 语句对所有用户开放，可以用–skip-show-databases 来关闭掉。





# mysql复制

 

通过热备份达到高可用性
如果服务器宕机， 一切都将停止；不能执行（可能很关键的）事务，无怯得到用户信息，也不能检索其他重要数据。要不惜（几乎）一切代价避免这种情况发生，因为它会严重破坏业务。最简单的方站就是配置一个额外的服务器专门作为热备份（ hotstandby ），在主服务器岩机的时候随时接管业务。

产生报表
直接用服务器上的数据创建报表将大大降低服务器的性能，在某些情况下尤其显著。
如果产生报表需要大量的后台作业，最好创建一个额外的服务器来运行这些作业。
停止报表数据库上的复制，然后在不影响主要业务服务器的情况下运行大量查询，从而得到数据库在某一特定时间的快照。例如，如果在每天最后一个事务处理完毕后停止复制，可以提取日报表而其他业务仍正常运转。
调试和审计
还可以审查服务器上的查询。例如，查看某些查询是否有性能问题，以及服务器是否由于某个糟糕的查询而不同步。

复制的基本步骤
本章将介绍一些最大化复制的效率和价值的尖端技术。首先我们要搭建一个如图二l 所示的简单复制，即单一的主从复制实例。这里不需要了解内部架构或复制过程的执行细节

![image-20210803121530230](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210803121530230.png)

1. 配置一个服务器作为master 。
2. 配置一个服务器作为slave ,
3. 将slave 连接到master ,
除非你从一开始就规划了复制且正确配置了my. c旷文件，否则步骤l 和步骤2 要求必须重启每个服务器。

向my.cnf.添加选项以自己配置master

user = mysql

pid-file = /var/run/mysqld/mysqld.pid

socket = /var/run/mysqld/mysqld.sock

port=3306

basedir=/usr

datadir = /var/lib/mysql

tmpdir = /tmp

log-bin = master-bin

log-bin-index = master-bin.index

server-id = 1

log-bin 选项给出了二进制日志产生的所有文件的基本名（稍后你将看到，二进制日志包含多个文件）。如果你创建了一个以log-bin 为基本名的扩展文件名，该扩展名将被忽略，而只使用文件的基本名（也就是没有扩展的文件名） 。
log-bin-index 选项给出了二进制索引文件的文件名，这个索引文件保存了所有binlog文件的列表

创建和运行复制的最后两步是· 使用CHANGE MASTER
TO 命令将s la v 巳定向到mas te r ，然后用START SLAVE 启动复制：
slave> CHANGE 问ASTER TO
－ 〉MASTER HOST =’ master-1',
－ 〉MASTER PORT = 3306,
－〉MASTER_USER = ＇repl_user'，
－ 〉MASTER_PASSWORD = ' xyzzy';

s lav e> START SLAVE;
Qu e 「y OK ，。「ow s affec t ed

二进制日志简介
复制过程需要使用二进制日志（ bin a r y l og ，或者bin l og ），它记录了服务器数据库上的所有变更。

二进制日志的作用是记录数据库中表的更改，然后用于复制和PITR （即时恢复），另外少数审计情况下也会用到。
注意， 二进制日志只包括数据库的改动，所以对那些不改变数据的语句则不会写入二进制日志。
从传统意义上说， MySQL 复制记录了产生变化的SQL 语句，称为基于语句的复制(statem e n t - b ased rep li cation ） 。由于基于语句的复制会在s l ave 上重新执行语句，如果m aster 和sl ave 的上下文环境不完全一致的话，可能导致slave 上的结果与master 不同。
所以在5 . 1 版本中， MySQL 还提供了基于行的复制（ row-based rep li cation ） 。相比基于语句的复制，基于行的复制将每一条记录记为二进制日志中的一行。基于行的复制不仅更加方便，而且有时候速度更快。

FLUSH LOGS 命令强制轮换（ rotate ） 二进制日志，从而得到一个“完整的” 二进制日志文件

使用SHOW BINGLOG EVENTS 命令进一步查看该文件， 如示例3-4 所示。
示例3-4 ：检查二进制日志中有哪些事件
maste 「＞ SHOW BINLOG EVENTS\G

![image-20210805085748178](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210805085748178.png)

二进制日志的结构和内容
前面讲过，二进制日志并不是一个单独的文件，而是由一系列易于管理的（例如在不影响新日志的情况下移除旧的日志）文件组成的。二进制日志包括一组存储实际内容的二进制日志文件和一个用来跟踪二进制日志文件存储位置的二进制日志索引文件。图3 -3给出了二进制日志的组织结构

![image-20210805085928381](C:\Users\14172\AppData\Roaming\Typora\typora-user-images\image-20210805085928381.png)

数据类型选择的一些建议
选小不选大：一般情况下选择可以正确存储数据的最小数据类型，越小的数据类型通常更快，占用磁盘，内存和CPU缓存更小。
简单就好：简单的数据类型的操作通常需要更少的CPU周期，例如：整型比字符操作代价要小得多，因为字符集和校对规则(排序规则)使字符比整型比较更加复杂。
尽量避免NULL：尽量制定列为NOT NULL，除非真的需要NULL类型的值，==有NULL的列值会使得索引、索引统计和值比较更加复杂。==
浮点类型的建议统一选择decimal
记录时间的建议使用int或者bigint类型，将时间转换为时间戳格式，如将时间转换为秒、毫秒，进行存储，方便走索引



# 如何查看和修改系统参数？

在MySQL里，参数也可以叫变量（Variables），一般配置文件为：/etc/my.cnf。当MySQL实例启动时，MySQL会先去读一个配置参数文件，用来寻找数据库的各种文件所在位置以及指定某些初始化参数，这些参数通常定义了某种内存结构有多大等设置。默认情况下，MySQL实例会按照一定的次序去读取所有参数文件，可以通过命令“mysql--help|grep my.cnf”来查找这些参数文件的位置。 在Linux下的次序为：/etc/my.cnf->/etc/mysql/my.cnf->/usr/local/mysql/etc/my.cnf-> ～/.my.cnf；在Windows下的次序为：C:\WINDOWS\my.ini->C:\WINDOWS\my.cnf->C:\m\my.cnf->%MySQL安装目录%\my.ini->%MySQL安装目录%\my.cnf。如果这几个配置文件中都有同一个参数，那么MySQL数据库会以读取到的最后一个配置文件中的参数为准。在Linux环境下，配置文件一般为/etc/my.cnf。在数据库启动的时候可以加上从指定参数文件，如下所示：
mysqld_safe --default-file=/etc/my.cnf &
  MySQL的变量可以分为系统变量和状态变量。MySQL没有类似于Oracle的隐含参数，也不需要隐含参数来设置。下面分别讲解。  

![img](https://img-blog.csdnimg.cn/6a0e27cacd5647deb538d1df99de95a3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAbWFyY2ggb2YgVGltZQ==,size_20,color_FFFFFF,t_70,g_se,x_16)

1.系统变量 配置MySQL服务器的运行环境。系统变量按其作用域的不同可以分为两种：
①全局（GLOBAL）级：**对整个MySQL服务器有效，但是在本次连接中并不生效，而对于新的连接则生效；**
②会话（SESSION或LOCAL）级：**只影响当前会话**，只在本次连接中生效。有些变量同时拥有以上两个级别，MySQL将在建立连接时用全局级变量初始化会话级变量，但一旦连接建立之后，全局级变量的改变不会影响到会话级变量。可以用show variables查看系统变量的值，如下所示： 可以通过show vairables或SELECT语句可以查看系统变量的值： 
 show variables like 'log%';
 show variables where Variable_name like 'log%' and value='ON';

在MySQL服务器启动时，可以通过以下两种方法设置系统变量的值：
 1）命令行参数，例如：mysqld--max_connections=200。 2）选项文件（my.cnf）。在MySQL服务器启动后，如果需要修改系统变量的值，那么可以通过SET语句

2.状态变量 状态变量用于监控MySQL服务器的运行状态，可以用show status查看。状态变量和系统变量类似，也分为全局级和会话级，show status也支持like匹配查询，不同之处在于状态变量只能由MySQL服务器本身设置和修改，对于用户来说是只读的，不可以通过SET语句设置和修改它们。另外，和系统变量类似，也可以查询通过表的方式来查询状态变量的值，MySQL 5.6查询information_schema.global_status和information_schema.session_status；MySQL 5.7查询performance_schema.global_status和performance_schema.session_status。 





# 如何管理MySQL多实例？  

MySQL多实例是指在一台机器上开启多个不同的服务端口（例如：3306、3307等），运行多个MySQL服务进程，通过不同的Socket监听不同的服务端口来提供各自的服务。 MySQL多实例可以有效利用服务器资源，当单个服务器资源有剩余时，可以充分利用剩余的资源提供更多的服务，从而节约了服务器资源。 一般有两种方式来部署MySQL多实例：**第一种是使用多个配置文件启动不同的进程来实现多实例**，这种方式的优势是逻辑简单、配置简单，缺点是管理起来不太方便；第二种是通过官方自带的mysqld_multi使用单独的配置文件来实现多实例，这种方式 定制每个实例的配置不太方面，优点是管理起来很方便，可以集中管理。 mysqld_multi常用的命令如下所示： 

● 启动全部实例：mysqld_multi start。 ● 查看全部实例状态：mysqld_multi report。 ● 启动单个实例：mysqld_multi start 3306。 ● 停止单个实例：mysqld_multi stop 3306。 ● 查看单个实例状态：mysqld_multi report 3306。 





# MySQL权限控制实现原理

MySQL 权限表在数据库启动时载入内存，用户通过身份认证后，系统会在内存中进行相应权限的存取。当 MySQL 允许一个用户执行各种操作时，它将首先核实该用户向 MySQL 服务器发送的连接请求，然后确认用户的操作请求是否被允许。

当用户进行连接时，MySQL 实现权限控制主要有以下两个阶段：

1）连接核实阶段

登录 MySQL 服务器时，客户端连接请求中会提供用户名称、主机地址和密码，MySQL 服务器会使用 user 表中的 Host、User 和 authentication_string （MySQL 5.7 版本之前是 Password）字段执行身份检查。

只有客户端请求的主机名和用户名在 user 表中有匹配的记录，并且密码正确时，MySQL 服务器才会通过身份认证，接受连接，否则拒绝连接。

> MySQL 通过 IP 地址和用户名联合进行身份认证。例如 MySQL 安装后默认创建的用户 root@localhost，表示用户 root 只能从本地（localhost）进行连接时才能通过认证。此用户从其它任何主机对数据库进行连接时都将被拒绝。也就是说，用户名相同，IP 地址不同，MySQL 则将其视为不同的用户。

服务器接受连接后进入请求核实阶段等待用户请求。如果连接核实没有通过，服务器则完全拒绝访问。

2）请求核实阶段

建立连接后，服务器进入请求核实阶段，对在此连接上的每个请求，服务器都会检查用户是否有足够的权限来执行它。这正是授权表中的权限列发挥作用的地方。

权限按照以下权限表的顺序得到数据库权限：user→db→tables_priv→columns_priv→procs_priv。在这几个权限表中，权限范围依次递减，全局权限覆盖局部权限。

请求核实的过程如下所示：

1）用户向 MySQL 发出操作请求。

2）MySQL 首先检查 user 表，匹配 User、Host 字段值，查看请求的全局权限在 user 表中是否被授权。授权则允许操作执行，如果指定的权限在 user 表中没有被授权。MySQL 将检查 db 表。

3）db 表是下一安全层级，其中的权限限定于数据库层级，在该层级的 SELECT 权限允许用户查看指定数据库的所有表中的数据。

MySQL 检查 db 权限表中的权限信息，匹配 User、Host 字段值，查看请求的数据库级别的权限在 db 表中是否被授权。授权则允许操作执行，否则 MySQL 继续向下查找。

4）MySQL 检查 tables_priv 权限表中的权限信息，匹配 User、Host 字段值，查看请求的数据表级别的权限在 tables_priv 表中是否被授权。授权则允许操作执行，否则 MySQL 继续向下查找。

5）MySQL 检查 columns_priv 权限表中的权限信息，匹配 User、Host 字段值，查看请求的列级别的权限在 columns_priv 表中是否被授权。授权则允许操作执行，否则 MySQL 继续向下查找。

6）如果所有权限表都检查完毕，还是没有找到允许的权限操作，那么 MySQL 将返回错误信息，即用户请求的操作不能执行，操作失败。

> 提示：上面提到 MySQL 通过向下层级的顺序检查权限表，但并不意味着所有的权限都要执行该过程。例如，一个用户登录到 MySQL 服务器之后只执行对 MySQL 的管理操作，此时只涉及管理权限，因此 MySQL 只检查 user 表。
